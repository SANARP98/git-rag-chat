{"name": "batch_files", "exported_at": "2026-01-12T14:45:20.918972Z", "count": 43, "documents": [{"id": "file_requirements_txt_617d9aca", "content": "# Gradio Web UI Requirements\n\n# Web UI Framework\ngradio>=5.9.0  # Gradio 5.x has Docker compatibility fixes\nhuggingface_hub>=0.27.0  # Compatible with Gradio 5.x\n\n# HTTP Client\nhttpx>=0.25.0\n\n# Git Operations\nGitPython>=3.1.40\n\n# Environment Variables\npython-dotenv>=1.0.0\n\n# Logging (standard library, but listed for clarity)\n# logging - built-in\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/requirements.txt", "relative_path": "services/web-ui/requirements.txt", "filename": "requirements.txt", "extension": ".txt"}}, {"id": "file_Dockerfile_e2e85c58", "content": "# Gradio Web UI Dockerfile\n\nFROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY src/ ./src/\n\n# Set Python path\nENV PYTHONPATH=/app/src\n\n# Expose Gradio port\nEXPOSE 7860\n\n# Run Gradio app (simple version for Docker compatibility)\nCMD [\"python\", \"src/simple_app.py\"]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/Dockerfile", "relative_path": "services/web-ui/Dockerfile", "filename": "Dockerfile", "extension": ""}}, {"id": "file_simple_app_py_317f15f9", "content": "\"\"\"Simplified Gradio web UI for Git RAG Chat - Gradio 6.x compatible.\"\"\"\n\nimport os\nimport logging\nimport gradio as gr\nimport httpx\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=os.getenv('LOG_LEVEL', 'INFO'),\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Configuration\nRAG_API_URL = os.getenv('RAG_API_URL', 'http://rag-pipeline:8001')\nGRADIO_SERVER_PORT = int(os.getenv('GRADIO_SERVER_PORT', '7860'))\n\n# Simple HTTP client with extended timeout for large repository indexing\nclient = httpx.Client(timeout=600.0)\n\n\ndef add_repository(repo_path: str, emb_provider: str, emb_model: str):\n    \"\"\"Add a repository to the system with embedding selection.\"\"\"\n    try:\n        if not repo_path.strip():\n            return \"\u274c Error: Please enter a repository path\"\n\n        # Don't validate path in web-ui since volumes are mounted in rag-pipeline\n        # Let the backend handle validation\n        repo_name = repo_path.rstrip('/').split('/')[-1]\n\n        # Step 1: Add repository\n        response = client.post(\n            f\"{RAG_API_URL}/api/repos\",\n            json={\"path\": repo_path.strip(), \"name\": repo_name}\n        )\n\n        if response.status_code == 200:\n            data = response.json()\n            repo_id = data.get('id', data.get('repo_id', 'unknown'))\n\n            # Step 2: Trigger indexing WITH embedding choice\n            logger.info(f\"Triggering indexing for repo {repo_id} with {emb_provider}/{emb_model}\")\n            index_payload = {\"force_reindex\": False, \"embedding_provider\": emb_provider}\n\n            # Only send embedding_model if it's not the default\n            if emb_model not in [\"sentence-transformers/all-MiniLM-L6-v2\", \"\"]:\n                index_payload[\"embedding_model\"] = emb_model\n\n            index_response = client.post(\n                f\"{RAG_API_URL}/api/repos/{repo_id}/index\",\n                json=index_payload\n            )\n\n            if index_response.status_code == 200:\n                index_data = index_response.json()\n                indexed_files = index_data.get('indexed_files', 0)\n                total_chunks = index_data.get('total_chunks', 0)\n                emb_info = f\"{index_data.get('embedding_provider', 'unknown')}/{index_data.get('embedding_model', 'unknown')}\"\n                emb_dim = index_data.get('embedding_dimension', 'unknown')\n\n                return f\"\"\"\u2705 Repository '{repo_name}' added and indexed!\n\n**Repo ID:** {repo_id}\n**Files indexed:** {indexed_files}\n**Chunks created:** {total_chunks}\n**Embedding:** {emb_info} ({emb_dim} dimensions)\n\"\"\"\n            else:\n                # Repository added but indexing failed\n                return f\"\u26a0\ufe0f Repository '{repo_name}' added (ID: {repo_id}) but indexing failed.\\n\\nError: {index_response.status_code}\\n{index_response.text}\"\n        else:\n            error_detail = response.json() if response.headers.get('content-type') == 'application/json' else response.text\n            return f\"\u274c Error: {response.status_code}\\n{error_detail}\"\n\n    except Exception as e:\n        logger.error(f\"Error adding repository: {e}\")\n        return f\"\u274c Error: {str(e)}\"\n\n\ndef list_repositories():\n    \"\"\"List all repositories with embedding info.\"\"\"\n    try:\n        response = client.get(f\"{RAG_API_URL}/api/repos\")\n        if response.status_code == 200:\n            repos = response.json()\n            if not repos:\n                return \"No repositories indexed yet.\", []\n\n            output = \"**Indexed Repositories:**\\n\\n\"\n            repo_choices = []\n            for repo in repos:\n                status = \"\ud83d\udfe2\" if repo.get('is_active') else \"\u26aa\"\n                repo_id = repo.get('id', '')\n                repo_name = repo.get('name', 'Unknown')\n\n                # Show embedding info\n                emb_provider = repo.get('embedding_provider', 'unknown')\n                emb_model = repo.get('embedding_model', 'unknown')\n                emb_dim = repo.get('embedding_dimension', 'unknown')\n\n                output += f\"{status} **{repo_name}** - `{repo.get('path')}`\\n\"\n                output += f\"   Files: {repo.get('total_files', 0)} | Chunks: {repo.get('total_chunks', 0)}\\n\"\n                output += f\"   Embedding: {emb_provider}/{emb_model} ({emb_dim}D)\\n\"\n                output += f\"   ID: `{repo_id}`\\n\\n\"\n\n                # Add to dropdown choices: display name + ID for selection\n                repo_choices.append(f\"{repo_name} ({repo_id})\")\n\n            return output, repo_choices\n        else:\n            return f\"Error: {response.status_code}\", []\n    except Exception as e:\n        logger.error(f\"Error listing repositories: {e}\")\n        return f\"Error: {str(e)}\", []\n\n\ndef reindex_repository(repo_selection: str, emb_provider: str, emb_model: str):\n    \"\"\"Re-index an existing repository with embedding choice.\"\"\"\n    try:\n        if not repo_selection:\n            return \"\u274c Please select a repository to re-index\"\n\n        # Extract repo_id from selection (format: \"name (repo_id)\")\n        repo_id = repo_selection.split('(')[-1].rstrip(')')\n\n        logger.info(f\"Re-indexing repository {repo_id} with {emb_provider}/{emb_model}\")\n        index_payload = {\"force_reindex\": True, \"embedding_provider\": emb_provider}\n\n        # Only send embedding_model if it's not the default\n        if emb_model not in [\"sentence-transformers/all-MiniLM-L6-v2\", \"\"]:\n            index_payload[\"embedding_model\"] = emb_model\n\n        response = client.post(\n            f\"{RAG_API_URL}/api/repos/{repo_id}/index\",\n            json=index_payload\n        )\n\n        if response.status_code == 200:\n            data = response.json()\n            indexed_files = data.get('indexed_files', 0)\n            total_chunks = data.get('total_chunks', 0)\n            emb_info = f\"{data.get('embedding_provider', 'unknown')}/{data.get('embedding_model', 'unknown')}\"\n            emb_dim = data.get('embedding_dimension', 'unknown')\n\n            return f\"\"\"\u2705 Repository re-indexed successfully!\n\n**Files indexed:** {indexed_files}\n**Chunks created:** {total_chunks}\n**Embedding:** {emb_info} ({emb_dim} dimensions)\n\"\"\"\n        else:\n            return f\"\u274c Re-indexing failed: {response.status_code}\\n{response.text}\"\n\n    except Exception as e:\n        logger.error(f\"Error re-indexing repository: {e}\")\n        return f\"\u274c Error: {str(e)}\"\n\n\ndef activate_repository(repo_selection: str):\n    \"\"\"Activate a repository to use it for queries.\"\"\"\n    try:\n        if not repo_selection:\n            return \"\u274c Please select a repository to activate\"\n\n        # Extract repo_id from selection\n        repo_id = repo_selection.split('(')[-1].rstrip(')')\n\n        logger.info(f\"Activating repository {repo_id}\")\n        response = client.put(f\"{RAG_API_URL}/api/repos/{repo_id}/activate\")\n\n        if response.status_code == 200:\n            return f\"\u2705 Repository activated successfully! You can now query this repository.\"\n        else:\n            return f\"\u274c Activation failed: {response.status_code}\\n{response.text}\"\n\n    except Exception as e:\n        logger.error(f\"Error activating repository: {e}\")\n        return f\"\u274c Error: {str(e)}\"\n\n\ndef delete_repository(repo_selection: str):\n    \"\"\"Delete a repository.\"\"\"\n    try:\n        if not repo_selection:\n            return \"\u274c Please select a repository to delete\"\n\n        # Extract repo_id from selection\n        repo_id = repo_selection.split('(')[-1].rstrip(')')\n\n        logger.info(f\"Deleting repository {repo_id}\")\n        response = client.delete(f\"{RAG_API_URL}/api/repos/{repo_id}\")\n\n        if response.status_code == 200:\n            return f\"\u2705 Repository deleted successfully!\"\n        else:\n            return f\"\u274c Delete failed: {response.status_code}\\n{response.text}\"\n\n    except Exception as e:\n        logger.error(f\"Error deleting repository: {e}\")\n        return f\"\u274c Error: {str(e)}\"\n\n\ndef check_codex_status():\n    \"\"\"Check Codex CLI status.\"\"\"\n    try:\n        response = client.get(f\"{RAG_API_URL}/api/codex/status\")\n        if response.status_code == 200:\n            data = response.json()\n\n            status_parts = [\"**Codex CLI Status:**\\n\"]\n\n            if data.get('installed'):\n                status_parts.append(f\"\u2705 **Installed**: {data.get('version', 'Unknown version')}\")\n            else:\n                status_parts.append(\"\u274c **Not Installed**\")\n\n            if data.get('authenticated'):\n                status_parts.append(\"\\n\u2705 **Authenticated**: Ready to use\")\n            elif data.get('authenticated') is False:\n                status_parts.append(\"\\n\u274c **Not Authenticated**\")\n            else:\n                status_parts.append(\"\\n\u26a0\ufe0f **Authentication Status**: Unknown\")\n\n            if data.get('error'):\n                status_parts.append(f\"\\n\\n**Error**: {data.get('error')}\")\n\n            return \"\\n\".join(status_parts)\n        else:\n            return f\"\u274c Failed to check Codex status: {response.status_code}\"\n    except Exception as e:\n        logger.error(f\"Error checking Codex status: {e}\")\n        return f\"\u274c Error: {str(e)}\"\n\n\ndef query_rag(message: str, history):\n    \"\"\"Query the RAG API.\"\"\"\n    if not message.strip():\n        return history\n\n    try:\n        response = client.post(\n            f\"{RAG_API_URL}/api/query\",\n            json={\"query\": message, \"top_k\": 10}\n        )\n\n        if response.status_code == 200:\n            data = response.json()\n            answer = data.get('answer', 'No answer generated')\n            sources = data.get('sources', [])\n\n            formatted = f\"{answer}\\n\\n**Sources:**\\n\"\n            for i, src in enumerate(sources[:5], 1):\n                file_path = src.get('file_path', 'Unknown')\n                formatted += f\"{i}. `{file_path}`\\n\"\n\n            # Gradio 6.x expects messages with 'role' and 'content' (array of content blocks)\n            history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n            history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": formatted}]})\n        else:\n            history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n            history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"\u274c Error: {response.status_code}\"}]})\n    except Exception as e:\n        logger.error(f\"Query error: {e}\")\n        history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n        history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"\u274c Error: {str(e)}\"}]})\n\n    return history\n\n\n# Create Gradio 6.x compatible interface\ndemo = gr.Blocks(title=\"Git RAG Chat\")\n\nwith demo:\n    gr.Markdown(\"# \ud83e\udd16 Git RAG Chat\\nQuery your Git repositories using natural language\")\n\n    gr.Markdown(\"## \ud83d\udcac Chat Interface\")\n\n    chatbot = gr.Chatbot(height=400)\n    msg = gr.Textbox(placeholder=\"Ask about your code...\", label=\"Your Question\", lines=2)\n\n    with gr.Row():\n        submit = gr.Button(\"Send\", variant=\"primary\")\n        clear = gr.Button(\"Clear\")\n\n    gr.Markdown(\"## \ud83d\udcc1 Repository Management\")\n\n    with gr.Accordion(\"\u2699\ufe0f Advanced Indexing Options\", open=False):\n        gr.Markdown(\"### Embedding Model Selection\")\n        embedding_provider = gr.Radio(\n            choices=[\"openai\"],\n            value=\"openai\",\n            label=\"Embedding Provider\",\n            info=\"OpenAI: High quality embeddings (1536-3072 dimensions) - requires API key\"\n        )\n\n        embedding_model = gr.Dropdown(\n            choices=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\"\n            ],\n            value=\"text-embedding-3-small\",\n            label=\"OpenAI Embedding Model\",\n            info=\"Choose the embedding model to use\"\n        )\n\n        gr.Markdown(\"\"\"\n**Model Comparison:**\n- `text-embedding-3-small`: 1536 dimensions, $0.02 per 1M tokens - Fast and cost-effective\n- `text-embedding-3-large`: 3072 dimensions, $0.13 per 1M tokens - Best quality\n        \"\"\")\n\n    repo_path_input = gr.Textbox(\n        label=\"Repository Path (Container Path)\",\n        placeholder=\"/repos  or  /host/Documents/your-repo-name\",\n        lines=1,\n        info=\"\ud83d\udca1 Use container paths: /repos or /host/Documents/[folder]\"\n    )\n    add_btn = gr.Button(\"Add & Index Repository\", variant=\"primary\")\n    add_status = gr.Textbox(label=\"Status\", lines=2, interactive=False)\n\n    gr.Markdown(\"### Existing Repositories\")\n    repos_list = gr.Markdown(\"Loading...\")\n\n    gr.Markdown(\"### Manage Existing Repository\")\n    repo_dropdown = gr.Dropdown(\n        label=\"Select Repository\",\n        choices=[],\n        interactive=True\n    )\n\n    with gr.Row():\n        refresh_btn = gr.Button(\"Refresh List\", variant=\"secondary\")\n        activate_btn = gr.Button(\"Activate Selected\", variant=\"primary\")\n        reindex_btn = gr.Button(\"Re-Index Selected\")\n        delete_btn = gr.Button(\"Delete Selected\", variant=\"stop\")\n\n    manage_status = gr.Textbox(label=\"Management Status\", lines=2, interactive=False)\n\n    gr.Markdown(\"## \ud83d\udd27 Codex CLI Status\")\n    codex_status_display = gr.Markdown(\"Checking Codex status...\")\n    with gr.Row():\n        check_codex_btn = gr.Button(\"Check Codex Status\", variant=\"secondary\")\n\n    # Event handlers\n    def respond(message, history):\n        return query_rag(message, history), \"\"\n\n    def refresh_repos():\n        \"\"\"Refresh repository list and update dropdown.\"\"\"\n        repos_text, repo_choices = list_repositories()\n        return repos_text, gr.Dropdown(choices=repo_choices)\n\n    submit.click(respond, [msg, chatbot], [chatbot, msg])\n    msg.submit(respond, [msg, chatbot], [chatbot, msg])\n    clear.click(lambda: [], None, chatbot)\n    add_btn.click(add_repository, [repo_path_input, embedding_provider, embedding_model], [add_status])\n    refresh_btn.click(refresh_repos, outputs=[repos_list, repo_dropdown])\n    activate_btn.click(activate_repository, [repo_dropdown], [manage_status])\n    reindex_btn.click(reindex_repository, [repo_dropdown, embedding_provider, embedding_model], [manage_status])\n    delete_btn.click(delete_repository, [repo_dropdown], [manage_status])\n    check_codex_btn.click(check_codex_status, outputs=[codex_status_display])\n\n    # Load repositories and Codex status on startup\n    demo.load(refresh_repos, outputs=[repos_list, repo_dropdown])\n    demo.load(check_codex_status, outputs=[codex_status_display])\n\n    gr.Markdown(\"---\\n\ud83d\udcbb **Git RAG Chat** | Powered by ChromaDB + Gradio + Codex CLI\")\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting Git RAG Chat UI...\")\n    logger.info(f\"RAG API URL: {RAG_API_URL}\")\n    demo.launch(server_name=\"0.0.0.0\", server_port=GRADIO_SERVER_PORT)\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/simple_app.py", "relative_path": "services/web-ui/src/simple_app.py", "filename": "simple_app.py", "extension": ".py"}}, {"id": "file_app_py_ff003075", "content": "\"\"\"Gradio web UI for Git RAG Chat.\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Tuple, Optional\nimport gradio as gr\nfrom dotenv import load_dotenv\n\nfrom components.chat import ChatInterface\nfrom components.repo_manager import RepositoryManager\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=os.getenv('LOG_LEVEL', 'INFO'),\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Configuration\nRAG_API_URL = os.getenv('RAG_API_URL', 'http://rag-pipeline:8000')\nGRADIO_SERVER_PORT = int(os.getenv('GRADIO_SERVER_PORT', '7860'))\nGRADIO_ALLOWED_PATHS = os.getenv('GRADIO_ALLOWED_PATHS', '').split(',')\nGRADIO_ALLOWED_PATHS = [p.strip() for p in GRADIO_ALLOWED_PATHS if p.strip()]\n\n# Initialize components\nchat_interface = ChatInterface(rag_api_url=RAG_API_URL)\nrepo_manager = RepositoryManager(\n    rag_api_url=RAG_API_URL,\n    allowed_paths=GRADIO_ALLOWED_PATHS\n)\n\n\ndef validate_path_realtime(repo_path: str) -> str:\n    \"\"\"Real-time path validation.\"\"\"\n    return repo_manager.validate_path(repo_path)\n\n\ndef add_repository(repo_path: str) -> Tuple[str, str]:\n    \"\"\"Add and index repository.\"\"\"\n    status, info, success = repo_manager.add_repository(repo_path)\n    return status, info\n\n\ndef get_indexing_status() -> str:\n    \"\"\"Get current repository indexing status.\"\"\"\n    return repo_manager.get_indexing_status()\n\n\ndef list_repositories() -> str:\n    \"\"\"List all repositories.\"\"\"\n    return repo_manager.list_repositories()\n\n\ndef chat_query(\n    message: str,\n    history: List[Tuple[str, str]],\n    temperature: float,\n    max_results: int\n) -> Tuple[str, List[Tuple[str, str]]]:\n    \"\"\"Process chat query.\"\"\"\n    return chat_interface.query(\n        message=message,\n        history=history,\n        repo_id=repo_manager.current_repo_id,\n        temperature=temperature,\n        max_results=max_results\n    )\n\n\ndef clear_chat() -> List[Tuple[str, str]]:\n    \"\"\"Clear chat history.\"\"\"\n    return chat_interface.clear_history()\n\n\n# Create Gradio Interface\nwith gr.Blocks(\n    title=\"Git RAG Chat\",\n    theme=gr.themes.Soft(),\n    css=\"\"\"\n    .container { max-width: 1200px; margin: auto; }\n    .repo-info { background-color: #f0f0f0; padding: 15px; border-radius: 5px; }\n    \"\"\",\n    analytics_enabled=False,\n    fill_height=True\n) as app:\n    gr.Markdown(\n        \"\"\"\n        # \ud83e\udd16 Git RAG Chat\n\n        Query your Git repositories using natural language with RAG (Retrieval-Augmented Generation).\n        \"\"\"\n    )\n\n    with gr.Tabs():\n        # Tab 1: Chat Interface\n        with gr.Tab(\"\ud83d\udcac Chat\"):\n            with gr.Row():\n                with gr.Column(scale=3):\n                    chatbot = gr.Chatbot(\n                        label=\"Conversation\",\n                        height=500,\n                        show_copy_button=True,\n                        render_markdown=True\n                    )\n\n                    with gr.Row():\n                        msg = gr.Textbox(\n                            label=\"Your Question\",\n                            placeholder=\"Ask about your code... (e.g., 'How does authentication work?')\",\n                            lines=2,\n                            scale=4\n                        )\n                        submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n\n                    with gr.Row():\n                        clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n\n                with gr.Column(scale=1):\n                    gr.Markdown(\"### \u2699\ufe0f Query Settings\")\n\n                    temperature_slider = gr.Slider(\n                        minimum=0.0,\n                        maximum=1.0,\n                        value=0.1,\n                        step=0.1,\n                        label=\"Temperature\",\n                        info=\"Lower = more focused, Higher = more creative\"\n                    )\n\n                    max_results_slider = gr.Slider(\n                        minimum=1,\n                        maximum=20,\n                        value=10,\n                        step=1,\n                        label=\"Max Results\",\n                        info=\"Number of code chunks to retrieve\"\n                    )\n\n                    gr.Markdown(\"### \ud83d\udcca Status\")\n                    status_display = gr.Markdown(\"No repository selected\")\n                    refresh_status_btn = gr.Button(\"Refresh Status\", size=\"sm\")\n\n            # Chat event handlers\n            submit_btn.click(\n                chat_query,\n                inputs=[msg, chatbot, temperature_slider, max_results_slider],\n                outputs=[msg, chatbot]\n            )\n\n            msg.submit(\n                chat_query,\n                inputs=[msg, chatbot, temperature_slider, max_results_slider],\n                outputs=[msg, chatbot]\n            )\n\n            clear_btn.click(clear_chat, outputs=[chatbot])\n\n            refresh_status_btn.click(\n                get_indexing_status,\n                outputs=[status_display]\n            )\n\n        # Tab 2: Repository Management\n        with gr.Tab(\"\ud83d\udcc1 Repository\"):\n            gr.Markdown(\"### Add New Repository\")\n\n            with gr.Row():\n                with gr.Column(scale=3):\n                    repo_path_input = gr.Textbox(\n                        label=\"Repository Path\",\n                        placeholder=\"Enter full path to Git repository (e.g., /Users/name/my-project)\",\n                        lines=1,\n                        info=\"Paste the full path to your local Git repository\"\n                    )\n\n                    validation_status = gr.Textbox(\n                        label=\"Validation Status\",\n                        interactive=False,\n                        lines=1\n                    )\n\n                    with gr.Row():\n                        validate_btn = gr.Button(\"Add & Index Repository\", variant=\"primary\")\n                        clear_path_btn = gr.Button(\"Clear\", variant=\"secondary\")\n\n                with gr.Column(scale=2):\n                    if GRADIO_ALLOWED_PATHS:\n                        allowed_info = \"\\n\".join([f\"- `{p}`\" for p in GRADIO_ALLOWED_PATHS])\n                        gr.Markdown(\n                            f\"\"\"\n                            ### \u2139\ufe0f Allowed Directories\n\n                            For security, only paths under these directories are allowed:\n\n                            {allowed_info}\n                            \"\"\"\n                        )\n\n            repo_info_display = gr.Markdown(\"\", elem_classes=[\"repo-info\"])\n\n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Existing Repositories\")\n\n            repos_list_display = gr.Markdown(\"Loading...\")\n            refresh_repos_btn = gr.Button(\"Refresh List\", variant=\"secondary\")\n\n            # Repository event handlers\n            repo_path_input.change(\n                validate_path_realtime,\n                inputs=[repo_path_input],\n                outputs=[validation_status]\n            )\n\n            validate_btn.click(\n                add_repository,\n                inputs=[repo_path_input],\n                outputs=[validation_status, repo_info_display]\n            )\n\n            clear_path_btn.click(\n                lambda: (\"\", \"\", \"\"),\n                outputs=[repo_path_input, validation_status, repo_info_display]\n            )\n\n            refresh_repos_btn.click(\n                list_repositories,\n                outputs=[repos_list_display]\n            )\n\n            # Load repositories on startup\n            app.load(list_repositories, outputs=[repos_list_display])\n\n        # Tab 3: Settings & Help\n        with gr.Tab(\"\u2699\ufe0f Settings\"):\n            gr.Markdown(\n                f\"\"\"\n                ### Configuration\n\n                **RAG API URL:** `{RAG_API_URL}`\n\n                **Server Port:** `{GRADIO_SERVER_PORT}`\n\n                **Allowed Paths:** {', '.join([f'`{p}`' for p in GRADIO_ALLOWED_PATHS]) if GRADIO_ALLOWED_PATHS else 'All paths allowed (not recommended for production)'}\n\n                ---\n\n                ### How It Works\n\n                1. **Add Repository**: Select a local Git repository to index\n                2. **Indexing**: The system extracts code, creates embeddings, and stores them in ChromaDB\n                3. **Query**: Ask questions in natural language\n                4. **Retrieval**: The system finds relevant code chunks using semantic search\n                5. **Generation**: LLM generates answers based on retrieved context\n\n                ### Tips\n\n                - **Be Specific**: Ask targeted questions about specific features or functions\n                - **Use Context**: Mention file names or component names when relevant\n                - **Adjust Settings**: Lower temperature for factual answers, higher for creative suggestions\n                - **Check Sources**: Review the source code snippets provided with each answer\n\n                ### Supported LLM Providers\n\n                - **Codex CLI (ChatGPT Enterprise)**: Primary provider using GPT-4\n                - **Ollama**: Offline fallback with local models (deepseek-coder, codellama)\n\n                ### Troubleshooting\n\n                - **No Response**: Check if RAG pipeline container is running\n                - **Indexing Stuck**: Check file-watcher logs for errors\n                - **Empty Results**: Repository might not be fully indexed yet\n                \"\"\"\n            )\n\n    gr.Markdown(\n        \"\"\"\n        ---\n\n        **Git RAG Chat** | Built with ChromaDB, sentence-transformers, and Gradio\n        \"\"\"\n    )\n\n\ndef main():\n    \"\"\"Launch Gradio app.\"\"\"\n    logger.info(\"Starting Git RAG Chat UI...\")\n    logger.info(f\"RAG API URL: {RAG_API_URL}\")\n    logger.info(f\"Server Port: {GRADIO_SERVER_PORT}\")\n    logger.info(f\"Allowed Paths: {GRADIO_ALLOWED_PATHS or 'All (unrestricted)'}\")\n\n    try:\n        # Simple launch - Gradio handles Docker detection automatically\n        app.launch(\n            server_name=\"0.0.0.0\",\n            server_port=GRADIO_SERVER_PORT,\n            share=False\n        )\n    except KeyboardInterrupt:\n        logger.info(\"Shutting down...\")\n        chat_interface.close()\n        repo_manager.close()\n    except Exception as e:\n        logger.error(f\"Failed to start UI: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/app.py", "relative_path": "services/web-ui/src/app.py", "filename": "app.py", "extension": ".py"}}, {"id": "file_repo_validator_py_c0ae27c1", "content": "\"\"\"Repository validation for directory picker.\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom typing import Tuple, Optional\nimport git\n\nlogger = logging.getLogger(__name__)\n\n\nclass RepositoryValidator:\n    \"\"\"Validate Git repositories for directory picker.\"\"\"\n\n    @staticmethod\n    def quick_validate(repo_path: str) -> str:\n        \"\"\"Quick validation for real-time feedback.\n\n        Args:\n            repo_path: Path to validate\n\n        Returns:\n            Status message\n        \"\"\"\n        if not repo_path or not repo_path.strip():\n            return \"\"\n\n        try:\n            path = Path(repo_path).resolve()\n\n            if not path.exists():\n                return \"\u274c Path does not exist\"\n\n            if not path.is_dir():\n                return \"\u274c Path is not a directory\"\n\n            if not (path / \".git\").exists():\n                return \"\u26a0\ufe0f Not a Git repository\"\n\n            return \"\u2705 Valid Git repository\"\n\n        except Exception as e:\n            return f\"\u274c Error: {str(e)}\"\n\n    @staticmethod\n    def validate_and_get_info(repo_path: str) -> Tuple[bool, str, Optional[dict]]:\n        \"\"\"Full validation and extract repository information.\n\n        Args:\n            repo_path: Path to repository\n\n        Returns:\n            Tuple of (is_valid, message, repo_info)\n        \"\"\"\n        if not repo_path or not repo_path.strip():\n            return False, \"Please enter a repository path\", None\n\n        try:\n            path = Path(repo_path).resolve()\n\n            # Check existence\n            if not path.exists():\n                return False, \"\u274c Error: Path does not exist\", None\n\n            if not path.is_dir():\n                return False, \"\u274c Error: Path is not a directory\", None\n\n            # Check Git repository\n            try:\n                repo = git.Repo(path)\n\n                # Get repository info\n                branch = repo.active_branch.name\n                latest_commit = repo.head.commit\n\n                # Count files\n                all_files = list(path.rglob(\"*\"))\n                file_count = sum(1 for f in all_files if f.is_file())\n\n                # Get tracked files\n                tracked_files = repo.git.ls_files().split('\\n')\n                tracked_count = len([f for f in tracked_files if f.strip()])\n\n                repo_info = {\n                    'path': str(path),\n                    'name': path.name,\n                    'branch': branch,\n                    'commit_hash': latest_commit.hexsha[:8],\n                    'commit_message': latest_commit.message.strip(),\n                    'file_count': file_count,\n                    'tracked_count': tracked_count,\n                    'author': f\"{latest_commit.author.name} <{latest_commit.author.email}>\",\n                    'commit_date': latest_commit.committed_datetime.strftime('%Y-%m-%d %H:%M:%S')\n                }\n\n                success_msg = f\"\u2705 Repository validated: {path.name}\"\n                return True, success_msg, repo_info\n\n            except git.exc.InvalidGitRepositoryError:\n                return False, \"\u274c Error: Not a valid Git repository\", None\n\n        except Exception as e:\n            logger.error(f\"Validation error: {e}\")\n            return False, f\"\u274c Error: {str(e)}\", None\n\n    @staticmethod\n    def is_path_allowed(repo_path: str, allowed_paths: list) -> bool:\n        \"\"\"Check if path is within allowed directories.\n\n        Args:\n            repo_path: Path to check\n            allowed_paths: List of allowed parent paths\n\n        Returns:\n            True if path is allowed\n        \"\"\"\n        try:\n            path = Path(repo_path).resolve()\n\n            for allowed in allowed_paths:\n                allowed_path = Path(allowed).resolve()\n                try:\n                    path.relative_to(allowed_path)\n                    return True\n                except ValueError:\n                    continue\n\n            return False\n\n        except Exception:\n            return False\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/components/repo_validator.py", "relative_path": "services/web-ui/src/components/repo_validator.py", "filename": "repo_validator.py", "extension": ".py"}}, {"id": "file_repo_manager_py_2187a41b", "content": "\"\"\"Repository management component with directory picker.\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Tuple\nimport httpx\n\nfrom .repo_validator import RepositoryValidator\n\nlogger = logging.getLogger(__name__)\n\n\nclass RepositoryManager:\n    \"\"\"Manage repository selection and indexing.\"\"\"\n\n    def __init__(self, rag_api_url: str, allowed_paths: Optional[list] = None):\n        \"\"\"Initialize repository manager.\n\n        Args:\n            rag_api_url: URL of RAG pipeline API\n            allowed_paths: List of allowed parent directories\n        \"\"\"\n        self.rag_api_url = rag_api_url\n        self.allowed_paths = allowed_paths or []\n        # Increased timeout to 10 minutes for large repository indexing with OpenAI embeddings\n        self.http_client = httpx.Client(timeout=600.0)\n        self.current_repo_id: Optional[str] = None\n\n    def validate_path(self, repo_path: str) -> str:\n        \"\"\"Quick validation for real-time feedback.\n\n        Args:\n            repo_path: Path to validate\n\n        Returns:\n            Status message\n        \"\"\"\n        # Use validator for quick check\n        status = RepositoryValidator.quick_validate(repo_path)\n\n        # Additional security check if allowed_paths configured\n        if status.startswith(\"\u2705\") and self.allowed_paths:\n            if not RepositoryValidator.is_path_allowed(repo_path, self.allowed_paths):\n                return \"\u274c Path not in allowed directories\"\n\n        return status\n\n    def add_repository(self, repo_path: str) -> Tuple[str, str, bool]:\n        \"\"\"Validate and add repository to system.\n\n        Args:\n            repo_path: Path to repository\n\n        Returns:\n            Tuple of (status_message, repo_info_markdown, success)\n        \"\"\"\n        if not repo_path or not repo_path.strip():\n            return \"\u26a0\ufe0f Please enter a repository path\", \"\", False\n\n        # Validate\n        is_valid, message, repo_info = RepositoryValidator.validate_and_get_info(repo_path)\n\n        if not is_valid:\n            return message, \"\", False\n\n        # Security check\n        if self.allowed_paths and not RepositoryValidator.is_path_allowed(\n            repo_path, self.allowed_paths\n        ):\n            return \"\u274c Error: Path not in allowed directories\", \"\", False\n\n        # Call RAG API to add repository\n        try:\n            response = self.http_client.post(\n                f\"{self.rag_api_url}/api/repos\",\n                json={\"path\": repo_info['path'], \"name\": repo_info['name']}\n            )\n\n            if response.status_code == 200:\n                result = response.json()\n                repo_id = result.get('repo_id')\n                self.current_repo_id = repo_id\n\n                # Format repository info\n                info_md = self._format_repo_info(repo_info, indexing=True)\n\n                success_msg = f\"\u2705 Repository added and indexing started\"\n                return success_msg, info_md, True\n\n            else:\n                error = response.json().get('detail', 'Unknown error')\n                return f\"\u274c API Error: {error}\", \"\", False\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to add repository: {e}\")\n            return f\"\u274c Connection Error: {str(e)}\", \"\", False\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            return f\"\u274c Error: {str(e)}\", \"\", False\n\n    def get_indexing_status(self, repo_id: Optional[str] = None) -> str:\n        \"\"\"Get indexing status for repository.\n\n        Args:\n            repo_id: Repository ID (uses current if not specified)\n\n        Returns:\n            Status markdown\n        \"\"\"\n        target_repo_id = repo_id or self.current_repo_id\n\n        if not target_repo_id:\n            return \"\u26a0\ufe0f No repository selected\"\n\n        try:\n            response = self.http_client.get(\n                f\"{self.rag_api_url}/api/repos/{target_repo_id}/status\"\n            )\n\n            if response.status_code == 200:\n                status_data = response.json()\n\n                # Format status\n                status_md = f\"\"\"\n### Indexing Status\n\n**Repository:** {status_data.get('repo_name', 'Unknown')}\n**Status:** {status_data.get('status', 'Unknown')}\n**Files Indexed:** {status_data.get('files_indexed', 0)} / {status_data.get('total_files', 0)}\n**Chunks:** {status_data.get('total_chunks', 0)}\n**Last Updated:** {status_data.get('last_indexed_at', 'Never')}\n\"\"\"\n                return status_md.strip()\n\n            else:\n                return f\"\u274c Could not fetch status (HTTP {response.status_code})\"\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to get status: {e}\")\n            return f\"\u274c Connection Error: {str(e)}\"\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            return f\"\u274c Error: {str(e)}\"\n\n    def list_repositories(self) -> str:\n        \"\"\"List all repositories in system.\n\n        Returns:\n            Markdown formatted list\n        \"\"\"\n        try:\n            response = self.http_client.get(f\"{self.rag_api_url}/api/repos\")\n\n            if response.status_code == 200:\n                repos = response.json().get('repositories', [])\n\n                if not repos:\n                    return \"No repositories indexed yet.\"\n\n                # Format list\n                lines = [\"### Available Repositories\\n\"]\n                for repo in repos:\n                    active_marker = \"**[ACTIVE]**\" if repo.get('is_active') else \"\"\n                    lines.append(\n                        f\"- {active_marker} **{repo['name']}**  \\n\"\n                        f\"  Path: `{repo['path']}`  \\n\"\n                        f\"  Chunks: {repo.get('total_chunks', 0)} | \"\n                        f\"Files: {repo.get('total_files', 0)}  \\n\"\n                        f\"  Last Indexed: {repo.get('last_indexed_at', 'Never')}\\n\"\n                    )\n\n                return \"\\n\".join(lines)\n\n            else:\n                return f\"\u274c Could not fetch repositories (HTTP {response.status_code})\"\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to list repositories: {e}\")\n            return f\"\u274c Connection Error: {str(e)}\"\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            return f\"\u274c Error: {str(e)}\"\n\n    def switch_repository(self, repo_id: str) -> str:\n        \"\"\"Switch active repository.\n\n        Args:\n            repo_id: Repository ID to activate\n\n        Returns:\n            Status message\n        \"\"\"\n        try:\n            response = self.http_client.put(\n                f\"{self.rag_api_url}/api/repos/{repo_id}/activate\"\n            )\n\n            if response.status_code == 200:\n                self.current_repo_id = repo_id\n                return f\"\u2705 Switched to repository {repo_id}\"\n            else:\n                return f\"\u274c Failed to switch repository (HTTP {response.status_code})\"\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to switch repository: {e}\")\n            return f\"\u274c Connection Error: {str(e)}\"\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            return f\"\u274c Error: {str(e)}\"\n\n    def _format_repo_info(self, repo_info: dict, indexing: bool = False) -> str:\n        \"\"\"Format repository information as markdown.\n\n        Args:\n            repo_info: Repository information dict\n            indexing: Whether indexing is in progress\n\n        Returns:\n            Formatted markdown\n        \"\"\"\n        status_line = \"**Status:** Indexing in progress...\" if indexing else \"**Status:** Ready\"\n\n        info = f\"\"\"\n### \u2705 Repository Information\n\n**Name:** {repo_info['name']}\n**Path:** `{repo_info['path']}`\n**Branch:** {repo_info['branch']}\n**Latest Commit:** {repo_info['commit_hash']}\n**Commit Message:** {repo_info['commit_message']}\n**Author:** {repo_info['author']}\n**Date:** {repo_info['commit_date']}\n**Total Files:** {repo_info['file_count']}\n**Tracked Files:** {repo_info['tracked_count']}\n\n{status_line}\n\"\"\"\n        return info.strip()\n\n    def close(self):\n        \"\"\"Close HTTP client.\"\"\"\n        self.http_client.close()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/components/repo_manager.py", "relative_path": "services/web-ui/src/components/repo_manager.py", "filename": "repo_manager.py", "extension": ".py"}}, {"id": "file___init___py_3dd143df", "content": "\"\"\"UI components package.\"\"\"\n\nfrom .chat import ChatInterface\nfrom .repo_manager import RepositoryManager\nfrom .repo_validator import RepositoryValidator\n\n__all__ = [\n    'ChatInterface',\n    'RepositoryManager',\n    'RepositoryValidator'\n]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/components/__init__.py", "relative_path": "services/web-ui/src/components/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_chat_py_57a2b70d", "content": "\"\"\"Chat interface component with code syntax highlighting.\"\"\"\n\nimport logging\nfrom typing import List, Tuple, Optional, AsyncIterator\nimport httpx\nimport asyncio\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatInterface:\n    \"\"\"Chat interface for querying repositories.\"\"\"\n\n    def __init__(self, rag_api_url: str):\n        \"\"\"Initialize chat interface.\n\n        Args:\n            rag_api_url: URL of RAG pipeline API\n        \"\"\"\n        self.rag_api_url = rag_api_url\n        self.http_client = httpx.Client(timeout=120.0)\n        self.async_http_client = httpx.AsyncClient(timeout=120.0)\n\n    def query(\n        self,\n        message: str,\n        history: List[Tuple[str, str]],\n        repo_id: Optional[str] = None,\n        temperature: float = 0.1,\n        max_results: int = 10\n    ) -> Tuple[str, List[Tuple[str, str]]]:\n        \"\"\"Process user query and return response.\n\n        Args:\n            message: User query\n            history: Chat history\n            repo_id: Repository ID to query\n            temperature: LLM temperature\n            max_results: Number of chunks to retrieve\n\n        Returns:\n            Tuple of (response, updated_history)\n        \"\"\"\n        if not message or not message.strip():\n            return \"\", history\n\n        # Show thinking state\n        history.append((message, \"Thinking...\"))\n\n        try:\n            # Call query API\n            response = self.http_client.post(\n                f\"{self.rag_api_url}/api/query\",\n                json={\n                    \"query\": message,\n                    \"repo_id\": repo_id,\n                    \"top_k\": max_results,\n                    \"temperature\": temperature,\n                    \"include_sources\": True\n                }\n            )\n\n            if response.status_code == 200:\n                result = response.json()\n\n                answer = result.get('answer', 'No response generated')\n                sources = result.get('sources', [])\n\n                # Format response with sources\n                formatted_response = self._format_response(answer, sources)\n\n                # Update history\n                history[-1] = (message, formatted_response)\n\n                return \"\", history\n\n            else:\n                error_detail = response.json().get('detail', 'Unknown error')\n                error_msg = f\"\u274c Error: {error_detail}\"\n                history[-1] = (message, error_msg)\n                return \"\", history\n\n        except httpx.RequestError as e:\n            logger.error(f\"Query request failed: {e}\")\n            error_msg = f\"\u274c Connection Error: {str(e)}\\n\\nPlease check if RAG pipeline is running.\"\n            history[-1] = (message, error_msg)\n            return \"\", history\n\n        except Exception as e:\n            logger.error(f\"Unexpected error during query: {e}\")\n            error_msg = f\"\u274c Unexpected Error: {str(e)}\"\n            history[-1] = (message, error_msg)\n            return \"\", history\n\n    async def query_stream(\n        self,\n        message: str,\n        history: List[Tuple[str, str]],\n        repo_id: Optional[str] = None,\n        temperature: float = 0.1,\n        max_results: int = 10\n    ) -> AsyncIterator[Tuple[str, List[Tuple[str, str]]]]:\n        \"\"\"Process user query with streaming response.\n\n        Args:\n            message: User query\n            history: Chat history\n            repo_id: Repository ID to query\n            temperature: LLM temperature\n            max_results: Number of chunks to retrieve\n\n        Yields:\n            Tuple of (empty_string, updated_history) with streaming response\n        \"\"\"\n        if not message or not message.strip():\n            yield \"\", history\n            return\n\n        # Initialize response in history\n        history.append((message, \"\"))\n        accumulated_response = \"\"\n\n        try:\n            # Call streaming query API\n            async with self.async_http_client.stream(\n                'POST',\n                f\"{self.rag_api_url}/api/query/stream\",\n                json={\n                    \"query\": message,\n                    \"repo_id\": repo_id,\n                    \"top_k\": max_results,\n                    \"temperature\": temperature,\n                    \"include_sources\": True\n                }\n            ) as response:\n                if response.status_code != 200:\n                    error_text = await response.aread()\n                    error_msg = f\"\u274c Error: {error_text.decode('utf-8')}\"\n                    history[-1] = (message, error_msg)\n                    yield \"\", history\n                    return\n\n                # Stream response chunks\n                async for line in response.aiter_lines():\n                    if not line:\n                        continue\n\n                    # Remove \"data: \" prefix if present (SSE format)\n                    if line.startswith(\"data: \"):\n                        line = line[6:]\n\n                    # Accumulate response\n                    accumulated_response += line\n\n                    # Update history with accumulated response\n                    history[-1] = (message, accumulated_response)\n                    yield \"\", history\n\n        except httpx.RequestError as e:\n            logger.error(f\"Streaming query failed: {e}\")\n            error_msg = f\"\u274c Connection Error: {str(e)}\"\n            history[-1] = (message, accumulated_response + \"\\n\\n\" + error_msg)\n            yield \"\", history\n\n        except Exception as e:\n            logger.error(f\"Unexpected streaming error: {e}\")\n            error_msg = f\"\u274c Error: {str(e)}\"\n            history[-1] = (message, accumulated_response + \"\\n\\n\" + error_msg)\n            yield \"\", history\n\n    def _format_response(self, answer: str, sources: List[dict]) -> str:\n        \"\"\"Format response with sources.\n\n        Args:\n            answer: LLM generated answer\n            sources: List of source chunks\n\n        Returns:\n            Formatted markdown response\n        \"\"\"\n        formatted = f\"{answer}\\n\\n\"\n\n        if sources:\n            formatted += \"---\\n\\n### \ud83d\udcda Sources\\n\\n\"\n\n            for idx, source in enumerate(sources, 1):\n                file_path = source.get('file_path', 'Unknown')\n                start_line = source.get('start_line', '?')\n                end_line = source.get('end_line', '?')\n                language = source.get('language', '')\n                code_snippet = source.get('code', '')\n\n                formatted += f\"**{idx}. {file_path}** (lines {start_line}-{end_line})\\n\\n\"\n\n                # Add code snippet with syntax highlighting\n                if code_snippet:\n                    formatted += f\"```{language}\\n{code_snippet}\\n```\\n\\n\"\n\n        return formatted.strip()\n\n    def clear_history(self) -> List[Tuple[str, str]]:\n        \"\"\"Clear chat history.\n\n        Returns:\n            Empty history list\n        \"\"\"\n        return []\n\n    def export_history(self, history: List[Tuple[str, str]]) -> str:\n        \"\"\"Export chat history as markdown.\n\n        Args:\n            history: Chat history\n\n        Returns:\n            Markdown formatted history\n        \"\"\"\n        if not history:\n            return \"No chat history to export.\"\n\n        lines = [\"# Chat History\\n\"]\n\n        for idx, (user_msg, bot_msg) in enumerate(history, 1):\n            lines.append(f\"## Exchange {idx}\\n\")\n            lines.append(f\"**User:** {user_msg}\\n\")\n            lines.append(f\"**Assistant:**\\n{bot_msg}\\n\")\n            lines.append(\"---\\n\")\n\n        return \"\\n\".join(lines)\n\n    def close(self):\n        \"\"\"Close HTTP clients.\"\"\"\n        self.http_client.close()\n        asyncio.create_task(self.async_http_client.aclose())\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/web-ui/src/components/chat.py", "relative_path": "services/web-ui/src/components/chat.py", "filename": "chat.py", "extension": ".py"}}, {"id": "file_requirements_txt_3df1490f", "content": "# File system monitoring\nwatchdog==3.0.0\n\n# Git operations\nGitPython==3.1.40\n\n# HTTP client\nhttpx==0.25.2\n\n# Utilities\npython-dotenv==1.0.0\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/file-watcher/requirements.txt", "relative_path": "services/file-watcher/requirements.txt", "filename": "requirements.txt", "extension": ".txt"}}, {"id": "file_Dockerfile_d24d7933", "content": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY src/ ./src/\n\n# Set Python path\nENV PYTHONPATH=/app/src\n\n# Run the application\nCMD [\"python\", \"src/main.py\"]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/file-watcher/Dockerfile", "relative_path": "services/file-watcher/Dockerfile", "filename": "Dockerfile", "extension": ""}}, {"id": "file_git_monitor_py_d3a692b7", "content": "\"\"\"Git commit monitor for tracking new commits.\"\"\"\n\nimport logging\nimport time\nimport threading\nfrom typing import Optional, Callable, List\nfrom pathlib import Path\nimport git\n\nlogger = logging.getLogger(__name__)\n\n\nclass GitCommitMonitor:\n    \"\"\"Monitor a Git repository for new commits.\"\"\"\n\n    def __init__(\n        self,\n        repo_path: str,\n        callback: Callable[[str, List[str]], None],\n        poll_interval: float = 5.0\n    ):\n        \"\"\"Initialize Git commit monitor.\n\n        Args:\n            repo_path: Path to Git repository\n            callback: Function to call with (commit_hash, changed_files) when new commit detected\n            poll_interval: Seconds between Git status checks\n        \"\"\"\n        self.repo_path = Path(repo_path).resolve()\n        self.callback = callback\n        self.poll_interval = poll_interval\n\n        # Validate Git repository\n        try:\n            self.repo = git.Repo(self.repo_path)\n        except git.InvalidGitRepositoryError:\n            raise ValueError(f\"Not a valid Git repository: {repo_path}\")\n\n        # Get initial HEAD commit\n        try:\n            self.last_commit_hash = self.repo.head.commit.hexsha\n            logger.info(f\"Initial commit: {self.last_commit_hash[:8]}\")\n        except ValueError:\n            # Empty repository (no commits yet)\n            self.last_commit_hash = None\n            logger.info(\"Repository has no commits yet\")\n\n        self._running = False\n        self._thread: Optional[threading.Thread] = None\n\n        logger.info(f\"Git monitor initialized for: {self.repo_path}\")\n\n    def _monitor_loop(self):\n        \"\"\"Main monitoring loop (runs in separate thread).\"\"\"\n        logger.info(\"Git monitor loop started\")\n\n        while self._running:\n            try:\n                self._check_for_new_commits()\n            except Exception as e:\n                logger.error(f\"Error checking for commits: {e}\")\n\n            # Sleep for poll interval\n            time.sleep(self.poll_interval)\n\n        logger.info(\"Git monitor loop stopped\")\n\n    def _check_for_new_commits(self):\n        \"\"\"Check if there are new commits since last check.\"\"\"\n        try:\n            # Refresh repository state\n            self.repo.git.fetch('--all', '--quiet')\n\n            # Get current HEAD commit\n            try:\n                current_commit = self.repo.head.commit\n                current_hash = current_commit.hexsha\n            except ValueError:\n                # Repository still has no commits\n                return\n\n            # Check if commit has changed\n            if current_hash != self.last_commit_hash:\n                logger.info(f\"New commit detected: {current_hash[:8]}\")\n\n                # Get list of changed files\n                changed_files = self._get_changed_files(\n                    self.last_commit_hash,\n                    current_hash\n                )\n\n                # Update last commit hash\n                old_commit = self.last_commit_hash\n                self.last_commit_hash = current_hash\n\n                # Trigger callback\n                try:\n                    self.callback(current_hash, changed_files)\n                except Exception as e:\n                    logger.error(f\"Error in commit callback: {e}\")\n\n                logger.info(\n                    f\"Processed commit: {old_commit[:8] if old_commit else 'none'} -> {current_hash[:8]}\"\n                )\n\n        except git.GitCommandError as e:\n            logger.error(f\"Git command failed: {e}\")\n        except Exception as e:\n            logger.error(f\"Error checking commits: {e}\")\n\n    def _get_changed_files(\n        self,\n        old_commit: Optional[str],\n        new_commit: str\n    ) -> List[str]:\n        \"\"\"Get list of files changed between commits.\n\n        Args:\n            old_commit: Old commit hash (None if first commit)\n            new_commit: New commit hash\n\n        Returns:\n            List of changed file paths (relative to repo root)\n        \"\"\"\n        try:\n            if old_commit is None:\n                # First commit - get all files in the commit\n                commit = self.repo.commit(new_commit)\n                changed_files = [item.path for item in commit.tree.traverse()]\n            else:\n                # Get diff between commits\n                old = self.repo.commit(old_commit)\n                new = self.repo.commit(new_commit)\n                diff = old.diff(new)\n\n                # Extract file paths from diff\n                changed_files = []\n                for item in diff:\n                    if item.a_path:\n                        changed_files.append(item.a_path)\n                    if item.b_path and item.b_path != item.a_path:\n                        changed_files.append(item.b_path)\n\n            logger.debug(f\"Changed files: {len(changed_files)}\")\n            return changed_files\n\n        except Exception as e:\n            logger.error(f\"Error getting changed files: {e}\")\n            return []\n\n    def get_uncommitted_files(self) -> List[str]:\n        \"\"\"Get list of files with uncommitted changes.\n\n        Returns:\n            List of file paths with uncommitted changes\n        \"\"\"\n        try:\n            # Get modified files (unstaged changes)\n            modified_files = [item.a_path for item in self.repo.index.diff(None)]\n\n            # Get staged files (staged changes)\n            staged_files = [item.a_path for item in self.repo.index.diff('HEAD')]\n\n            # Get untracked files\n            untracked_files = self.repo.untracked_files\n\n            # Combine all (remove duplicates)\n            all_files = set(modified_files + staged_files + untracked_files)\n\n            logger.debug(f\"Uncommitted files: {len(all_files)}\")\n            return list(all_files)\n\n        except Exception as e:\n            logger.error(f\"Error getting uncommitted files: {e}\")\n            return []\n\n    def start(self):\n        \"\"\"Start monitoring for new commits.\"\"\"\n        if self._running:\n            logger.warning(\"Git monitor already running\")\n            return\n\n        logger.info(\"Starting Git commit monitor\")\n        self._running = True\n\n        # Start monitoring thread\n        self._thread = threading.Thread(\n            target=self._monitor_loop,\n            daemon=True,\n            name=\"GitMonitor\"\n        )\n        self._thread.start()\n\n    def stop(self):\n        \"\"\"Stop monitoring for new commits.\"\"\"\n        if not self._running:\n            return\n\n        logger.info(\"Stopping Git commit monitor\")\n        self._running = False\n\n        # Wait for thread to finish\n        if self._thread:\n            self._thread.join(timeout=self.poll_interval + 1)\n            self._thread = None\n\n    def is_running(self) -> bool:\n        \"\"\"Check if monitor is running.\n\n        Returns:\n            True if monitor is running\n        \"\"\"\n        return self._running\n\n    def get_current_commit(self) -> Optional[str]:\n        \"\"\"Get current HEAD commit hash.\n\n        Returns:\n            Commit hash or None if no commits\n        \"\"\"\n        try:\n            return self.repo.head.commit.hexsha\n        except ValueError:\n            return None\n\n    def get_branch_name(self) -> Optional[str]:\n        \"\"\"Get current branch name.\n\n        Returns:\n            Branch name or None if detached HEAD\n        \"\"\"\n        try:\n            return self.repo.active_branch.name\n        except TypeError:\n            return None\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.stop()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/file-watcher/src/git_monitor.py", "relative_path": "services/file-watcher/src/git_monitor.py", "filename": "git_monitor.py", "extension": ".py"}}, {"id": "file_watcher_py_f68fb2e4", "content": "\"\"\"File system watcher for monitoring uncommitted changes.\"\"\"\n\nimport logging\nimport time\nfrom typing import Dict, Set, Optional, Callable\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler, FileSystemEvent\nimport threading\n\nlogger = logging.getLogger(__name__)\n\n\nclass DebounceHandler(FileSystemEventHandler):\n    \"\"\"File system event handler with debouncing.\"\"\"\n\n    def __init__(\n        self,\n        callback: Callable[[str], None],\n        debounce_seconds: float = 2.0,\n        file_extensions: Optional[Set[str]] = None\n    ):\n        \"\"\"Initialize debounce handler.\n\n        Args:\n            callback: Function to call with file path when debounce period expires\n            debounce_seconds: Seconds to wait before triggering callback\n            file_extensions: Set of file extensions to monitor (e.g., {'.py', '.js'})\n        \"\"\"\n        super().__init__()\n        self.callback = callback\n        self.debounce_seconds = debounce_seconds\n        self.file_extensions = file_extensions or {\n            '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.go', '.rs',\n            '.rb', '.c', '.cpp', '.h', '.hpp', '.md', '.txt', '.json',\n            '.yaml', '.yml', '.toml', '.ini', '.cfg'\n        }\n\n        # Track pending changes with their last modification time\n        self._pending_changes: Dict[str, float] = {}\n        self._lock = threading.Lock()\n        self._timer: Optional[threading.Timer] = None\n\n    def on_modified(self, event: FileSystemEvent):\n        \"\"\"Handle file modification event.\"\"\"\n        if event.is_directory:\n            return\n\n        self._handle_event(event.src_path)\n\n    def on_created(self, event: FileSystemEvent):\n        \"\"\"Handle file creation event.\"\"\"\n        if event.is_directory:\n            return\n\n        self._handle_event(event.src_path)\n\n    def on_deleted(self, event: FileSystemEvent):\n        \"\"\"Handle file deletion event.\"\"\"\n        if event.is_directory:\n            return\n\n        # For deletions, trigger immediately\n        self._handle_event(event.src_path, immediate=True)\n\n    def _handle_event(self, file_path: str, immediate: bool = False):\n        \"\"\"Handle a file system event with debouncing.\n\n        Args:\n            file_path: Path to the file that changed\n            immediate: If True, trigger callback immediately without debouncing\n        \"\"\"\n        path = Path(file_path)\n\n        # Check if we should monitor this file\n        if not self._should_monitor(path):\n            return\n\n        logger.debug(f\"File change detected: {file_path}\")\n\n        with self._lock:\n            if immediate:\n                # Trigger callback immediately\n                self.callback(file_path)\n            else:\n                # Update pending changes\n                current_time = time.time()\n                self._pending_changes[file_path] = current_time\n\n                # Cancel existing timer if any\n                if self._timer:\n                    self._timer.cancel()\n\n                # Start new timer\n                self._timer = threading.Timer(\n                    self.debounce_seconds,\n                    self._process_pending_changes\n                )\n                self._timer.daemon = True\n                self._timer.start()\n\n    def _should_monitor(self, path: Path) -> bool:\n        \"\"\"Check if a file should be monitored.\n\n        Args:\n            path: File path\n\n        Returns:\n            True if file should be monitored\n        \"\"\"\n        # Skip hidden files\n        if path.name.startswith('.'):\n            return False\n\n        # Skip files in common ignore directories\n        ignore_dirs = {\n            '__pycache__', 'node_modules', '.git', '.venv', 'venv',\n            'env', 'dist', 'build', 'target', '.pytest_cache',\n            '.mypy_cache', '.tox', 'coverage', 'htmlcov'\n        }\n        if any(part in ignore_dirs for part in path.parts):\n            return False\n\n        # Skip files without relevant extensions\n        if self.file_extensions and path.suffix.lower() not in self.file_extensions:\n            return False\n\n        return True\n\n    def _process_pending_changes(self):\n        \"\"\"Process all pending changes after debounce period.\"\"\"\n        with self._lock:\n            if not self._pending_changes:\n                return\n\n            current_time = time.time()\n            files_to_process = []\n\n            # Find files that have exceeded debounce period\n            for file_path, mod_time in list(self._pending_changes.items()):\n                if current_time - mod_time >= self.debounce_seconds:\n                    files_to_process.append(file_path)\n                    del self._pending_changes[file_path]\n\n            # Trigger callback for each file\n            for file_path in files_to_process:\n                try:\n                    logger.info(f\"Processing change: {file_path}\")\n                    self.callback(file_path)\n                except Exception as e:\n                    logger.error(f\"Error processing file {file_path}: {e}\")\n\n            # If there are still pending changes, schedule another check\n            if self._pending_changes:\n                self._timer = threading.Timer(\n                    self.debounce_seconds,\n                    self._process_pending_changes\n                )\n                self._timer.daemon = True\n                self._timer.start()\n\n    def stop(self):\n        \"\"\"Stop the debounce timer.\"\"\"\n        with self._lock:\n            if self._timer:\n                self._timer.cancel()\n                self._timer = None\n\n\nclass FileWatcher:\n    \"\"\"Monitor file system for changes in a repository.\"\"\"\n\n    def __init__(\n        self,\n        repo_path: str,\n        callback: Callable[[str], None],\n        debounce_seconds: float = 2.0,\n        recursive: bool = True\n    ):\n        \"\"\"Initialize file watcher.\n\n        Args:\n            repo_path: Path to repository to watch\n            callback: Function to call with file path when changes are detected\n            debounce_seconds: Seconds to wait before triggering callback\n            recursive: Whether to watch subdirectories\n        \"\"\"\n        self.repo_path = Path(repo_path).resolve()\n        self.callback = callback\n        self.debounce_seconds = debounce_seconds\n        self.recursive = recursive\n\n        if not self.repo_path.exists():\n            raise ValueError(f\"Repository path does not exist: {repo_path}\")\n\n        # Create event handler\n        self.event_handler = DebounceHandler(\n            callback=self._on_file_changed,\n            debounce_seconds=debounce_seconds\n        )\n\n        # Create observer\n        self.observer = Observer()\n        self.observer.schedule(\n            self.event_handler,\n            str(self.repo_path),\n            recursive=recursive\n        )\n\n        self._running = False\n        logger.info(f\"File watcher initialized for: {self.repo_path}\")\n\n    def _on_file_changed(self, file_path: str):\n        \"\"\"Handle file change event.\n\n        Args:\n            file_path: Absolute path to changed file\n        \"\"\"\n        try:\n            # Convert to relative path\n            path = Path(file_path)\n            relative_path = path.relative_to(self.repo_path)\n\n            logger.info(f\"File changed: {relative_path}\")\n\n            # Call user callback with relative path\n            self.callback(str(relative_path))\n\n        except ValueError:\n            # Path is not relative to repo (shouldn't happen)\n            logger.warning(f\"File outside repo: {file_path}\")\n        except Exception as e:\n            logger.error(f\"Error handling file change: {e}\")\n\n    def start(self):\n        \"\"\"Start watching for file changes.\"\"\"\n        if self._running:\n            logger.warning(\"File watcher already running\")\n            return\n\n        logger.info(f\"Starting file watcher for: {self.repo_path}\")\n        self.observer.start()\n        self._running = True\n\n    def stop(self):\n        \"\"\"Stop watching for file changes.\"\"\"\n        if not self._running:\n            return\n\n        logger.info(\"Stopping file watcher\")\n        self.observer.stop()\n        self.observer.join(timeout=5)\n        self.event_handler.stop()\n        self._running = False\n\n    def is_running(self) -> bool:\n        \"\"\"Check if watcher is running.\n\n        Returns:\n            True if watcher is running\n        \"\"\"\n        return self._running\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.stop()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/file-watcher/src/watcher.py", "relative_path": "services/file-watcher/src/watcher.py", "filename": "watcher.py", "extension": ".py"}}, {"id": "file_main_py_cb39ec9b", "content": "\"\"\"Main file watcher service for Git repository monitoring.\"\"\"\n\nimport logging\nimport os\nimport sys\nimport time\nimport signal\nfrom typing import Optional, List\nfrom pathlib import Path\nimport httpx\n\nfrom watcher import FileWatcher\nfrom git_monitor import GitCommitMonitor\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass WatcherService:\n    \"\"\"Main service that coordinates file watching and Git monitoring.\"\"\"\n\n    def __init__(\n        self,\n        repo_path: str,\n        repo_id: str,\n        rag_api_url: str = \"http://rag-pipeline:8001\",\n        debounce_seconds: float = 2.0,\n        poll_interval: float = 5.0\n    ):\n        \"\"\"Initialize watcher service.\n\n        Args:\n            repo_path: Path to repository to watch\n            repo_id: Repository UUID for API calls\n            rag_api_url: URL of RAG pipeline API\n            debounce_seconds: Debounce period for file changes\n            poll_interval: Poll interval for Git commits\n        \"\"\"\n        self.repo_path = Path(repo_path).resolve()\n        self.repo_id = repo_id\n        self.rag_api_url = rag_api_url.rstrip('/')\n        self.debounce_seconds = debounce_seconds\n        self.poll_interval = poll_interval\n\n        # Validate repository\n        if not self.repo_path.exists():\n            raise ValueError(f\"Repository path does not exist: {repo_path}\")\n\n        # HTTP client for API calls\n        self.http_client = httpx.Client(timeout=30.0)\n\n        # Initialize watchers\n        self.file_watcher = FileWatcher(\n            repo_path=str(self.repo_path),\n            callback=self._on_file_changed,\n            debounce_seconds=debounce_seconds\n        )\n\n        self.git_monitor = GitCommitMonitor(\n            repo_path=str(self.repo_path),\n            callback=self._on_new_commit,\n            poll_interval=poll_interval\n        )\n\n        self._running = False\n        logger.info(f\"Watcher service initialized for repo: {repo_id}\")\n        logger.info(f\"Repository path: {self.repo_path}\")\n        logger.info(f\"RAG API URL: {self.rag_api_url}\")\n\n    def _on_file_changed(self, relative_path: str):\n        \"\"\"Handle file change event.\n\n        Args:\n            relative_path: Path relative to repository root\n        \"\"\"\n        logger.info(f\"File changed: {relative_path}\")\n\n        try:\n            # Call RAG pipeline API to re-index the file\n            url = f\"{self.rag_api_url}/api/repos/{self.repo_id}/index/file\"\n            params = {\"file_path\": relative_path}\n\n            logger.debug(f\"Calling API: POST {url}\")\n            response = self.http_client.post(url, params=params)\n\n            if response.status_code == 200:\n                result = response.json()\n                chunks_added = result.get('chunks_added', 0)\n                logger.info(f\"File re-indexed: {relative_path} ({chunks_added} chunks)\")\n            else:\n                logger.error(\n                    f\"API error: {response.status_code} - {response.text}\"\n                )\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to call RAG API: {e}\")\n        except Exception as e:\n            logger.error(f\"Error handling file change: {e}\")\n\n    def _on_new_commit(self, commit_hash: str, changed_files: List[str]):\n        \"\"\"Handle new commit event.\n\n        Args:\n            commit_hash: New commit hash\n            changed_files: List of files changed in the commit\n        \"\"\"\n        logger.info(f\"New commit: {commit_hash[:8]} ({len(changed_files)} files)\")\n\n        try:\n            # Call RAG pipeline API to trigger incremental indexing\n            url = f\"{self.rag_api_url}/api/repos/{self.repo_id}/index/incremental\"\n\n            logger.debug(f\"Calling API: POST {url}\")\n            response = self.http_client.post(url)\n\n            if response.status_code == 200:\n                result = response.json()\n                indexed_files = result.get('indexed_files', 0)\n                total_chunks = result.get('total_chunks', 0)\n                logger.info(\n                    f\"Incremental indexing completed: {indexed_files} files, {total_chunks} chunks\"\n                )\n            else:\n                logger.error(\n                    f\"API error: {response.status_code} - {response.text}\"\n                )\n\n        except httpx.RequestError as e:\n            logger.error(f\"Failed to call RAG API: {e}\")\n        except Exception as e:\n            logger.error(f\"Error handling new commit: {e}\")\n\n    def start(self):\n        \"\"\"Start the watcher service.\"\"\"\n        if self._running:\n            logger.warning(\"Watcher service already running\")\n            return\n\n        logger.info(\"Starting watcher service\")\n\n        # Start file watcher\n        self.file_watcher.start()\n        logger.info(\"File watcher started\")\n\n        # Start Git monitor\n        self.git_monitor.start()\n        logger.info(\"Git monitor started\")\n\n        self._running = True\n        logger.info(\"Watcher service is now running\")\n\n    def stop(self):\n        \"\"\"Stop the watcher service.\"\"\"\n        if not self._running:\n            return\n\n        logger.info(\"Stopping watcher service\")\n\n        # Stop watchers\n        self.file_watcher.stop()\n        self.git_monitor.stop()\n\n        # Close HTTP client\n        self.http_client.close()\n\n        self._running = False\n        logger.info(\"Watcher service stopped\")\n\n    def is_running(self) -> bool:\n        \"\"\"Check if service is running.\n\n        Returns:\n            True if service is running\n        \"\"\"\n        return self._running\n\n    def run_forever(self):\n        \"\"\"Run the service until interrupted.\"\"\"\n        self.start()\n\n        try:\n            # Keep main thread alive\n            while self._running:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            logger.info(\"Received keyboard interrupt\")\n        finally:\n            self.stop()\n\n\ndef get_env_var(name: str, default: Optional[str] = None, required: bool = False) -> Optional[str]:\n    \"\"\"Get environment variable with validation.\n\n    Args:\n        name: Environment variable name\n        default: Default value if not set\n        required: If True, raise error if not set\n\n    Returns:\n        Environment variable value or default\n\n    Raises:\n        ValueError: If required variable is not set\n    \"\"\"\n    value = os.environ.get(name, default)\n\n    if required and value is None:\n        raise ValueError(f\"Required environment variable not set: {name}\")\n\n    return value\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"Git RAG File Watcher Service\")\n    logger.info(\"=\" * 60)\n\n    # Get configuration from environment\n    try:\n        repo_path = get_env_var('REPO_PATH', required=True)\n        repo_id = get_env_var('REPO_ID', required=True)\n        rag_api_url = get_env_var('RAG_API_URL', default='http://rag-pipeline:8001')\n        debounce_seconds = float(get_env_var('DEBOUNCE_SECONDS', default='2.0'))\n        poll_interval = float(get_env_var('POLL_INTERVAL', default='5.0'))\n\n        logger.info(f\"Configuration:\")\n        logger.info(f\"  REPO_PATH: {repo_path}\")\n        logger.info(f\"  REPO_ID: {repo_id}\")\n        logger.info(f\"  RAG_API_URL: {rag_api_url}\")\n        logger.info(f\"  DEBOUNCE_SECONDS: {debounce_seconds}\")\n        logger.info(f\"  POLL_INTERVAL: {poll_interval}\")\n\n    except ValueError as e:\n        logger.error(f\"Configuration error: {e}\")\n        sys.exit(1)\n\n    # Create and run service\n    try:\n        service = WatcherService(\n            repo_path=repo_path,\n            repo_id=repo_id,\n            rag_api_url=rag_api_url,\n            debounce_seconds=debounce_seconds,\n            poll_interval=poll_interval\n        )\n\n        # Setup signal handlers\n        def signal_handler(signum, frame):\n            logger.info(f\"Received signal {signum}\")\n            service.stop()\n            sys.exit(0)\n\n        signal.signal(signal.SIGINT, signal_handler)\n        signal.signal(signal.SIGTERM, signal_handler)\n\n        # Run service\n        service.run_forever()\n\n    except Exception as e:\n        logger.error(f\"Fatal error: {e}\", exc_info=True)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/file-watcher/src/main.py", "relative_path": "services/file-watcher/src/main.py", "filename": "main.py", "extension": ".py"}}, {"id": "file_requirements_txt_bf078bb5", "content": "# Web framework\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# Database - ChromaDB with minimal dependencies\nchromadb==0.4.24\n\n# Git operations\nGitPython==3.1.40\n\n# OpenAI for embeddings (lightweight, no PyTorch needed)\nopenai>=1.40.0\n\n# Code parsing\ntree-sitter>=0.20.0\ntree-sitter-languages>=1.10.0\n\n# HTTP client\nhttpx==0.25.2\n\n# Utilities\npython-dotenv==1.0.0\nnumpy>=1.24.0,<2.0.0  # Still needed by ChromaDB but much lighter than PyTorch\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/requirements.txt", "relative_path": "services/rag-pipeline/requirements.txt", "filename": "requirements.txt", "extension": ".txt"}}, {"id": "file_Dockerfile_f1ab33ce", "content": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies in a single layer and clean up\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    curl \\\n    && curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \\\n    && apt-get install -y --no-install-recommends nodejs \\\n    && npm install -g @openai/codex \\\n    && npm cache clean --force \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt \\\n    && rm -rf /root/.cache/pip\n\n# Copy application code\nCOPY src/ ./src/\n\n# Create data directories\nRUN mkdir -p /app/data/metadata /app/data/models\n\n# Expose port\nEXPOSE 8001\n\n# Run the application\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/Dockerfile", "relative_path": "services/rag-pipeline/Dockerfile", "filename": "Dockerfile", "extension": ""}}, {"id": "file_requirements_light_txt_b2e7288c", "content": "# Web framework\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# Database\nchromadb==0.4.24\n\n# Git operations\nGitPython==3.1.40\n\n# Minimal ML (CPU-only, much lighter)\n# Instead of sentence-transformers with PyTorch, use lighter alternatives\nopenai>=1.40.0  # For OpenAI embeddings (no heavy deps)\n\n# Code parsing\ntree-sitter>=0.20.0\ntree-sitter-languages>=1.10.0\n\n# HTTP client\nhttpx==0.25.2\n\n# Utilities\npython-dotenv==1.0.0\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/requirements.light.txt", "relative_path": "services/rag-pipeline/requirements.light.txt", "filename": "requirements.light.txt", "extension": ".txt"}}, {"id": "file_Dockerfile_optimized_28ad96a2", "content": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install minimal system dependencies in a single layer\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && apt-get clean\n\n# Install Codex CLI globally (if really needed, otherwise remove)\nRUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \\\n    && apt-get update && apt-get install -y --no-install-recommends nodejs \\\n    && npm install -g @openai/codex \\\n    && apt-get remove -y curl \\\n    && apt-get autoremove -y \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && npm cache clean --force\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python dependencies with CPU-only PyTorch to save ~3GB\nRUN pip install --no-cache-dir -r requirements.txt \\\n    && pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu \\\n    && rm -rf /root/.cache/pip\n\n# Copy application code\nCOPY src/ ./src/\n\n# Create data directories\nRUN mkdir -p /app/data/metadata /app/data/models\n\n# Expose port\nEXPOSE 8001\n\n# Run the application\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\"]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/Dockerfile.optimized", "relative_path": "services/rag-pipeline/Dockerfile.optimized", "filename": "Dockerfile.optimized", "extension": ".optimized"}}, {"id": "file_config_py_9c0ce5a0", "content": "\"\"\"Configuration management for RAG pipeline.\"\"\"\n\nimport os\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom pydantic_settings import BaseSettings\n\n\nclass ChromaDBConfig(BaseModel):\n    \"\"\"ChromaDB configuration.\"\"\"\n    host: str = \"chromadb\"\n    port: int = 8000\n\n    @property\n    def url(self) -> str:\n        return f\"http://{self.host}:{self.port}\"\n\n\nclass LLMConfig(BaseModel):\n    \"\"\"LLM provider configuration.\"\"\"\n    provider: str = \"codex\"  # codex, ollama, chatgpt-enterprise\n    codex_profile: Optional[str] = None\n    ollama_base_url: str = \"http://ollama:11434\"\n    ollama_model: str = \"deepseek-coder:33b\"\n    temperature: float = 0.1\n    max_tokens: int = 4096\n\n\nclass EmbeddingConfig(BaseModel):\n    \"\"\"Embedding model configuration.\"\"\"\n    provider: str = \"local\"  # \"local\" or \"openai\"\n    model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    openai_model: str = \"text-embedding-3-large\"\n    cache_dir: str = \"/app/data/models\"\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings loaded from environment variables.\"\"\"\n\n    # ChromaDB settings\n    chroma_host: str = \"chromadb\"\n    chroma_port: int = 8000\n\n    # LLM settings\n    llm_provider: str = \"codex\"\n    codex_profile: Optional[str] = None\n    ollama_base_url: str = \"http://ollama:11434\"\n    ollama_model: str = \"deepseek-coder:33b\"\n\n    # Embedding settings (Iteration 2)\n    embedding_provider: str = \"local\"  # \"local\" or \"openai\"\n    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    openai_api_key: Optional[str] = None\n    openai_embedding_model: str = \"text-embedding-3-large\"\n\n    # Database settings\n    metadata_db_path: str = \"/app/data/metadata/repos.db\"\n\n    # Logging\n    log_level: str = \"INFO\"\n\n    # API settings\n    api_host: str = \"0.0.0.0\"\n    api_port: int = 8001\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\n\n    @property\n    def chromadb_config(self) -> ChromaDBConfig:\n        \"\"\"Get ChromaDB configuration.\"\"\"\n        return ChromaDBConfig(\n            host=self.chroma_host,\n            port=self.chroma_port\n        )\n\n    @property\n    def llm_config(self) -> LLMConfig:\n        \"\"\"Get LLM configuration.\"\"\"\n        return LLMConfig(\n            provider=self.llm_provider,\n            codex_profile=self.codex_profile,\n            ollama_base_url=self.ollama_base_url,\n            ollama_model=self.ollama_model\n        )\n\n    @property\n    def embedding_config(self) -> EmbeddingConfig:\n        \"\"\"Get embedding configuration.\"\"\"\n        return EmbeddingConfig(\n            provider=self.embedding_provider,\n            model=self.embedding_model,\n            openai_model=self.openai_embedding_model\n        )\n\n\n# Global settings instance\nsettings = Settings()\n\n\ndef get_settings() -> Settings:\n    \"\"\"Get application settings.\"\"\"\n    return settings\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/config.py", "relative_path": "services/rag-pipeline/src/config.py", "filename": "config.py", "extension": ".py"}}, {"id": "file___init___py_4e593116", "content": "\"\"\"RAG Pipeline service.\"\"\"\n\n__version__ = \"0.1.0\"\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/__init__.py", "relative_path": "services/rag-pipeline/src/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_main_py_1046cf88", "content": "\"\"\"Main FastAPI application entry point.\"\"\"\n\nimport logging\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom .config import get_settings\nfrom .api.routes import router\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Get settings\nsettings = get_settings()\n\n# Set log level from settings\nlogging.getLogger().setLevel(settings.log_level)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Git RAG Pipeline\",\n    description=\"RAG pipeline for Git repository code analysis\",\n    version=\"0.1.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # In production, specify allowed origins\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include API routes\napp.include_router(router, prefix=\"/api\")\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Run on application startup.\"\"\"\n    logger.info(\"Starting Git RAG Pipeline service...\")\n    logger.info(f\"LLM Provider: {settings.llm_provider}\")\n    logger.info(f\"ChromaDB: {settings.chroma_host}:{settings.chroma_port}\")\n    logger.info(f\"Metadata DB: {settings.metadata_db_path}\")\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Run on application shutdown.\"\"\"\n    logger.info(\"Shutting down Git RAG Pipeline service...\")\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\n        \"service\": \"Git RAG Pipeline\",\n        \"version\": \"0.1.0\",\n        \"status\": \"running\"\n    }\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\n        \"main:app\",\n        host=settings.api_host,\n        port=settings.api_port,\n        reload=True,\n        log_level=settings.log_level.lower()\n    )\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/main.py", "relative_path": "services/rag-pipeline/src/main.py", "filename": "main.py", "extension": ".py"}}, {"id": "file_codex_provider_py_166a7555", "content": "\"\"\"Codex CLI provider for ChatGPT Enterprise integration.\"\"\"\n\nimport subprocess\nimport json\nimport asyncio\nimport logging\nfrom typing import Optional, AsyncIterator, Dict, Any, List\n\nfrom .base import LLMProvider, LLMConnectionError, LLMTimeoutError, LLMInvalidRequestError\n\nlogger = logging.getLogger(__name__)\n\n\nclass CodexCLIProvider(LLMProvider):\n    \"\"\"LLM provider using Codex CLI with ChatGPT Enterprise.\n\n    This provider calls the Codex CLI tool which is authenticated\n    with ChatGPT Enterprise credentials.\n    \"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize Codex CLI provider.\n\n        Args:\n            config: Configuration dict with optional keys:\n                - profile: Codex profile name (None = default Enterprise)\n                - timeout: Request timeout in seconds (default: 60)\n                - temperature: Default temperature (default: 0.7)\n                - max_tokens: Default max tokens (default: 2000)\n        \"\"\"\n        super().__init__(config)\n\n        self.profile = self.config.get('profile')\n        self.timeout = self.config.get('timeout', 120)  # Increased from 60s to 120s for complex queries\n        self.default_temperature = self.config.get('temperature', 0.7)\n        self.default_max_tokens = self.config.get('max_tokens', 2000)\n\n        # Verify Codex CLI is available\n        try:\n            result = subprocess.run(\n                ['codex', '--version'],\n                capture_output=True,\n                text=True,\n                timeout=5\n            )\n            if result.returncode != 0:\n                logger.warning(\"Codex CLI not found or not working\")\n        except (FileNotFoundError, subprocess.TimeoutExpired) as e:\n            logger.warning(f\"Codex CLI check failed: {e}\")\n\n        logger.info(f\"Codex CLI provider initialized (profile={self.profile or 'default'})\")\n\n    async def generate(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate response using Codex CLI.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            **kwargs: Additional parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        logger.info(\"Generating response via Codex CLI\")\n\n        try:\n            # Build command\n            # Note: --skip-git-repo-check and --dangerously-bypass-approvals-and-sandbox\n            # are required when running in Docker containers\n            cmd = ['codex', 'exec', '--skip-git-repo-check', '--dangerously-bypass-approvals-and-sandbox']\n\n            if self.profile:\n                cmd.extend(['--profile', self.profile])\n\n            cmd.extend(['--json', prompt])\n\n            # Execute Codex CLI\n            result = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    result.communicate(),\n                    timeout=self.timeout\n                )\n            except asyncio.TimeoutError:\n                result.kill()\n                raise LLMTimeoutError(f\"Codex CLI timeout after {self.timeout}s\")\n\n            if result.returncode != 0:\n                error_msg = stderr.decode('utf-8')\n                logger.error(f\"Codex CLI error: {error_msg}\")\n                raise LLMConnectionError(f\"Codex CLI failed: {error_msg}\")\n\n            # Parse JSONL output\n            output = stdout.decode('utf-8')\n            response_text = self._parse_jsonl_output(output)\n\n            logger.info(f\"Generated {len(response_text)} chars\")\n            return response_text\n\n        except (FileNotFoundError, PermissionError) as e:\n            raise LLMConnectionError(f\"Codex CLI not available: {e}\")\n        except Exception as e:\n            logger.error(f\"Codex generation failed: {e}\")\n            raise\n\n    async def generate_stream(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate streaming response using Codex CLI.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            **kwargs: Additional parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        logger.info(\"Generating streaming response via Codex CLI\")\n\n        try:\n            # Build command\n            # Note: --skip-git-repo-check and --dangerously-bypass-approvals-and-sandbox\n            # are required when running in Docker containers\n            cmd = ['codex', 'exec', '--skip-git-repo-check', '--dangerously-bypass-approvals-and-sandbox']\n\n            if self.profile:\n                cmd.extend(['--profile', self.profile])\n\n            cmd.extend(['--json', prompt])\n\n            # Execute Codex CLI\n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            # Read output line by line and stream all relevant content\n            while True:\n                line = await process.stdout.readline()\n                if not line:\n                    break\n\n                try:\n                    event = json.loads(line.decode('utf-8'))\n                    event_type = event.get('type', '')\n\n                    # Handle item.created events with text deltas\n                    if event_type == 'item.created':\n                        item = event.get('item', {})\n                        item_type = item.get('type', '')\n\n                        # Stream thinking steps (optional - comment out if too verbose)\n                        if item_type == 'agent_thinking':\n                            thinking = item.get('text', '')\n                            if thinking:\n                                yield f\"\\n[Thinking] {thinking}\\n\\n\"\n\n                        # Stream agent messages\n                        elif item_type == 'agent_message':\n                            text = item.get('text', '')\n                            if text:\n                                yield text\n\n                    # Handle text deltas (for streaming responses)\n                    elif event_type == 'item.text.delta':\n                        delta = event.get('delta', '')\n                        if delta:\n                            yield delta\n\n                    # Handle completed items\n                    elif event_type == 'item.completed':\n                        item = event.get('item', {})\n                        item_type = item.get('type', '')\n\n                        # If we haven't streamed the message yet, send it now\n                        if item_type == 'agent_message':\n                            text = item.get('text', '')\n                            if text:\n                                yield text\n\n                    # Turn completed - end of response\n                    elif event_type == 'turn.completed':\n                        break\n\n                    # Handle errors\n                    elif event_type == 'error':\n                        error_msg = event.get('message', 'Unknown error')\n                        raise LLMConnectionError(f\"Codex error: {error_msg}\")\n\n                except json.JSONDecodeError:\n                    logger.warning(f\"Failed to parse JSONL line: {line}\")\n                    continue\n\n            await process.wait()\n\n        except (FileNotFoundError, PermissionError) as e:\n            raise LLMConnectionError(f\"Codex CLI not available: {e}\")\n        except Exception as e:\n            logger.error(f\"Codex streaming failed: {e}\")\n            raise\n\n    async def generate_chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate response using chat format.\n\n        Args:\n            messages: List of message dicts\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            system: System message\n            **kwargs: Additional parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        # Convert chat messages to single prompt\n        prompt = self._format_chat_prompt(messages, system)\n        return await self.generate(prompt, temperature, max_tokens, **kwargs)\n\n    async def generate_chat_stream(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate streaming response using chat format.\n\n        Args:\n            messages: List of message dicts\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            system: System message\n            **kwargs: Additional parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        # Convert chat messages to single prompt\n        prompt = self._format_chat_prompt(messages, system)\n\n        async for chunk in self.generate_stream(prompt, temperature, max_tokens, **kwargs):\n            yield chunk\n\n    def _parse_jsonl_output(self, output: str, include_all_output: bool = False) -> str:\n        \"\"\"Parse JSONL output from Codex CLI.\n\n        Args:\n            output: JSONL output string\n            include_all_output: If True, include thinking and all agent messages\n\n        Returns:\n            Final response text or full output\n        \"\"\"\n        lines = output.strip().split('\\n')\n        response_text = \"\"\n        all_messages = []\n\n        for line in lines:\n            if not line.strip():\n                continue\n\n            try:\n                event = json.loads(line)\n                event_type = event.get('type', '')\n\n                # Look for completed items with agent_message\n                if event_type == 'item.completed':\n                    item = event.get('item', {})\n                    item_type = item.get('type', '')\n\n                    # Capture agent messages (main response)\n                    if item_type == 'agent_message':\n                        response_text = item.get('text', '')\n                        if include_all_output:\n                            all_messages.append(f\"[Response]\\n{response_text}\")\n\n                    # Optionally capture thinking steps for debugging/transparency\n                    elif item_type == 'agent_thinking' and include_all_output:\n                        thinking = item.get('text', '')\n                        if thinking:\n                            all_messages.append(f\"[Thinking]\\n{thinking}\")\n\n                # Also look for turn.completed\n                elif event_type == 'turn.completed':\n                    # Turn completed, we should have the message already\n                    break\n\n            except json.JSONDecodeError:\n                logger.warning(f\"Failed to parse JSONL line: {line}\")\n                continue\n\n        if include_all_output and all_messages:\n            return \"\\n\\n\".join(all_messages)\n\n        if not response_text:\n            logger.warning(\"No response found in Codex output\")\n            # Fallback: return raw output\n            response_text = output\n\n        return response_text\n\n    def _format_chat_prompt(\n        self,\n        messages: List[Dict[str, str]],\n        system: Optional[str] = None\n    ) -> str:\n        \"\"\"Format chat messages into a single prompt.\n\n        Args:\n            messages: List of message dicts\n            system: Optional system message\n\n        Returns:\n            Formatted prompt string\n        \"\"\"\n        prompt_parts = []\n\n        # Add system message\n        if system:\n            prompt_parts.append(f\"System: {system}\\n\")\n\n        # Add conversation history\n        for msg in messages:\n            role = msg.get('role', 'user')\n            content = msg.get('content', '')\n\n            if role == 'user':\n                prompt_parts.append(f\"User: {content}\")\n            elif role == 'assistant':\n                prompt_parts.append(f\"Assistant: {content}\")\n            elif role == 'system':\n                prompt_parts.append(f\"System: {content}\")\n\n        return \"\\n\\n\".join(prompt_parts)\n\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get model information.\n\n        Returns:\n            Model info dict\n        \"\"\"\n        return {\n            'provider': 'Codex CLI (ChatGPT Enterprise)',\n            'profile': self.profile or 'default',\n            'backend': 'ChatGPT Enterprise (GPT-4)',\n            'timeout': self.timeout,\n            'streaming': True,\n            'chat_format': True\n        }\n\n    def estimate_cost(\n        self,\n        input_tokens: int,\n        output_tokens: int\n    ) -> float:\n        \"\"\"Estimate cost (free with Enterprise).\n\n        Returns:\n            0.0 (included in Enterprise subscription)\n        \"\"\"\n        return 0.0\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/llm/codex_provider.py", "relative_path": "services/rag-pipeline/src/llm/codex_provider.py", "filename": "codex_provider.py", "extension": ".py"}}, {"id": "file___init___py_5d74c557", "content": "\"\"\"LLM integration module.\"\"\"\n\nfrom .base import LLMProvider, LLMError, LLMConnectionError, LLMTimeoutError\nfrom .codex_provider import CodexCLIProvider\nfrom .ollama_provider import OllamaProvider\nfrom .factory import LLMFactory\n\n__all__ = [\n    'LLMProvider',\n    'LLMError',\n    'LLMConnectionError',\n    'LLMTimeoutError',\n    'CodexCLIProvider',\n    'OllamaProvider',\n    'LLMFactory'\n]\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/llm/__init__.py", "relative_path": "services/rag-pipeline/src/llm/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_factory_py_9d35e757", "content": "\"\"\"Factory for creating LLM provider instances.\"\"\"\n\nimport logging\nfrom typing import Optional, Dict, Any\n\nfrom .base import LLMProvider\nfrom .codex_provider import CodexCLIProvider\nfrom .ollama_provider import OllamaProvider\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMFactory:\n    \"\"\"Factory for creating LLM provider instances.\"\"\"\n\n    @staticmethod\n    def create(provider_type: str, config: Optional[Dict[str, Any]] = None) -> LLMProvider:\n        \"\"\"Create an LLM provider instance.\n\n        Args:\n            provider_type: Type of provider ('codex', 'ollama', 'chatgpt-enterprise')\n            config: Provider-specific configuration\n\n        Returns:\n            LLMProvider instance\n\n        Raises:\n            ValueError: If provider type is unknown\n        \"\"\"\n        provider_type = provider_type.lower()\n\n        logger.info(f\"Creating LLM provider: {provider_type}\")\n\n        if provider_type == 'codex':\n            return CodexCLIProvider(config)\n\n        elif provider_type == 'ollama':\n            return OllamaProvider(config)\n\n        elif provider_type == 'chatgpt-enterprise':\n            # For now, use Codex CLI which connects to ChatGPT Enterprise\n            logger.info(\"Using Codex CLI for ChatGPT Enterprise\")\n            return CodexCLIProvider(config)\n\n        else:\n            raise ValueError(\n                f\"Unknown provider type: {provider_type}. \"\n                f\"Supported types: codex, ollama, chatgpt-enterprise\"\n            )\n\n    @staticmethod\n    def create_from_settings(settings) -> LLMProvider:\n        \"\"\"Create LLM provider from application settings.\n\n        Args:\n            settings: Application settings object\n\n        Returns:\n            LLMProvider instance\n        \"\"\"\n        provider_type = settings.llm_provider\n\n        # Build config based on provider type\n        if provider_type == 'codex':\n            config = {\n                'profile': settings.codex_profile,\n                'timeout': 180  # Increased to 3 minutes for complex queries with large context\n            }\n        elif provider_type == 'ollama':\n            config = {\n                'base_url': settings.ollama_base_url,\n                'model': settings.ollama_model,\n                'timeout': 120\n            }\n        else:\n            config = {}\n\n        return LLMFactory.create(provider_type, config)\n\n    @staticmethod\n    def get_available_providers() -> list:\n        \"\"\"Get list of available provider types.\n\n        Returns:\n            List of provider type strings\n        \"\"\"\n        return ['codex', 'ollama', 'chatgpt-enterprise']\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/llm/factory.py", "relative_path": "services/rag-pipeline/src/llm/factory.py", "filename": "factory.py", "extension": ".py"}}, {"id": "file_ollama_provider_py_aa13b079", "content": "\"\"\"Ollama provider for offline LLM capability.\"\"\"\n\nimport httpx\nimport json\nimport logging\nfrom typing import Optional, AsyncIterator, Dict, Any, List\n\nfrom .base import LLMProvider, LLMConnectionError, LLMTimeoutError\n\nlogger = logging.getLogger(__name__)\n\n\nclass OllamaProvider(LLMProvider):\n    \"\"\"LLM provider using Ollama for offline capability.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize Ollama provider.\n\n        Args:\n            config: Configuration dict with optional keys:\n                - base_url: Ollama API URL (default: http://ollama:11434)\n                - model: Model name (default: deepseek-coder:33b)\n                - timeout: Request timeout in seconds (default: 120)\n        \"\"\"\n        super().__init__(config)\n\n        self.base_url = self.config.get('base_url', 'http://ollama:11434')\n        self.model = self.config.get('model', 'deepseek-coder:33b')\n        self.timeout = self.config.get('timeout', 120)\n\n        self.client = httpx.AsyncClient(timeout=self.timeout)\n\n        logger.info(f\"Ollama provider initialized (model={self.model}, url={self.base_url})\")\n\n    async def generate(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate response using Ollama.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            **kwargs: Additional parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        logger.info(f\"Generating response via Ollama ({self.model})\")\n\n        try:\n            url = f\"{self.base_url}/api/generate\"\n\n            payload = {\n                'model': self.model,\n                'prompt': prompt,\n                'stream': False,\n                'options': {\n                    'temperature': temperature,\n                }\n            }\n\n            if max_tokens:\n                payload['options']['num_predict'] = max_tokens\n\n            response = await self.client.post(url, json=payload)\n\n            if response.status_code != 200:\n                raise LLMConnectionError(\n                    f\"Ollama API error: {response.status_code} - {response.text}\"\n                )\n\n            result = response.json()\n            generated_text = result.get('response', '')\n\n            logger.info(f\"Generated {len(generated_text)} chars\")\n            return generated_text\n\n        except httpx.TimeoutException:\n            raise LLMTimeoutError(f\"Ollama timeout after {self.timeout}s\")\n        except httpx.RequestError as e:\n            raise LLMConnectionError(f\"Ollama connection failed: {e}\")\n        except Exception as e:\n            logger.error(f\"Ollama generation failed: {e}\")\n            raise\n\n    async def generate_stream(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate streaming response using Ollama.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            **kwargs: Additional parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        logger.info(f\"Generating streaming response via Ollama ({self.model})\")\n\n        try:\n            url = f\"{self.base_url}/api/generate\"\n\n            payload = {\n                'model': self.model,\n                'prompt': prompt,\n                'stream': True,\n                'options': {\n                    'temperature': temperature,\n                }\n            }\n\n            if max_tokens:\n                payload['options']['num_predict'] = max_tokens\n\n            async with self.client.stream('POST', url, json=payload) as response:\n                if response.status_code != 200:\n                    error_text = await response.aread()\n                    raise LLMConnectionError(\n                        f\"Ollama API error: {response.status_code} - {error_text}\"\n                    )\n\n                async for line in response.aiter_lines():\n                    if not line:\n                        continue\n\n                    try:\n                        chunk = json.loads(line)\n                        text = chunk.get('response', '')\n                        if text:\n                            yield text\n\n                        # Check if done\n                        if chunk.get('done', False):\n                            break\n\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to parse chunk: {line}\")\n                        continue\n\n        except httpx.TimeoutException:\n            raise LLMTimeoutError(f\"Ollama timeout after {self.timeout}s\")\n        except httpx.RequestError as e:\n            raise LLMConnectionError(f\"Ollama connection failed: {e}\")\n        except Exception as e:\n            logger.error(f\"Ollama streaming failed: {e}\")\n            raise\n\n    async def generate_chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate response using chat API.\n\n        Args:\n            messages: List of message dicts\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            system: System message\n            **kwargs: Additional parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        logger.info(f\"Generating chat response via Ollama ({self.model})\")\n\n        try:\n            url = f\"{self.base_url}/api/chat\"\n\n            # Build messages list\n            chat_messages = []\n\n            if system:\n                chat_messages.append({\n                    'role': 'system',\n                    'content': system\n                })\n\n            chat_messages.extend(messages)\n\n            payload = {\n                'model': self.model,\n                'messages': chat_messages,\n                'stream': False,\n                'options': {\n                    'temperature': temperature,\n                }\n            }\n\n            if max_tokens:\n                payload['options']['num_predict'] = max_tokens\n\n            response = await self.client.post(url, json=payload)\n\n            if response.status_code != 200:\n                raise LLMConnectionError(\n                    f\"Ollama API error: {response.status_code} - {response.text}\"\n                )\n\n            result = response.json()\n            message = result.get('message', {})\n            generated_text = message.get('content', '')\n\n            logger.info(f\"Generated {len(generated_text)} chars\")\n            return generated_text\n\n        except httpx.TimeoutException:\n            raise LLMTimeoutError(f\"Ollama timeout after {self.timeout}s\")\n        except httpx.RequestError as e:\n            raise LLMConnectionError(f\"Ollama connection failed: {e}\")\n        except Exception as e:\n            logger.error(f\"Ollama chat generation failed: {e}\")\n            raise\n\n    async def generate_chat_stream(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate streaming response using chat API.\n\n        Args:\n            messages: List of message dicts\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            system: System message\n            **kwargs: Additional parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        logger.info(f\"Generating streaming chat response via Ollama ({self.model})\")\n\n        try:\n            url = f\"{self.base_url}/api/chat\"\n\n            # Build messages list\n            chat_messages = []\n\n            if system:\n                chat_messages.append({\n                    'role': 'system',\n                    'content': system\n                })\n\n            chat_messages.extend(messages)\n\n            payload = {\n                'model': self.model,\n                'messages': chat_messages,\n                'stream': True,\n                'options': {\n                    'temperature': temperature,\n                }\n            }\n\n            if max_tokens:\n                payload['options']['num_predict'] = max_tokens\n\n            async with self.client.stream('POST', url, json=payload) as response:\n                if response.status_code != 200:\n                    error_text = await response.aread()\n                    raise LLMConnectionError(\n                        f\"Ollama API error: {response.status_code} - {error_text}\"\n                    )\n\n                async for line in response.aiter_lines():\n                    if not line:\n                        continue\n\n                    try:\n                        chunk = json.loads(line)\n                        message = chunk.get('message', {})\n                        text = message.get('content', '')\n                        if text:\n                            yield text\n\n                        # Check if done\n                        if chunk.get('done', False):\n                            break\n\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to parse chunk: {line}\")\n                        continue\n\n        except httpx.TimeoutException:\n            raise LLMTimeoutError(f\"Ollama timeout after {self.timeout}s\")\n        except httpx.RequestError as e:\n            raise LLMConnectionError(f\"Ollama connection failed: {e}\")\n        except Exception as e:\n            logger.error(f\"Ollama chat streaming failed: {e}\")\n            raise\n\n    async def check_health(self) -> bool:\n        \"\"\"Check if Ollama is accessible.\n\n        Returns:\n            True if Ollama is healthy\n        \"\"\"\n        try:\n            url = f\"{self.base_url}/api/tags\"\n            response = await self.client.get(url, timeout=5)\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Ollama health check failed: {e}\")\n            return False\n\n    async def list_models(self) -> List[str]:\n        \"\"\"List available models.\n\n        Returns:\n            List of model names\n        \"\"\"\n        try:\n            url = f\"{self.base_url}/api/tags\"\n            response = await self.client.get(url)\n\n            if response.status_code != 200:\n                return []\n\n            result = response.json()\n            models = result.get('models', [])\n            return [m.get('name', '') for m in models]\n\n        except Exception as e:\n            logger.error(f\"Failed to list models: {e}\")\n            return []\n\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get model information.\n\n        Returns:\n            Model info dict\n        \"\"\"\n        return {\n            'provider': 'Ollama',\n            'model': self.model,\n            'base_url': self.base_url,\n            'timeout': self.timeout,\n            'streaming': True,\n            'chat_format': True,\n            'offline': True\n        }\n\n    def estimate_cost(\n        self,\n        input_tokens: int,\n        output_tokens: int\n    ) -> float:\n        \"\"\"Estimate cost (free for local Ollama).\n\n        Returns:\n            0.0 (local/offline)\n        \"\"\"\n        return 0.0\n\n    async def close(self):\n        \"\"\"Close HTTP client.\"\"\"\n        await self.client.aclose()\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/llm/ollama_provider.py", "relative_path": "services/rag-pipeline/src/llm/ollama_provider.py", "filename": "ollama_provider.py", "extension": ".py"}}, {"id": "file_base_py_1df371c8", "content": "\"\"\"Base interface for LLM providers.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, AsyncIterator, Dict, Any, List\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMProvider(ABC):\n    \"\"\"Abstract base class for LLM providers.\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Initialize LLM provider.\n\n        Args:\n            config: Provider-specific configuration\n        \"\"\"\n        self.config = config or {}\n        logger.info(f\"Initializing {self.__class__.__name__}\")\n\n    @abstractmethod\n    async def generate(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate a response from the LLM.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature (0-1)\n            max_tokens: Maximum tokens to generate\n            **kwargs: Provider-specific parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def generate_stream(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate a streaming response from the LLM.\n\n        Args:\n            prompt: Input prompt\n            temperature: Sampling temperature (0-1)\n            max_tokens: Maximum tokens to generate\n            **kwargs: Provider-specific parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def generate_chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate a response using chat-based API.\n\n        Args:\n            messages: List of message dicts with 'role' and 'content'\n            temperature: Sampling temperature (0-1)\n            max_tokens: Maximum tokens to generate\n            system: Optional system message\n            **kwargs: Provider-specific parameters\n\n        Returns:\n            Generated text\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def generate_chat_stream(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        system: Optional[str] = None,\n        **kwargs\n    ) -> AsyncIterator[str]:\n        \"\"\"Generate a streaming response using chat-based API.\n\n        Args:\n            messages: List of message dicts with 'role' and 'content'\n            temperature: Sampling temperature (0-1)\n            max_tokens: Maximum tokens to generate\n            system: Optional system message\n            **kwargs: Provider-specific parameters\n\n        Yields:\n            Chunks of generated text\n        \"\"\"\n        pass\n\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the model.\n\n        Returns:\n            Dictionary with model information\n        \"\"\"\n        return {\n            'provider': self.__class__.__name__,\n            'config': self.config\n        }\n\n    def estimate_cost(\n        self,\n        input_tokens: int,\n        output_tokens: int\n    ) -> float:\n        \"\"\"Estimate cost for a request.\n\n        Args:\n            input_tokens: Number of input tokens\n            output_tokens: Number of output tokens\n\n        Returns:\n            Estimated cost in USD (0.0 if not applicable)\n        \"\"\"\n        return 0.0\n\n    def supports_streaming(self) -> bool:\n        \"\"\"Check if provider supports streaming.\n\n        Returns:\n            True if streaming is supported\n        \"\"\"\n        return True\n\n    def supports_chat(self) -> bool:\n        \"\"\"Check if provider supports chat format.\n\n        Returns:\n            True if chat format is supported\n        \"\"\"\n        return True\n\n\nclass LLMError(Exception):\n    \"\"\"Base exception for LLM errors.\"\"\"\n    pass\n\n\nclass LLMConnectionError(LLMError):\n    \"\"\"Exception for connection errors.\"\"\"\n    pass\n\n\nclass LLMTimeoutError(LLMError):\n    \"\"\"Exception for timeout errors.\"\"\"\n    pass\n\n\nclass LLMRateLimitError(LLMError):\n    \"\"\"Exception for rate limit errors.\"\"\"\n    pass\n\n\nclass LLMInvalidRequestError(LLMError):\n    \"\"\"Exception for invalid request errors.\"\"\"\n    pass\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/llm/base.py", "relative_path": "services/rag-pipeline/src/llm/base.py", "filename": "base.py", "extension": ".py"}}, {"id": "file_git_ops_py_53a28632", "content": "\"\"\"Git operations for repository management.\"\"\"\n\nimport git\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass GitOperations:\n    \"\"\"Handle Git operations for repository indexing.\"\"\"\n\n    def __init__(self, repo_path: str):\n        \"\"\"Initialize Git operations.\n\n        Args:\n            repo_path: Path to Git repository\n\n        Raises:\n            git.exc.InvalidGitRepositoryError: If path is not a Git repository\n        \"\"\"\n        self.repo_path = Path(repo_path).resolve()\n        self.repo = git.Repo(self.repo_path)\n        logger.info(f\"Initialized Git operations for {self.repo_path}\")\n\n    def is_valid_repo(self) -> bool:\n        \"\"\"Check if the repository is valid.\n\n        Returns:\n            True if valid Git repository\n        \"\"\"\n        try:\n            return not self.repo.bare\n        except Exception as e:\n            logger.error(f\"Invalid repository: {e}\")\n            return False\n\n    def get_current_branch(self) -> str:\n        \"\"\"Get the name of the current branch.\n\n        Returns:\n            Branch name\n        \"\"\"\n        try:\n            return self.repo.active_branch.name\n        except TypeError:\n            return \"HEAD\"  # Detached HEAD state\n\n    def get_latest_commit(self) -> Optional[git.Commit]:\n        \"\"\"Get the latest commit.\n\n        Returns:\n            Latest commit object or None\n        \"\"\"\n        try:\n            return self.repo.head.commit\n        except ValueError:\n            logger.warning(\"No commits found in repository\")\n            return None\n\n    def get_commit_history(self, max_count: int = 100) -> List[Dict[str, Any]]:\n        \"\"\"Get commit history.\n\n        Args:\n            max_count: Maximum number of commits to retrieve\n\n        Returns:\n            List of commit data dicts\n        \"\"\"\n        commits = []\n        try:\n            for commit in self.repo.iter_commits(max_count=max_count):\n                commits.append({\n                    \"hash\": commit.hexsha,\n                    \"short_hash\": commit.hexsha[:8],\n                    \"message\": commit.message.strip(),\n                    \"author\": commit.author.name,\n                    \"author_email\": commit.author.email,\n                    \"committed_at\": datetime.fromtimestamp(commit.committed_date),\n                    \"files_changed\": len(commit.stats.files)\n                })\n        except Exception as e:\n            logger.error(f\"Error retrieving commit history: {e}\")\n\n        return commits\n\n    def get_commits_since(self, since_hash: str) -> List[Dict[str, Any]]:\n        \"\"\"Get commits since a specific commit.\n\n        Args:\n            since_hash: Commit hash to start from (exclusive)\n\n        Returns:\n            List of commit data dicts\n        \"\"\"\n        commits = []\n        try:\n            for commit in self.repo.iter_commits(f\"{since_hash}..HEAD\"):\n                commits.append({\n                    \"hash\": commit.hexsha,\n                    \"short_hash\": commit.hexsha[:8],\n                    \"message\": commit.message.strip(),\n                    \"author\": commit.author.name,\n                    \"author_email\": commit.author.email,\n                    \"committed_at\": datetime.fromtimestamp(commit.committed_date),\n                    \"files_changed\": len(commit.stats.files)\n                })\n        except Exception as e:\n            logger.error(f\"Error retrieving commits since {since_hash}: {e}\")\n\n        return commits\n\n    def get_changed_files(self, commit_hash: str) -> List[str]:\n        \"\"\"Get list of files changed in a commit.\n\n        Args:\n            commit_hash: Commit hash\n\n        Returns:\n            List of file paths\n        \"\"\"\n        try:\n            commit = self.repo.commit(commit_hash)\n            return list(commit.stats.files.keys())\n        except Exception as e:\n            logger.error(f\"Error getting changed files for {commit_hash}: {e}\")\n            return []\n\n    def get_file_content(self, file_path: str, commit_hash: Optional[str] = None) -> Optional[str]:\n        \"\"\"Get file content at a specific commit.\n\n        Args:\n            file_path: Relative path to file\n            commit_hash: Optional commit hash (defaults to HEAD)\n\n        Returns:\n            File content as string or None\n        \"\"\"\n        try:\n            if commit_hash:\n                commit = self.repo.commit(commit_hash)\n                blob = commit.tree / file_path\n            else:\n                blob = self.repo.head.commit.tree / file_path\n\n            return blob.data_stream.read().decode('utf-8', errors='ignore')\n        except Exception as e:\n            logger.warning(f\"Could not read file {file_path}: {e}\")\n            return None\n\n    def get_tracked_files(self, extensions: Optional[List[str]] = None) -> List[Path]:\n        \"\"\"Get list of tracked files in the repository.\n\n        Args:\n            extensions: Optional list of file extensions to filter (e.g., ['.py', '.js'])\n\n        Returns:\n            List of file paths relative to repo root\n        \"\"\"\n        files = []\n        try:\n            # Get all tracked files from the index\n            for item in self.repo.head.commit.tree.traverse():\n                if item.type == 'blob':  # It's a file\n                    file_path = Path(item.path)\n\n                    # Filter by extension if specified\n                    if extensions:\n                        if file_path.suffix.lower() in extensions:\n                            files.append(file_path)\n                    else:\n                        files.append(file_path)\n\n        except Exception as e:\n            logger.error(f\"Error getting tracked files: {e}\")\n\n        return files\n\n    def get_untracked_changes(self) -> List[str]:\n        \"\"\"Get list of untracked files.\n\n        Returns:\n            List of untracked file paths\n        \"\"\"\n        try:\n            return self.repo.untracked_files\n        except Exception as e:\n            logger.error(f\"Error getting untracked files: {e}\")\n            return []\n\n    def get_modified_files(self) -> List[str]:\n        \"\"\"Get list of modified but not committed files.\n\n        Returns:\n            List of modified file paths\n        \"\"\"\n        try:\n            # Get modified files (both staged and unstaged)\n            modified = [item.a_path for item in self.repo.index.diff(None)]\n            # Add staged files\n            staged = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n            return list(set(modified + staged))\n        except Exception as e:\n            logger.error(f\"Error getting modified files: {e}\")\n            return []\n\n    def get_repo_stats(self) -> Dict[str, Any]:\n        \"\"\"Get repository statistics.\n\n        Returns:\n            Dictionary with repository stats\n        \"\"\"\n        try:\n            latest_commit = self.get_latest_commit()\n            tracked_files = self.get_tracked_files()\n            modified_files = self.get_modified_files()\n            untracked_files = self.get_untracked_changes()\n\n            return {\n                \"path\": str(self.repo_path),\n                \"branch\": self.get_current_branch(),\n                \"total_commits\": len(list(self.repo.iter_commits(max_count=10000))),\n                \"latest_commit\": {\n                    \"hash\": latest_commit.hexsha if latest_commit else None,\n                    \"message\": latest_commit.message.strip() if latest_commit else None,\n                    \"author\": latest_commit.author.name if latest_commit else None,\n                    \"date\": datetime.fromtimestamp(latest_commit.committed_date) if latest_commit else None,\n                } if latest_commit else None,\n                \"total_files\": len(tracked_files),\n                \"modified_files\": len(modified_files),\n                \"untracked_files\": len(untracked_files),\n            }\n        except Exception as e:\n            logger.error(f\"Error getting repository stats: {e}\")\n            return {}\n\n    def is_file_ignored(self, file_path: str) -> bool:\n        \"\"\"Check if a file is ignored by .gitignore.\n\n        Args:\n            file_path: Relative path to file\n\n        Returns:\n            True if file is ignored\n        \"\"\"\n        try:\n            # Check if file matches any .gitignore pattern\n            return self.repo.git.check_ignore(file_path) is not None\n        except git.exc.GitCommandError:\n            return False\n\n    @staticmethod\n    def is_git_repository(path: str) -> bool:\n        \"\"\"Check if a path is a Git repository.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if it's a Git repository\n        \"\"\"\n        try:\n            git.Repo(path)\n            return True\n        except (git.exc.InvalidGitRepositoryError, git.exc.NoSuchPathError):\n            return False\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/git_ops.py", "relative_path": "services/rag-pipeline/src/core/git_ops.py", "filename": "git_ops.py", "extension": ".py"}}, {"id": "file_chunker_py_3e022fbc", "content": "\"\"\"Chunking strategies for code and text.\"\"\"\n\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass CodeChunker:\n    \"\"\"Chunk code and text for embedding.\"\"\"\n\n    def __init__(self, max_chunk_size: int = 1000, overlap: int = 50):\n        \"\"\"Initialize chunker.\n\n        Args:\n            max_chunk_size: Maximum chunk size in tokens (approximate with chars/4)\n            overlap: Overlap size in tokens for fixed-size chunks\n        \"\"\"\n        self.max_chunk_size = max_chunk_size\n        self.overlap = overlap\n        self.max_chars = max_chunk_size * 4  # Rough estimate: 1 token \u2248 4 chars\n        self.overlap_chars = overlap * 4\n\n    def chunk_code(self, parsed_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Chunk parsed code, splitting large chunks if necessary.\n\n        Args:\n            parsed_chunks: List of parsed code chunks from parser\n\n        Returns:\n            List of processed chunks (may include sub-chunks)\n        \"\"\"\n        processed_chunks = []\n\n        for chunk in parsed_chunks:\n            code = chunk['code']\n            code_length = len(code)\n\n            # If chunk is small enough, keep as-is\n            if code_length <= self.max_chars:\n                processed_chunks.append(self._finalize_chunk(chunk))\n            else:\n                # Split large chunks with overlap\n                logger.debug(f\"Splitting large chunk: {chunk['name']} ({code_length} chars)\")\n                sub_chunks = self._split_with_overlap(chunk)\n                processed_chunks.extend(sub_chunks)\n\n        return processed_chunks\n\n    def chunk_text(self, content: str, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Chunk plain text files (markdown, documentation, etc.).\n\n        Args:\n            content: Text content\n            file_path: File path\n\n        Returns:\n            List of text chunks\n        \"\"\"\n        chunks = []\n\n        # For markdown, try to split by sections (headers)\n        if file_path.suffix.lower() in ['.md', '.markdown']:\n            chunks = self._chunk_markdown(content, file_path)\n        else:\n            # Generic text chunking with overlap\n            chunks = self._chunk_generic_text(content, file_path)\n\n        return chunks\n\n    def _split_with_overlap(self, chunk: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Smart splitting at logical boundaries (empty lines, comments, block endings).\n\n        Args:\n            chunk: Original chunk\n\n        Returns:\n            List of sub-chunks\n        \"\"\"\n        code = chunk['code']\n        lines = code.split('\\n')\n        sub_chunks = []\n\n        # Calculate lines per chunk (approximate)\n        avg_line_length = len(code) / len(lines) if lines else 100\n        lines_per_chunk = int(self.max_chars / avg_line_length) if avg_line_length > 0 else 50\n\n        # Find logical split points\n        split_points = self._find_split_points(lines, lines_per_chunk)\n\n        prev_end = 0\n        for idx, split_point in enumerate(split_points, 1):\n            # Add overlap from previous chunk\n            overlap_start = max(0, prev_end - int(self.overlap_chars / avg_line_length))\n            chunk_lines = lines[overlap_start:split_point]\n\n            sub_code = '\\n'.join(chunk_lines)\n            sub_chunk = chunk.copy()\n            sub_chunk['code'] = sub_code\n            sub_chunk['name'] = f\"{chunk['name']}_part{idx}\"\n            sub_chunk['start_line'] = chunk['start_line'] + overlap_start\n            sub_chunk['end_line'] = chunk['start_line'] + split_point - 1\n            sub_chunk['line_count'] = len(chunk_lines)\n            sub_chunk['is_partial'] = True\n            sub_chunk['part_number'] = idx\n            sub_chunk['parent_chunk'] = chunk['name']\n\n            # Preserve signature in split chunks\n            if 'signature' in chunk:\n                sub_chunk['parent_signature'] = chunk['signature']\n            if 'docstring' in chunk and idx == 1:\n                sub_chunk['docstring'] = chunk['docstring']\n\n            sub_chunks.append(self._finalize_chunk(sub_chunk))\n            prev_end = split_point\n\n        return sub_chunks\n\n    def _find_split_points(self, lines: List[str], target_chunk_size: int) -> List[int]:\n        \"\"\"Find logical split points (empty lines, comments, block endings).\n\n        Args:\n            lines: List of code lines\n            target_chunk_size: Target number of lines per chunk\n\n        Returns:\n            List of line indices where splits should occur\n        \"\"\"\n        split_points = []\n        current_pos = 0\n\n        while current_pos < len(lines):\n            target_pos = min(current_pos + target_chunk_size, len(lines))\n\n            # Search for best split point within a window around target position\n            window = 10\n            search_start = max(current_pos + target_chunk_size - window, current_pos)\n            search_end = min(target_pos + window, len(lines))\n\n            best_split = target_pos\n            best_score = 0\n\n            for i in range(search_start, search_end):\n                score = self._score_split_point(lines, i)\n                if score > best_score:\n                    best_score = score\n                    best_split = i\n\n            split_points.append(best_split)\n            current_pos = best_split\n\n        return split_points\n\n    def _score_split_point(self, lines: List[str], idx: int) -> int:\n        \"\"\"Score split point quality (higher = better).\n\n        Args:\n            lines: List of code lines\n            idx: Line index to score\n\n        Returns:\n            Score (0-10, higher is better)\n        \"\"\"\n        if idx >= len(lines):\n            return 0\n\n        line = lines[idx].strip()\n\n        # Empty line - perfect split\n        if not line:\n            return 10\n\n        # Comment - good split\n        if line.startswith('#') or line.startswith('//') or line.startswith('/*'):\n            return 8\n\n        # Check for dedent (block boundary)\n        if idx + 1 < len(lines):\n            current_indent = len(lines[idx]) - len(lines[idx].lstrip())\n            next_indent = len(lines[idx + 1]) - len(lines[idx + 1].lstrip())\n            if next_indent < current_indent:\n                return 7\n\n        # Closing brace/bracket/return\n        if line in ['}', ')', ']'] or line.startswith('return') or line.startswith('}'):\n            return 6\n\n        # Regular line\n        return 1\n\n    def _chunk_markdown(self, content: str, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Chunk markdown content by sections (headers).\n\n        Args:\n            content: Markdown content\n            file_path: File path\n\n        Returns:\n            List of markdown chunks\n        \"\"\"\n        chunks = []\n        lines = content.split('\\n')\n        current_section = []\n        section_header = None\n        start_line = 1\n\n        for i, line in enumerate(lines, 1):\n            # Detect markdown headers (# Header)\n            if line.strip().startswith('#'):\n                # Save previous section\n                if current_section:\n                    section_text = '\\n'.join(current_section)\n                    if len(section_text.strip()) > 0:\n                        chunks.append({\n                            'code': section_text,\n                            'chunk_type': 'section',\n                            'name': section_header or 'intro',\n                            'file_path': str(file_path),\n                            'language': 'markdown',\n                            'start_line': start_line,\n                            'end_line': i - 1,\n                            'line_count': len(current_section)\n                        })\n\n                # Start new section\n                section_header = line.strip().lstrip('#').strip()\n                current_section = [line]\n                start_line = i\n            else:\n                current_section.append(line)\n\n            # Split if section gets too large\n            if len('\\n'.join(current_section)) > self.max_chars and current_section:\n                section_text = '\\n'.join(current_section)\n                chunks.append({\n                    'code': section_text,\n                    'chunk_type': 'section',\n                    'name': section_header or 'content',\n                    'file_path': str(file_path),\n                    'language': 'markdown',\n                    'start_line': start_line,\n                    'end_line': i,\n                    'line_count': len(current_section)\n                })\n                current_section = []\n                start_line = i + 1\n\n        # Add final section\n        if current_section:\n            section_text = '\\n'.join(current_section)\n            if len(section_text.strip()) > 0:\n                chunks.append({\n                    'code': section_text,\n                    'chunk_type': 'section',\n                    'name': section_header or 'content',\n                    'file_path': str(file_path),\n                    'language': 'markdown',\n                    'start_line': start_line,\n                    'end_line': len(lines),\n                    'line_count': len(current_section)\n                })\n\n        return [self._finalize_chunk(chunk) for chunk in chunks]\n\n    def _chunk_generic_text(self, content: str, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Chunk generic text with fixed-size and overlap.\n\n        Args:\n            content: Text content\n            file_path: File path\n\n        Returns:\n            List of text chunks\n        \"\"\"\n        chunks = []\n        lines = content.split('\\n')\n\n        # Estimate lines per chunk\n        avg_line_length = len(content) / len(lines) if lines else 100\n        lines_per_chunk = int(self.max_chars / avg_line_length) if avg_line_length > 0 else 50\n        overlap_lines = int(self.overlap_chars / avg_line_length) if avg_line_length > 0 else 5\n\n        i = 0\n        part_num = 1\n        while i < len(lines):\n            end_idx = min(i + lines_per_chunk, len(lines))\n            chunk_lines = lines[i:end_idx]\n            chunk_text = '\\n'.join(chunk_lines)\n\n            chunks.append({\n                'code': chunk_text,\n                'chunk_type': 'text',\n                'name': f\"{file_path.stem}_part{part_num}\",\n                'file_path': str(file_path),\n                'language': 'text',\n                'start_line': i + 1,\n                'end_line': end_idx,\n                'line_count': len(chunk_lines)\n            })\n\n            i += lines_per_chunk - overlap_lines\n            part_num += 1\n\n        return [self._finalize_chunk(chunk) for chunk in chunks]\n\n    def _finalize_chunk(self, chunk: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Finalize chunk by adding computed fields and complexity metrics (Phase 2).\n\n        Args:\n            chunk: Chunk dictionary\n\n        Returns:\n            Finalized chunk with additional metadata\n        \"\"\"\n        code = chunk['code']\n\n        # Existing metrics\n        chunk['char_count'] = len(code)\n        chunk['token_count_estimate'] = chunk['char_count'] // 4\n        chunk['preview'] = code[:100] + '...' if len(code) > 100 else code\n\n        # Phase 2: Complexity estimation\n        lines = [l for l in code.split('\\n') if l.strip() and not l.strip().startswith('#')]\n\n        # Base complexity: lines of code\n        complexity = len(lines)\n\n        # Conditionals add complexity\n        complexity += code.count('if ') * 2\n        complexity += code.count('elif ') * 2\n        complexity += code.count('else:') * 1\n\n        # Loops add significant complexity\n        complexity += code.count('for ') * 3\n        complexity += code.count('while ') * 3\n\n        # Error handling adds complexity\n        complexity += code.count('try:') * 2\n        complexity += code.count('except ') * 2\n\n        # Function calls and complexity indicators\n        complexity += code.count('lambda ') * 2\n        complexity += code.count('yield ') * 2\n\n        chunk['complexity_estimate'] = min(complexity, 1000)  # Cap at 1000\n        chunk['loc'] = len(lines)  # Lines of code (non-comment, non-blank)\n\n        # Phase 2: Function classification (if it's a function chunk)\n        if chunk.get('chunk_type') == 'function':\n            chunk['is_public'] = not chunk.get('name', '').startswith('_')\n            chunk['is_property'] = '@property' in ' '.join(chunk.get('decorators', []))\n            chunk['is_async'] = 'async def' in chunk.get('signature', '')\n        else:\n            chunk['is_public'] = True  # Classes and other chunks default to public\n            chunk['is_property'] = False\n            chunk['is_async'] = False\n\n        return chunk\n\n    def should_index_file(self, file_path: Path) -> bool:\n        \"\"\"Determine if a file should be indexed.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            True if file should be indexed\n        \"\"\"\n        # Skip binary files\n        binary_extensions = {\n            '.pyc', '.pyo', '.so', '.dylib', '.dll', '.exe',\n            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.pdf',\n            '.zip', '.tar', '.gz', '.bz2', '.xz',\n            '.db', '.sqlite', '.sqlite3'\n        }\n\n        if file_path.suffix.lower() in binary_extensions:\n            return False\n\n        # Skip hidden files (except .gitignore, etc.)\n        if file_path.name.startswith('.') and file_path.name not in {'.gitignore', '.env.example'}:\n            return False\n\n        # Skip common non-code directories\n        skip_dirs = {'node_modules', '__pycache__', '.git', '.venv', 'venv', 'env', 'dist', 'build'}\n        if any(part in skip_dirs for part in file_path.parts):\n            return False\n\n        return True\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/chunker.py", "relative_path": "services/rag-pipeline/src/core/chunker.py", "filename": "chunker.py", "extension": ".py"}}, {"id": "file_vector_store_py_a4694db6", "content": "\"\"\"ChromaDB vector store interface for code embeddings.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nimport chromadb\nfrom chromadb.config import Settings as ChromaSettings\nfrom chromadb.utils import embedding_functions\n\nlogger = logging.getLogger(__name__)\n\n\nclass VectorStore:\n    \"\"\"Interface to ChromaDB for storing and retrieving code embeddings.\"\"\"\n\n    def __init__(self, host: str = \"chromadb\", port: int = 8000, embedding_model: Optional[str] = None):\n        \"\"\"Initialize ChromaDB client.\n\n        Args:\n            host: ChromaDB host\n            port: ChromaDB port\n            embedding_model: Sentence transformer model name (optional, for backward compatibility)\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.embedding_model = embedding_model\n\n        # Initialize ChromaDB client\n        try:\n            self.client = chromadb.HttpClient(\n                host=host,\n                port=port\n            )\n            logger.info(f\"Connected to ChromaDB at {host}:{port}\")\n\n            # Test connection\n            self.client.heartbeat()\n            logger.info(\"ChromaDB heartbeat successful\")\n\n        except Exception as e:\n            logger.error(f\"Failed to connect to ChromaDB: {e}\")\n            raise\n\n        # Initialize embedding function (optional - for backward compatibility)\n        # When embeddings are pre-computed, this won't be used\n        self.embedding_function = None\n        if embedding_model:\n            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n                model_name=embedding_model\n            )\n            logger.info(f\"Initialized embedding function: {embedding_model}\")\n        else:\n            logger.info(\"No embedding function initialized - will use pre-computed embeddings\")\n\n    def create_collection(self, collection_name: str, metadata: Optional[Dict[str, Any]] = None) -> Any:\n        \"\"\"Create or get a ChromaDB collection.\n\n        Args:\n            collection_name: Name of the collection\n            metadata: Optional metadata for the collection\n\n        Returns:\n            ChromaDB collection object\n        \"\"\"\n        try:\n            # ChromaDB requires non-empty metadata dict, provide default\n            collection_metadata = metadata if metadata else {\"description\": \"Code repository collection\"}\n\n            collection = self.client.get_or_create_collection(\n                name=collection_name,\n                embedding_function=self.embedding_function,\n                metadata=collection_metadata\n            )\n            logger.info(f\"Created/retrieved collection: {collection_name}\")\n            return collection\n        except Exception as e:\n            logger.error(f\"Failed to create collection {collection_name}: {e}\")\n            raise\n\n    def delete_collection(self, collection_name: str) -> bool:\n        \"\"\"Delete a ChromaDB collection.\n\n        Args:\n            collection_name: Name of the collection\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            self.client.delete_collection(name=collection_name)\n            logger.info(f\"Deleted collection: {collection_name}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to delete collection {collection_name}: {e}\")\n            return False\n\n    def add_chunks(\n        self,\n        collection_name: str,\n        chunks: List[Dict[str, Any]],\n        embeddings: Optional[Any] = None,\n        batch_size: int = 100\n    ) -> int:\n        \"\"\"Add code chunks to a collection with embeddings.\n\n        Args:\n            collection_name: Name of the collection\n            chunks: List of chunk dictionaries with 'code' and metadata\n            embeddings: Pre-computed embeddings (numpy array or list), optional\n            batch_size: Number of chunks to process at once\n\n        Returns:\n            Number of chunks added\n        \"\"\"\n        collection = self.create_collection(collection_name)\n\n        total_added = 0\n\n        # Process in batches\n        for i in range(0, len(chunks), batch_size):\n            batch = chunks[i:i + batch_size]\n\n            # Prepare data for ChromaDB\n            ids = []\n            documents = []\n            metadatas = []\n\n            for idx, chunk in enumerate(batch):\n                # Generate unique ID\n                chunk_id = f\"{collection_name}_{i + idx}_{chunk.get('name', 'unknown')}\"\n                ids.append(chunk_id)\n\n                # Document is the code content\n                documents.append(chunk['code'])\n\n                # Metadata (excluding the code itself to avoid duplication)\n                metadata = {\n                    # Phase 1: Basic metadata\n                    'file_path': chunk.get('file_path', ''),\n                    'chunk_type': chunk.get('chunk_type', 'unknown'),\n                    'name': chunk.get('name', 'unknown'),\n                    'language': chunk.get('language', 'unknown'),\n                    'start_line': chunk.get('start_line', 0),\n                    'end_line': chunk.get('end_line', 0),\n                    'line_count': chunk.get('line_count', 0),\n                    'char_count': chunk.get('char_count', 0),\n                    'token_count_estimate': chunk.get('token_count_estimate', 0),\n                    'is_uncommitted': chunk.get('is_uncommitted', False),\n                    'commit_hash': chunk.get('commit_hash', ''),\n                    'is_partial': chunk.get('is_partial', False),\n                    'part_number': chunk.get('part_number', 0),\n                    'parent_chunk': chunk.get('parent_chunk', ''),\n\n                    # Phase 2: Enhanced metadata - Signatures\n                    'signature': chunk.get('signature', ''),\n                    'signature_params': chunk.get('signature_params', ''),\n                    'signature_return': chunk.get('signature_return', ''),\n                    'param_count': chunk.get('param_count', 0),\n                    'parent_signature': chunk.get('parent_signature', ''),\n\n                    # Phase 2: Enhanced metadata - Decorators\n                    'decorators': ','.join(chunk.get('decorators', [])),\n\n                    # Phase 2: Enhanced metadata - Docstrings\n                    'docstring': chunk.get('docstring', ''),\n                    'docstring_full': chunk.get('docstring_full', '')[:500],  # Truncated to 500 chars\n                    'has_docstring': chunk.get('has_docstring', False),\n\n                    # Phase 2: Enhanced metadata - Imports\n                    'imports': ','.join(chunk.get('imports', []))[:500],  # Truncated to 500 chars\n\n                    # Phase 2: Enhanced metadata - Complexity\n                    'complexity_estimate': chunk.get('complexity_estimate', 0),\n                    'loc': chunk.get('loc', 0),\n\n                    # Phase 2: Enhanced metadata - Function classification\n                    'is_public': chunk.get('is_public', True),\n                    'is_property': chunk.get('is_property', False),\n                    'is_async': chunk.get('is_async', False),\n                }\n\n                # Convert all values to strings (ChromaDB metadata requirement)\n                metadatas.append({k: str(v) for k, v in metadata.items()})\n\n            try:\n                # Prepare batch embeddings if provided\n                batch_embeddings = None\n                if embeddings is not None:\n                    # Extract embeddings for this batch\n                    batch_embeddings = embeddings[i:i + batch_size].tolist()\n\n                # Add batch to collection with pre-computed embeddings or let ChromaDB compute them\n                if batch_embeddings is not None:\n                    collection.add(\n                        ids=ids,\n                        documents=documents,\n                        metadatas=metadatas,\n                        embeddings=batch_embeddings\n                    )\n                    logger.info(f\"Added batch {i // batch_size + 1}: {len(batch)} chunks with pre-computed embeddings to {collection_name}\")\n                else:\n                    # Fallback to ChromaDB's embedding function (backward compatibility)\n                    collection.add(\n                        ids=ids,\n                        documents=documents,\n                        metadatas=metadatas\n                    )\n                    logger.info(f\"Added batch {i // batch_size + 1}: {len(batch)} chunks (embeddings computed by ChromaDB) to {collection_name}\")\n\n                total_added += len(batch)\n\n            except Exception as e:\n                logger.error(f\"Failed to add batch to {collection_name}: {e}\")\n                raise\n\n        logger.info(f\"Successfully added {total_added} chunks to {collection_name}\")\n        return total_added\n\n    def query(\n        self,\n        collection_name: str,\n        query_text: Optional[str] = None,\n        query_embeddings: Optional[List[float]] = None,\n        n_results: int = 10,\n        where: Optional[Dict[str, Any]] = None,\n        where_document: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Query a collection for similar code chunks.\n\n        Args:\n            collection_name: Name of the collection\n            query_text: Query text (used if embedding_function is set)\n            query_embeddings: Pre-computed query embeddings (optional)\n            n_results: Number of results to return\n            where: Metadata filter (e.g., {\"language\": \"python\"})\n            where_document: Document content filter\n\n        Returns:\n            Dictionary with query results\n        \"\"\"\n        try:\n            collection = self.client.get_collection(\n                name=collection_name,\n                embedding_function=self.embedding_function\n            )\n\n            # Use pre-computed embeddings if provided, otherwise use query_text\n            if query_embeddings is not None:\n                results = collection.query(\n                    query_embeddings=[query_embeddings],\n                    n_results=n_results,\n                    where=where,\n                    where_document=where_document,\n                    include=['documents', 'metadatas', 'distances']\n                )\n            elif query_text is not None:\n                results = collection.query(\n                    query_texts=[query_text],\n                    n_results=n_results,\n                    where=where,\n                    where_document=where_document,\n                    include=['documents', 'metadatas', 'distances']\n                )\n            else:\n                raise ValueError(\"Either query_text or query_embeddings must be provided\")\n\n            logger.info(f\"Query returned {len(results['ids'][0])} results from {collection_name}\")\n            return results\n\n        except Exception as e:\n            logger.error(f\"Failed to query collection {collection_name}: {e}\")\n            raise\n\n    def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:\n        \"\"\"Get statistics for a collection.\n\n        Args:\n            collection_name: Name of the collection\n\n        Returns:\n            Dictionary with collection statistics\n        \"\"\"\n        try:\n            collection = self.client.get_collection(\n                name=collection_name,\n                embedding_function=self.embedding_function\n            )\n\n            count = collection.count()\n\n            return {\n                'name': collection_name,\n                'count': count,\n                'metadata': collection.metadata\n            }\n\n        except Exception as e:\n            logger.error(f\"Failed to get stats for {collection_name}: {e}\")\n            return {'name': collection_name, 'count': 0, 'error': str(e)}\n\n    def update_chunk(\n        self,\n        collection_name: str,\n        chunk_id: str,\n        code: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> bool:\n        \"\"\"Update a specific chunk in the collection.\n\n        Args:\n            collection_name: Name of the collection\n            chunk_id: ID of the chunk to update\n            code: Updated code content (optional)\n            metadata: Updated metadata (optional)\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            collection = self.client.get_collection(\n                name=collection_name,\n                embedding_function=self.embedding_function\n            )\n\n            update_data = {'ids': [chunk_id]}\n\n            if code is not None:\n                update_data['documents'] = [code]\n\n            if metadata is not None:\n                # Convert all values to strings\n                update_data['metadatas'] = [{k: str(v) for k, v in metadata.items()}]\n\n            collection.update(**update_data)\n            logger.info(f\"Updated chunk {chunk_id} in {collection_name}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to update chunk {chunk_id} in {collection_name}: {e}\")\n            return False\n\n    def delete_chunks(\n        self,\n        collection_name: str,\n        chunk_ids: Optional[List[str]] = None,\n        where: Optional[Dict[str, Any]] = None\n    ) -> bool:\n        \"\"\"Delete chunks from a collection.\n\n        Args:\n            collection_name: Name of the collection\n            chunk_ids: List of chunk IDs to delete (optional)\n            where: Metadata filter for deletion (optional)\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            collection = self.client.get_collection(\n                name=collection_name,\n                embedding_function=self.embedding_function\n            )\n\n            if chunk_ids:\n                collection.delete(ids=chunk_ids)\n                logger.info(f\"Deleted {len(chunk_ids)} chunks from {collection_name}\")\n            elif where:\n                collection.delete(where=where)\n                logger.info(f\"Deleted chunks matching filter from {collection_name}\")\n            else:\n                logger.warning(\"No deletion criteria provided\")\n                return False\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to delete chunks from {collection_name}: {e}\")\n            return False\n\n    def list_collections(self) -> List[str]:\n        \"\"\"List all collections in ChromaDB.\n\n        Returns:\n            List of collection names\n        \"\"\"\n        try:\n            collections = self.client.list_collections()\n            return [col.name for col in collections]\n        except Exception as e:\n            logger.error(f\"Failed to list collections: {e}\")\n            return []\n\n    def collection_exists(self, collection_name: str) -> bool:\n        \"\"\"Check if a collection exists.\n\n        Args:\n            collection_name: Name of the collection\n\n        Returns:\n            True if collection exists\n        \"\"\"\n        try:\n            self.client.get_collection(name=collection_name)\n            return True\n        except Exception:\n            return False\n\n    def clear_collection(self, collection_name: str) -> bool:\n        \"\"\"Clear all data from a collection without deleting it.\n\n        Args:\n            collection_name: Name of the collection\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            # Delete and recreate collection\n            self.delete_collection(collection_name)\n            self.create_collection(collection_name)\n            logger.info(f\"Cleared collection: {collection_name}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to clear collection {collection_name}: {e}\")\n            return False\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/vector_store.py", "relative_path": "services/rag-pipeline/src/core/vector_store.py", "filename": "vector_store.py", "extension": ".py"}}, {"id": "file___init___py_544f750e", "content": "\"\"\"Core modules for RAG pipeline.\"\"\"\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/__init__.py", "relative_path": "services/rag-pipeline/src/core/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_parser_py_0e9acc3e", "content": "\"\"\"Code parser using tree-sitter for AST-based parsing.\"\"\"\n\nimport tree_sitter\nfrom tree_sitter import Language, Parser\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass CodeParser:\n    \"\"\"Parse code files using tree-sitter to extract semantic chunks.\"\"\"\n\n    # Supported language file extensions\n    LANGUAGE_EXTENSIONS = {\n        '.py': 'python',\n        '.js': 'javascript',\n        '.jsx': 'javascript',\n        '.ts': 'typescript',\n        '.tsx': 'typescript',\n        '.java': 'java',\n        '.go': 'go',\n        '.rs': 'rust',\n        '.rb': 'ruby',\n        '.c': 'c',\n        '.cpp': 'cpp',\n        '.h': 'cpp',\n        '.hpp': 'cpp',\n    }\n\n    def __init__(self):\n        \"\"\"Initialize code parser.\n\n        Note: For initial implementation, we'll use a simplified approach.\n        Full tree-sitter integration requires building language libraries.\n        \"\"\"\n        self.parsers = {}\n        logger.info(\"Code parser initialized\")\n\n    def detect_language(self, file_path: Path) -> Optional[str]:\n        \"\"\"Detect programming language from file extension.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            Language name or None\n        \"\"\"\n        suffix = file_path.suffix.lower()\n        return self.LANGUAGE_EXTENSIONS.get(suffix)\n\n    def parse_file(self, file_path: Path, content: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse a code file and extract semantic chunks.\n\n        Args:\n            file_path: Path to file\n            content: File content\n\n        Returns:\n            List of parsed chunks with metadata\n        \"\"\"\n        language = self.detect_language(file_path)\n\n        if not language:\n            logger.debug(f\"Unsupported file type: {file_path.suffix}\")\n            return []\n\n        # For now, use a simplified parsing approach\n        # TODO: Integrate actual tree-sitter parsing\n        if language == 'python':\n            return self._parse_python_simple(content, file_path)\n        elif language in ['javascript', 'typescript']:\n            return self._parse_javascript_simple(content, file_path)\n        else:\n            return self._parse_generic(content, file_path, language)\n\n    def _parse_python_simple(self, content: str, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Simple Python parser (line-based) with enhanced metadata extraction.\n\n        This is a simplified implementation. For production, use tree-sitter AST parsing.\n\n        Args:\n            content: Python code\n            file_path: File path\n\n        Returns:\n            List of code chunks\n        \"\"\"\n        chunks = []\n        lines = content.split('\\n')\n        current_chunk = []\n        chunk_type = None\n        chunk_name = None\n        chunk_signature = None\n        chunk_decorators = []\n        chunk_docstring = None\n        start_line = 0\n\n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n\n            # Detect function or class definitions\n            if stripped.startswith('def ') or stripped.startswith('class '):\n                # Save previous chunk if exists\n                if current_chunk:\n                    chunks.append(self._create_chunk(\n                        '\\n'.join(current_chunk),\n                        chunk_type or 'code',\n                        chunk_name or 'unknown',\n                        file_path,\n                        start_line,\n                        i - 1,\n                        signature=chunk_signature,\n                        decorators=chunk_decorators,\n                        docstring=chunk_docstring\n                    ))\n\n                # Extract signature info for new chunk\n                sig_info = self._extract_function_signature(lines, i - 1)\n\n                # Start new chunk\n                current_chunk = [line]\n                start_line = i\n                chunk_decorators = sig_info['decorators']\n                chunk_signature = sig_info['signature']\n                chunk_docstring = sig_info['docstring']\n\n                if stripped.startswith('def '):\n                    chunk_type = 'function'\n                    chunk_name = stripped.split('(')[0].replace('def ', '').strip()\n                else:\n                    chunk_type = 'class'\n                    chunk_name = stripped.split('(')[0].replace('class ', '').replace(':', '').strip()\n            elif current_chunk:\n                current_chunk.append(line)\n\n        # Add final chunk\n        if current_chunk:\n            chunks.append(self._create_chunk(\n                '\\n'.join(current_chunk),\n                chunk_type or 'code',\n                chunk_name or 'unknown',\n                file_path,\n                start_line,\n                len(lines),\n                signature=chunk_signature,\n                decorators=chunk_decorators,\n                docstring=chunk_docstring\n            ))\n\n        return chunks\n\n    def _parse_javascript_simple(self, content: str, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Simple JavaScript/TypeScript parser (line-based).\n\n        Args:\n            content: JavaScript/TypeScript code\n            file_path: File path\n\n        Returns:\n            List of code chunks\n        \"\"\"\n        chunks = []\n        lines = content.split('\\n')\n        current_chunk = []\n        chunk_type = None\n        chunk_name = None\n        start_line = 0\n        brace_count = 0\n\n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n\n            # Detect function definitions\n            if 'function ' in stripped or '=>' in stripped or stripped.startswith('class '):\n                if current_chunk and brace_count == 0:\n                    # Save previous chunk\n                    chunks.append(self._create_chunk(\n                        '\\n'.join(current_chunk),\n                        chunk_type or 'code',\n                        chunk_name or 'unknown',\n                        file_path,\n                        start_line,\n                        i - 1\n                    ))\n                    current_chunk = []\n\n                start_line = i if not current_chunk else start_line\n                if 'function' in stripped:\n                    chunk_type = 'function'\n                    # Extract function name\n                    if 'function ' in stripped:\n                        chunk_name = stripped.split('function ')[1].split('(')[0].strip()\n                    else:\n                        chunk_name = stripped.split('=')[0].strip()\n                elif 'class ' in stripped:\n                    chunk_type = 'class'\n                    chunk_name = stripped.split('class ')[1].split('{')[0].strip()\n\n            if current_chunk or stripped:\n                current_chunk.append(line)\n                brace_count += stripped.count('{') - stripped.count('}')\n\n        # Add final chunk\n        if current_chunk:\n            chunks.append(self._create_chunk(\n                '\\n'.join(current_chunk),\n                chunk_type or 'code',\n                chunk_name or 'unknown',\n                file_path,\n                start_line,\n                len(lines)\n            ))\n\n        return chunks\n\n    def _parse_generic(self, content: str, file_path: Path, language: str) -> List[Dict[str, Any]]:\n        \"\"\"Generic parser for unsupported languages.\n\n        Args:\n            content: Code content\n            file_path: File path\n            language: Language name\n\n        Returns:\n            List with single chunk (entire file)\n        \"\"\"\n        return [self._create_chunk(\n            content,\n            'file',\n            file_path.stem,\n            file_path,\n            1,\n            len(content.split('\\n'))\n        )]\n\n    def _extract_function_signature(self, lines: List[str], start_idx: int) -> Dict[str, str]:\n        \"\"\"Extract complete function signature with decorators and docstring.\n\n        Args:\n            lines: All lines of code\n            start_idx: Index of the function/class definition line (0-based)\n\n        Returns:\n            Dictionary with signature, decorators, and docstring\n        \"\"\"\n        result = {\n            'signature': '',\n            'decorators': [],\n            'docstring': ''\n        }\n\n        # Look backward for decorators (@decorator)\n        i = start_idx - 1\n        while i >= 0 and lines[i].strip().startswith('@'):\n            result['decorators'].insert(0, lines[i].strip())\n            i -= 1\n\n        # Get function signature (may span multiple lines for long params)\n        sig_lines = []\n        paren_count = 0\n        i = start_idx\n        while i < len(lines):\n            line = lines[i]\n            sig_lines.append(line)\n            paren_count += line.count('(') - line.count(')')\n            if paren_count == 0 and ':' in line:\n                break\n            i += 1\n        result['signature'] = '\\n'.join(sig_lines)\n\n        # Extract docstring (if follows immediately)\n        next_idx = i + 1\n        if next_idx < len(lines):\n            next_line = lines[next_idx].strip()\n            if next_line.startswith('\"\"\"') or next_line.startswith(\"'''\"):\n                quote = '\"\"\"' if '\"\"\"' in next_line else \"'''\"\n                # Handle single-line and multi-line docstrings\n                if next_line.count(quote) >= 2:\n                    result['docstring'] = next_line.strip(quote).strip()\n                else:\n                    docstring_lines = [next_line.strip(quote)]\n                    i = next_idx + 1\n                    while i < len(lines) and quote not in lines[i]:\n                        docstring_lines.append(lines[i].strip())\n                        i += 1\n                    if i < len(lines):\n                        docstring_lines.append(lines[i].strip(quote))\n                    result['docstring'] = ' '.join(docstring_lines).strip()\n\n        return result\n\n    def _parse_signature_components(self, signature: str, language: str) -> Dict[str, str]:\n        \"\"\"Parse signature into searchable components.\n\n        Args:\n            signature: Function signature string\n            language: Programming language\n\n        Returns:\n            Dictionary with params, return_type, param_count\n        \"\"\"\n        result = {'params': '', 'return_type': '', 'param_count': '0'}\n\n        if not signature:\n            return result\n\n        if language == 'python':\n            # Extract parameters\n            if '(' in signature and ')' in signature:\n                params_str = signature[signature.index('(') + 1:signature.rindex(')')]\n                result['params'] = params_str.strip()\n                if params_str.strip():\n                    param_count = len([p for p in params_str.split(',') if p.strip() and p.strip() != 'self'])\n                    result['param_count'] = str(param_count)\n\n            # Extract return type\n            if '->' in signature:\n                return_type = signature.split('->')[1].split(':')[0].strip()\n                result['return_type'] = return_type\n\n        return result\n\n    def _extract_imports_from_chunk(self, code: str, language: str) -> List[str]:\n        \"\"\"Extract import statements relevant to this chunk.\n\n        Args:\n            code: Code chunk\n            language: Programming language\n\n        Returns:\n            List of import statements\n        \"\"\"\n        imports = []\n        lines = code.split('\\n')\n\n        for line in lines:\n            stripped = line.strip()\n            if language == 'python':\n                if stripped.startswith('import ') or stripped.startswith('from '):\n                    imports.append(stripped)\n            elif language in ['javascript', 'typescript']:\n                if stripped.startswith('import ') or stripped.startswith('require('):\n                    imports.append(stripped)\n\n        return imports\n\n    def _create_chunk(self, code: str, chunk_type: str, name: str,\n                      file_path: Path, start_line: int, end_line: int,\n                      signature: Optional[str] = None,\n                      decorators: Optional[List[str]] = None,\n                      docstring: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Create a code chunk with enhanced metadata (Phase 2).\n\n        Args:\n            code: Code content\n            chunk_type: Type of chunk (function, class, file, etc.)\n            name: Name of the chunk\n            file_path: File path\n            start_line: Starting line number\n            end_line: Ending line number\n            signature: Function/class signature (optional)\n            decorators: List of decorators (optional)\n            docstring: Docstring content (optional)\n\n        Returns:\n            Chunk dictionary with enhanced metadata\n        \"\"\"\n        language = self.detect_language(file_path)\n\n        chunk = {\n            'code': code,\n            'chunk_type': chunk_type,\n            'name': name,\n            'file_path': str(file_path),\n            'language': language,\n            'start_line': start_line,\n            'end_line': end_line,\n            'line_count': end_line - start_line + 1\n        }\n\n        # Phase 2: Enhanced metadata\n        if signature:\n            sig_components = self._parse_signature_components(signature, language)\n            chunk['signature'] = signature\n            chunk['signature_params'] = sig_components['params']\n            chunk['signature_return'] = sig_components['return_type']\n            chunk['param_count'] = sig_components['param_count']\n\n        if decorators:\n            chunk['decorators'] = decorators\n\n        if docstring:\n            chunk['docstring'] = docstring.split('\\n')[0][:200]  # First line, max 200 chars\n            chunk['docstring_full'] = docstring[:500]  # Full text, truncated\n            chunk['has_docstring'] = True\n\n        # Extract imports from chunk\n        imports = self._extract_imports_from_chunk(code, language)\n        if imports:\n            chunk['imports'] = imports\n\n        return chunk\n\n    def extract_imports(self, content: str, language: str) -> List[str]:\n        \"\"\"Extract import statements from code.\n\n        Args:\n            content: Code content\n            language: Programming language\n\n        Returns:\n            List of import statements\n        \"\"\"\n        imports = []\n        lines = content.split('\\n')\n\n        if language == 'python':\n            for line in lines:\n                stripped = line.strip()\n                if stripped.startswith('import ') or stripped.startswith('from '):\n                    imports.append(stripped)\n\n        elif language in ['javascript', 'typescript']:\n            for line in lines:\n                stripped = line.strip()\n                if stripped.startswith('import ') or stripped.startswith('require('):\n                    imports.append(stripped)\n\n        return imports\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/parser.py", "relative_path": "services/rag-pipeline/src/core/parser.py", "filename": "parser.py", "extension": ".py"}}, {"id": "file_embedder_py_28716c1f", "content": "\"\"\"Embedding generation using OpenAI.\"\"\"\n\nimport logging\nimport os\nimport time\nfrom typing import List, Optional\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom openai import OpenAI\nimport hashlib\nimport pickle\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseEmbedder(ABC):\n    \"\"\"Abstract base class for embedders.\"\"\"\n\n    @abstractmethod\n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"Generate embedding for a single text.\n\n        Args:\n            text: Text to embed\n\n        Returns:\n            Numpy array of embeddings\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def embed_batch(\n        self,\n        texts: List[str],\n        batch_size: int = 32,\n        show_progress: bool = True\n    ) -> np.ndarray:\n        \"\"\"Generate embeddings for a batch of texts.\n\n        Args:\n            texts: List of texts to embed\n            batch_size: Batch size for encoding\n            show_progress: Whether to show progress bar\n\n        Returns:\n            Numpy array of embeddings (n_texts x embedding_dim)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_embedding_dimension(self) -> int:\n        \"\"\"Get the embedding dimension.\n\n        Returns:\n            Embedding dimension\n        \"\"\"\n        pass\n\n    def embed_code_chunks(\n        self,\n        chunks: List[dict],\n        batch_size: int = 32\n    ) -> List[np.ndarray]:\n        \"\"\"Generate embeddings for code chunks.\n\n        Args:\n            chunks: List of chunk dictionaries with 'code' field\n            batch_size: Batch size for encoding\n\n        Returns:\n            List of embeddings (one per chunk)\n        \"\"\"\n        # Extract code from chunks\n        code_texts = [chunk.get('code', '') for chunk in chunks]\n\n        # Generate embeddings\n        embeddings = self.embed_batch(code_texts, batch_size=batch_size)\n\n        return embeddings\n\n    def compute_similarity(\n        self,\n        embedding1: np.ndarray,\n        embedding2: np.ndarray\n    ) -> float:\n        \"\"\"Compute cosine similarity between two embeddings.\n\n        Args:\n            embedding1: First embedding\n            embedding2: Second embedding\n\n        Returns:\n            Cosine similarity score (-1 to 1)\n        \"\"\"\n        # Normalize embeddings\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        # Compute cosine similarity\n        similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)\n        return float(similarity)\n\n\n# LocalEmbedder class removed - using OpenAI embeddings only\n# If you need local embeddings, add sentence-transformers to requirements.txt\n\n\nclass OpenAIEmbedder(BaseEmbedder):\n    \"\"\"Generate embeddings using OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = \"text-embedding-3-large\",\n        batch_size: int = 100,\n        max_retries: int = 3\n    ):\n        \"\"\"Initialize the OpenAI embedder.\n\n        Args:\n            api_key: OpenAI API key (if None, reads from OPENAI_API_KEY env var)\n            model: OpenAI embedding model name\n            batch_size: Batch size for API requests (OpenAI allows up to 2048)\n            max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OPENAI_API_KEY not set. Please set it in environment variables or pass it directly.\")\n\n        self.model_name = model\n        self.batch_size = batch_size\n        self.max_retries = max_retries\n\n        # Initialize OpenAI client\n        self.client = OpenAI(api_key=self.api_key)\n\n        # Set dimension based on model\n        if model == \"text-embedding-3-large\":\n            self._dimension = 3072\n        elif model == \"text-embedding-3-small\":\n            self._dimension = 1536\n        elif model == \"text-embedding-ada-002\":\n            self._dimension = 1536\n        else:\n            self._dimension = 1536  # Default\n\n        logger.info(f\"Initialized OpenAI embedder with model: {model} ({self._dimension} dimensions)\")\n\n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"Generate embedding for a single text.\n\n        Args:\n            text: Text to embed\n\n        Returns:\n            Numpy array of embeddings\n        \"\"\"\n        try:\n            response = self.client.embeddings.create(\n                input=text,\n                model=self.model_name\n            )\n            embedding = np.array(response.data[0].embedding)\n            return embedding\n        except Exception as e:\n            logger.error(f\"Failed to generate OpenAI embedding: {e}\")\n            raise\n\n    def embed_batch(\n        self,\n        texts: List[str],\n        batch_size: int = 32,\n        show_progress: bool = True\n    ) -> np.ndarray:\n        \"\"\"Generate embeddings for a batch of texts.\n\n        Args:\n            texts: List of texts to embed\n            batch_size: Batch size for encoding (will be capped to self.batch_size)\n            show_progress: Whether to show progress (logged)\n\n        Returns:\n            Numpy array of embeddings (n_texts x embedding_dim)\n        \"\"\"\n        embeddings = []\n        batch_size = min(batch_size, self.batch_size)\n\n        logger.info(f\"Generating OpenAI embeddings for {len(texts)} texts in batches of {batch_size}\")\n\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n\n            if show_progress:\n                logger.info(f\"Processing batch {i // batch_size + 1}/{(len(texts) + batch_size - 1) // batch_size}\")\n\n            # Retry logic with exponential backoff\n            for attempt in range(self.max_retries):\n                try:\n                    response = self.client.embeddings.create(\n                        input=batch,\n                        model=self.model_name\n                    )\n\n                    # Extract embeddings from response\n                    batch_embeddings = [np.array(data.embedding) for data in response.data]\n                    embeddings.extend(batch_embeddings)\n                    break  # Success, exit retry loop\n\n                except Exception as e:\n                    if attempt == self.max_retries - 1:\n                        logger.error(f\"Failed to generate embeddings after {self.max_retries} attempts: {e}\")\n                        raise\n                    else:\n                        wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n                        logger.warning(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}. Waiting {wait_time}s...\")\n                        time.sleep(wait_time)\n\n        logger.info(f\"Generated {len(embeddings)} OpenAI embeddings\")\n        return np.array(embeddings)\n\n    def get_embedding_dimension(self) -> int:\n        \"\"\"Get the embedding dimension.\n\n        Returns:\n            Embedding dimension\n        \"\"\"\n        return self._dimension\n\n    def get_model_info(self) -> dict:\n        \"\"\"Get information about the model.\n\n        Returns:\n            Dictionary with model information\n        \"\"\"\n        return {\n            'provider': 'openai',\n            'model_name': self.model_name,\n            'embedding_dimension': self._dimension,\n            'batch_size': self.batch_size,\n            'max_retries': self.max_retries\n        }\n\n\ndef create_embedder(\n    provider: str = \"openai\",\n    model_name: Optional[str] = None,\n    **kwargs\n) -> BaseEmbedder:\n    \"\"\"Factory function to create an embedder based on provider.\n\n    Args:\n        provider: Only 'openai' is supported\n        model_name: Model name (optional, uses defaults)\n        **kwargs: Additional arguments for embedder\n\n    Returns:\n        Embedder instance\n\n    Raises:\n        ValueError: If provider is unknown\n    \"\"\"\n    provider = provider.lower()\n\n    if provider == \"openai\":\n        default_model = \"text-embedding-3-large\"\n        return OpenAIEmbedder(\n            model=model_name or default_model,\n            **kwargs\n        )\n    else:\n        raise ValueError(f\"Unknown provider: {provider}. Only 'openai' is supported. For local embeddings, add sentence-transformers to requirements.txt\")\n\n\n# Utility functions for backward compatibility and caching\n\ndef get_cache_key(text: str) -> str:\n    \"\"\"Generate a cache key for a text.\n\n    Args:\n        text: Text to hash\n\n    Returns:\n        MD5 hash of the text\n    \"\"\"\n    return hashlib.md5(text.encode('utf-8')).hexdigest()\n\n\ndef save_embeddings(embeddings: np.ndarray, file_path: str) -> bool:\n    \"\"\"Save embeddings to disk.\n\n    Args:\n        embeddings: Numpy array of embeddings\n        file_path: Path to save embeddings\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n\n        with open(file_path, 'wb') as f:\n            pickle.dump(embeddings, f)\n\n        logger.info(f\"Saved embeddings to {file_path}\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"Failed to save embeddings: {e}\")\n        return False\n\n\ndef load_embeddings(file_path: str) -> Optional[np.ndarray]:\n    \"\"\"Load embeddings from disk.\n\n    Args:\n        file_path: Path to embeddings file\n\n    Returns:\n        Numpy array of embeddings or None if failed\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            embeddings = pickle.load(f)\n\n        logger.info(f\"Loaded embeddings from {file_path}\")\n        return embeddings\n\n    except Exception as e:\n        logger.error(f\"Failed to load embeddings: {e}\")\n        return None\n\n\ndef preprocess_code(code: str, max_length: int = 512) -> str:\n    \"\"\"Preprocess code for embedding.\n\n    Args:\n        code: Code text\n        max_length: Maximum length in tokens (approximate)\n\n    Returns:\n        Preprocessed code\n    \"\"\"\n    # Remove excessive whitespace\n    lines = code.split('\\n')\n    lines = [line.rstrip() for line in lines]\n    code = '\\n'.join(lines)\n\n    # Truncate if too long (rough estimate: 1 token \u2248 4 chars)\n    max_chars = max_length * 4\n    if len(code) > max_chars:\n        code = code[:max_chars]\n        logger.debug(f\"Truncated code to {max_chars} characters\")\n\n    return code\n\n\n# For backward compatibility - use OpenAIEmbedder directly\nEmbedder = OpenAIEmbedder\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/core/embedder.py", "relative_path": "services/rag-pipeline/src/core/embedder.py", "filename": "embedder.py", "extension": ".py"}}, {"id": "file_reranker_py_6d71104d", "content": "\"\"\"Reranking algorithms for improving retrieval quality.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\nclass Reranker:\n    \"\"\"Rerank retrieved chunks to improve diversity and relevance.\"\"\"\n\n    def __init__(self, embedder=None):\n        \"\"\"Initialize reranker.\n\n        Args:\n            embedder: Optional embedder for computing embeddings\n        \"\"\"\n        self.embedder = embedder\n        logger.info(\"Reranker initialized\")\n\n    def mmr_rerank(\n        self,\n        chunks: List[Dict[str, Any]],\n        query_embedding: Optional[np.ndarray] = None,\n        lambda_param: float = 0.5,\n        top_k: Optional[int] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Rerank using Maximal Marginal Relevance (MMR).\n\n        MMR balances relevance and diversity:\n        - High relevance: Results similar to query\n        - High diversity: Results different from each other\n\n        Args:\n            chunks: List of retrieved chunks with 'code' field\n            query_embedding: Query embedding (optional, uses similarity from chunks)\n            lambda_param: Trade-off between relevance and diversity (0-1)\n                         1.0 = pure relevance, 0.0 = pure diversity\n            top_k: Number of results to return (None = all)\n\n        Returns:\n            Reranked list of chunks\n        \"\"\"\n        if not chunks:\n            return []\n\n        logger.info(f\"MMR reranking {len(chunks)} chunks (\u03bb={lambda_param})\")\n\n        # If we don't have embeddings, fall back to similarity scores\n        if query_embedding is None and 'similarity' in chunks[0]:\n            return self._mmr_rerank_by_similarity(chunks, lambda_param, top_k)\n\n        # Need embedder to compute new embeddings\n        if self.embedder is None:\n            logger.warning(\"No embedder available, returning original order\")\n            return chunks[:top_k] if top_k else chunks\n\n        try:\n            # Generate embeddings for all chunks\n            chunk_texts = [chunk['code'] for chunk in chunks]\n            chunk_embeddings = self.embedder.embed_batch(\n                chunk_texts,\n                show_progress=False\n            )\n\n            # Compute MMR\n            selected_indices = self._mmr_select(\n                query_embedding,\n                chunk_embeddings,\n                lambda_param,\n                top_k or len(chunks)\n            )\n\n            # Reorder chunks\n            reranked = [chunks[i] for i in selected_indices]\n\n            # Add MMR scores\n            for idx, chunk in enumerate(reranked):\n                chunk['mmr_rank'] = idx + 1\n                chunk['mmr_score'] = 1.0 - (idx / len(reranked))\n\n            logger.info(f\"MMR reranking complete: {len(reranked)} results\")\n            return reranked\n\n        except Exception as e:\n            logger.error(f\"MMR reranking failed: {e}\")\n            return chunks[:top_k] if top_k else chunks\n\n    def _mmr_rerank_by_similarity(\n        self,\n        chunks: List[Dict[str, Any]],\n        lambda_param: float,\n        top_k: Optional[int]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Simplified MMR using pre-computed similarity scores.\n\n        Args:\n            chunks: Chunks with 'similarity' field\n            lambda_param: Relevance vs diversity trade-off\n            top_k: Number of results\n\n        Returns:\n            Reranked chunks\n        \"\"\"\n        if not chunks:\n            return []\n\n        # Sort by similarity initially\n        sorted_chunks = sorted(\n            chunks,\n            key=lambda x: x.get('similarity', 0.0),\n            reverse=True\n        )\n\n        selected = []\n        remaining = sorted_chunks.copy()\n        k = top_k or len(chunks)\n\n        # Select first (most relevant)\n        if remaining:\n            selected.append(remaining.pop(0))\n\n        # Iteratively select remaining\n        while len(selected) < k and remaining:\n            max_mmr_score = -float('inf')\n            max_idx = 0\n\n            for idx, candidate in enumerate(remaining):\n                # Relevance score\n                relevance = candidate.get('similarity', 0.0)\n\n                # Diversity: max similarity to already selected\n                max_sim_to_selected = max(\n                    self._text_similarity(\n                        candidate['code'],\n                        sel['code']\n                    )\n                    for sel in selected\n                )\n\n                # MMR score\n                mmr_score = (\n                    lambda_param * relevance -\n                    (1 - lambda_param) * max_sim_to_selected\n                )\n\n                if mmr_score > max_mmr_score:\n                    max_mmr_score = mmr_score\n                    max_idx = idx\n\n            selected.append(remaining.pop(max_idx))\n\n        return selected\n\n    def _mmr_select(\n        self,\n        query_embedding: np.ndarray,\n        document_embeddings: np.ndarray,\n        lambda_param: float,\n        k: int\n    ) -> List[int]:\n        \"\"\"Select k document indices using MMR algorithm.\n\n        Args:\n            query_embedding: Query embedding vector\n            document_embeddings: Matrix of document embeddings (n_docs x dim)\n            lambda_param: Relevance vs diversity parameter\n            k: Number of documents to select\n\n        Returns:\n            List of selected document indices\n        \"\"\"\n        # Compute similarities to query\n        query_sims = self._cosine_similarity_batch(\n            query_embedding,\n            document_embeddings\n        )\n\n        # Initialize\n        selected_indices = []\n        remaining_indices = list(range(len(document_embeddings)))\n\n        # Select first document (most relevant)\n        first_idx = int(np.argmax(query_sims))\n        selected_indices.append(first_idx)\n        remaining_indices.remove(first_idx)\n\n        # Iteratively select remaining documents\n        while len(selected_indices) < k and remaining_indices:\n            mmr_scores = []\n\n            for idx in remaining_indices:\n                # Relevance: similarity to query\n                relevance = query_sims[idx]\n\n                # Diversity: max similarity to selected documents\n                if selected_indices:\n                    selected_embeddings = document_embeddings[selected_indices]\n                    doc_sims = self._cosine_similarity_batch(\n                        document_embeddings[idx],\n                        selected_embeddings\n                    )\n                    max_sim = np.max(doc_sims)\n                else:\n                    max_sim = 0.0\n\n                # MMR score\n                mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim\n                mmr_scores.append(mmr_score)\n\n            # Select document with highest MMR score\n            best_idx = remaining_indices[int(np.argmax(mmr_scores))]\n            selected_indices.append(best_idx)\n            remaining_indices.remove(best_idx)\n\n        return selected_indices\n\n    def _cosine_similarity_batch(\n        self,\n        vec1: np.ndarray,\n        vec2_batch: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"Compute cosine similarity between one vector and a batch.\n\n        Args:\n            vec1: Single vector (dim,)\n            vec2_batch: Batch of vectors (n x dim)\n\n        Returns:\n            Array of similarity scores (n,)\n        \"\"\"\n        # Ensure vec1 is 1D\n        if vec1.ndim == 2:\n            vec1 = vec1.flatten()\n\n        # Ensure vec2_batch is 2D\n        if vec2_batch.ndim == 1:\n            vec2_batch = vec2_batch.reshape(1, -1)\n\n        # Normalize vectors\n        vec1_norm = vec1 / (np.linalg.norm(vec1) + 1e-8)\n        vec2_norms = vec2_batch / (\n            np.linalg.norm(vec2_batch, axis=1, keepdims=True) + 1e-8\n        )\n\n        # Compute dot product\n        similarities = np.dot(vec2_norms, vec1_norm)\n\n        return similarities\n\n    def _text_similarity(self, text1: str, text2: str) -> float:\n        \"\"\"Simple text similarity using character overlap.\n\n        Args:\n            text1: First text\n            text2: Second text\n\n        Returns:\n            Similarity score (0-1)\n        \"\"\"\n        # Simple character-level Jaccard similarity\n        set1 = set(text1.lower())\n        set2 = set(text2.lower())\n\n        intersection = len(set1 & set2)\n        union = len(set1 | set2)\n\n        if union == 0:\n            return 0.0\n\n        return intersection / union\n\n    def diversity_rerank(\n        self,\n        chunks: List[Dict[str, Any]],\n        top_k: Optional[int] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Rerank to maximize diversity (spread across files/types).\n\n        Args:\n            chunks: List of chunks\n            top_k: Number of results\n\n        Returns:\n            Reranked chunks with diverse sources\n        \"\"\"\n        if not chunks:\n            return []\n\n        logger.info(f\"Diversity reranking {len(chunks)} chunks\")\n\n        # Track what we've selected\n        selected = []\n        seen_files = set()\n        seen_types = set()\n\n        k = top_k or len(chunks)\n\n        # Sort by similarity initially\n        sorted_chunks = sorted(\n            chunks,\n            key=lambda x: x.get('similarity', 0.0),\n            reverse=True\n        )\n\n        # First pass: one chunk per file\n        for chunk in sorted_chunks:\n            file_path = chunk.get('file_path', '')\n\n            if file_path not in seen_files:\n                selected.append(chunk)\n                seen_files.add(file_path)\n                seen_types.add(chunk.get('chunk_type', ''))\n\n                if len(selected) >= k:\n                    break\n\n        # Second pass: different types from same files\n        if len(selected) < k:\n            for chunk in sorted_chunks:\n                if chunk in selected:\n                    continue\n\n                chunk_type = chunk.get('chunk_type', '')\n                chunk_key = (chunk.get('file_path', ''), chunk_type)\n\n                # Select if different type in same file\n                if chunk_key not in {(c.get('file_path', ''), c.get('chunk_type', '')) for c in selected}:\n                    selected.append(chunk)\n\n                    if len(selected) >= k:\n                        break\n\n        # Third pass: fill remaining with most relevant\n        if len(selected) < k:\n            for chunk in sorted_chunks:\n                if chunk not in selected:\n                    selected.append(chunk)\n\n                    if len(selected) >= k:\n                        break\n\n        logger.info(f\"Diversity reranking complete: {len(selected)} results from {len(seen_files)} files\")\n        return selected\n\n    def reciprocal_rank_fusion(\n        self,\n        result_lists: List[List[Dict[str, Any]]],\n        k: int = 60,\n        top_k: Optional[int] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Combine multiple ranking lists using Reciprocal Rank Fusion.\n\n        Args:\n            result_lists: List of ranked result lists\n            k: Constant for RRF formula (default 60)\n            top_k: Number of final results\n\n        Returns:\n            Fused ranking\n        \"\"\"\n        if not result_lists:\n            return []\n\n        logger.info(f\"RRF fusion of {len(result_lists)} result lists\")\n\n        # Compute RRF scores\n        rrf_scores = {}\n\n        for result_list in result_lists:\n            for rank, chunk in enumerate(result_list):\n                chunk_id = chunk.get('id', chunk.get('code', '')[:50])\n\n                if chunk_id not in rrf_scores:\n                    rrf_scores[chunk_id] = {\n                        'chunk': chunk,\n                        'score': 0.0\n                    }\n\n                # RRF formula: 1 / (k + rank)\n                rrf_scores[chunk_id]['score'] += 1.0 / (k + rank + 1)\n\n        # Sort by RRF score\n        sorted_items = sorted(\n            rrf_scores.values(),\n            key=lambda x: x['score'],\n            reverse=True\n        )\n\n        # Extract chunks\n        fused_results = [item['chunk'] for item in sorted_items]\n\n        # Add RRF scores\n        for idx, chunk in enumerate(fused_results):\n            chunk['rrf_score'] = sorted_items[idx]['score']\n            chunk['rrf_rank'] = idx + 1\n\n        if top_k:\n            fused_results = fused_results[:top_k]\n\n        logger.info(f\"RRF fusion complete: {len(fused_results)} results\")\n        return fused_results\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/retrieval/reranker.py", "relative_path": "services/rag-pipeline/src/retrieval/reranker.py", "filename": "reranker.py", "extension": ".py"}}, {"id": "file_retriever_py_8c517eda", "content": "\"\"\"RAG retrieval module for semantic code search.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\n\nfrom ..core.vector_store import VectorStore\nfrom ..core.embedder import Embedder\n\nlogger = logging.getLogger(__name__)\n\n\nclass CodeRetriever:\n    \"\"\"Retrieve relevant code chunks using semantic search.\"\"\"\n\n    def __init__(\n        self,\n        vector_store: VectorStore,\n        embedder: Embedder\n    ):\n        \"\"\"Initialize retriever.\n\n        Args:\n            vector_store: Vector store instance\n            embedder: Embedder instance\n        \"\"\"\n        self.vector_store = vector_store\n        self.embedder = embedder\n\n        logger.info(\"Code retriever initialized\")\n\n    def retrieve(\n        self,\n        collection_name: str,\n        query: str,\n        n_results: int = 10,\n        filters: Optional[Dict[str, Any]] = None,\n        min_similarity: float = 0.0\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve relevant code chunks for a query.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            n_results: Maximum number of results to return\n            filters: Metadata filters (e.g., {\"language\": \"python\"})\n            min_similarity: Minimum similarity threshold (0-1)\n\n        Returns:\n            List of retrieved chunks with metadata and scores\n        \"\"\"\n        logger.info(f\"Retrieving {n_results} chunks for query: {query[:100]}\")\n\n        try:\n            # Generate query embedding\n            query_embedding = self.embedder.embed_text(query)\n\n            # Convert numpy array to list for ChromaDB\n            query_embedding_list = query_embedding.tolist() if hasattr(query_embedding, 'tolist') else list(query_embedding)\n\n            # Query vector store with pre-computed embedding\n            results = self.vector_store.query(\n                collection_name=collection_name,\n                query_embeddings=query_embedding_list,\n                n_results=n_results,\n                where=filters\n            )\n\n            # Format results\n            chunks = self._format_results(results, min_similarity)\n\n            logger.info(f\"Retrieved {len(chunks)} relevant chunks\")\n            return chunks\n\n        except Exception as e:\n            logger.error(f\"Retrieval failed: {e}\")\n            raise\n\n    def retrieve_with_context(\n        self,\n        collection_name: str,\n        query: str,\n        n_results: int = 10,\n        context_lines: int = 5,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve chunks with additional context from surrounding code.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            n_results: Maximum number of results\n            context_lines: Number of lines before/after to include\n            filters: Metadata filters\n\n        Returns:\n            List of chunks with expanded context\n        \"\"\"\n        # Get base results\n        chunks = self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results,\n            filters=filters\n        )\n\n        # TODO: Add logic to fetch surrounding chunks from same file\n        # For now, return as-is\n        return chunks\n\n    def retrieve_by_file(\n        self,\n        collection_name: str,\n        query: str,\n        file_path: str,\n        n_results: int = 5\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve chunks from a specific file.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            file_path: File path to search within\n            n_results: Maximum number of results\n\n        Returns:\n            List of chunks from the specified file\n        \"\"\"\n        filters = {\"file_path\": file_path}\n\n        return self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results,\n            filters=filters\n        )\n\n    def retrieve_by_language(\n        self,\n        collection_name: str,\n        query: str,\n        language: str,\n        n_results: int = 10\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve chunks of a specific programming language.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            language: Programming language (e.g., \"python\", \"javascript\")\n            n_results: Maximum number of results\n\n        Returns:\n            List of chunks in the specified language\n        \"\"\"\n        filters = {\"language\": language}\n\n        return self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results,\n            filters=filters\n        )\n\n    def retrieve_by_type(\n        self,\n        collection_name: str,\n        query: str,\n        chunk_type: str,\n        n_results: int = 10\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve chunks of a specific type.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            chunk_type: Type of chunk (e.g., \"function\", \"class\")\n            n_results: Maximum number of results\n\n        Returns:\n            List of chunks of the specified type\n        \"\"\"\n        filters = {\"chunk_type\": chunk_type}\n\n        return self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results,\n            filters=filters\n        )\n\n    def retrieve_uncommitted(\n        self,\n        collection_name: str,\n        query: str,\n        n_results: int = 10\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve only uncommitted changes.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            n_results: Maximum number of results\n\n        Returns:\n            List of uncommitted chunks\n        \"\"\"\n        filters = {\"is_uncommitted\": \"True\"}\n\n        return self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results,\n            filters=filters\n        )\n\n    def find_similar_code(\n        self,\n        collection_name: str,\n        code_snippet: str,\n        n_results: int = 5,\n        exclude_exact: bool = True\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find code similar to a given snippet.\n\n        Args:\n            collection_name: ChromaDB collection name\n            code_snippet: Code to find similar examples of\n            n_results: Maximum number of results\n            exclude_exact: If True, exclude exact matches\n\n        Returns:\n            List of similar code chunks\n        \"\"\"\n        logger.info(f\"Finding similar code (n={n_results})\")\n\n        # Use code as query (embedder handles code well)\n        chunks = self.retrieve(\n            collection_name=collection_name,\n            query=code_snippet,\n            n_results=n_results + (1 if exclude_exact else 0)\n        )\n\n        # Filter exact matches if requested\n        if exclude_exact:\n            chunks = [\n                chunk for chunk in chunks\n                if chunk['code'].strip() != code_snippet.strip()\n            ][:n_results]\n\n        return chunks\n\n    def _format_results(\n        self,\n        raw_results: Dict[str, Any],\n        min_similarity: float = 0.0\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Format raw ChromaDB results into structured chunks.\n\n        Args:\n            raw_results: Raw results from ChromaDB\n            min_similarity: Minimum similarity threshold\n\n        Returns:\n            List of formatted chunks\n        \"\"\"\n        chunks = []\n\n        # ChromaDB returns results in lists\n        ids = raw_results.get('ids', [[]])[0]\n        documents = raw_results.get('documents', [[]])[0]\n        metadatas = raw_results.get('metadatas', [[]])[0]\n        distances = raw_results.get('distances', [[]])[0]\n\n        for idx, chunk_id in enumerate(ids):\n            # Convert distance to similarity (ChromaDB uses L2 distance)\n            # Similarity = 1 / (1 + distance)\n            distance = distances[idx] if idx < len(distances) else 1.0\n            similarity = 1.0 / (1.0 + distance)\n\n            # Skip if below threshold\n            if similarity < min_similarity:\n                continue\n\n            metadata = metadatas[idx] if idx < len(metadatas) else {}\n\n            # Build chunk dict\n            chunk = {\n                'id': chunk_id,\n                'code': documents[idx] if idx < len(documents) else '',\n                'similarity': similarity,\n                'distance': distance,\n                'metadata': metadata,\n                # Extract common metadata fields\n                'file_path': metadata.get('file_path', ''),\n                'chunk_type': metadata.get('chunk_type', 'unknown'),\n                'name': metadata.get('name', 'unknown'),\n                'language': metadata.get('language', 'unknown'),\n                'start_line': int(metadata.get('start_line', 0)),\n                'end_line': int(metadata.get('end_line', 0)),\n                'line_count': int(metadata.get('line_count', 0)),\n            }\n\n            chunks.append(chunk)\n\n        return chunks\n\n    def compute_chunk_relevance(\n        self,\n        query_embedding: np.ndarray,\n        chunk_embeddings: List[np.ndarray]\n    ) -> List[float]:\n        \"\"\"Compute relevance scores between query and chunks.\n\n        Args:\n            query_embedding: Query embedding vector\n            chunk_embeddings: List of chunk embedding vectors\n\n        Returns:\n            List of relevance scores (cosine similarity)\n        \"\"\"\n        scores = []\n\n        for chunk_emb in chunk_embeddings:\n            score = self.embedder.compute_similarity(query_embedding, chunk_emb)\n            scores.append(score)\n\n        return scores\n\n    def hybrid_search(\n        self,\n        collection_name: str,\n        query: str,\n        keywords: List[str],\n        n_results: int = 10,\n        semantic_weight: float = 0.7\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Hybrid search combining semantic and keyword matching.\n\n        Args:\n            collection_name: ChromaDB collection name\n            query: Natural language query\n            keywords: Keywords to boost\n            n_results: Maximum number of results\n            semantic_weight: Weight for semantic score (1 - weight for keyword)\n\n        Returns:\n            List of chunks with hybrid scores\n        \"\"\"\n        # Get semantic results\n        semantic_chunks = self.retrieve(\n            collection_name=collection_name,\n            query=query,\n            n_results=n_results * 2  # Get more for reranking\n        )\n\n        # Compute keyword scores\n        keyword_weight = 1.0 - semantic_weight\n\n        for chunk in semantic_chunks:\n            # Count keyword matches\n            code_lower = chunk['code'].lower()\n            keyword_score = sum(\n                1 for kw in keywords\n                if kw.lower() in code_lower\n            ) / max(len(keywords), 1)\n\n            # Compute hybrid score\n            chunk['semantic_score'] = chunk['similarity']\n            chunk['keyword_score'] = keyword_score\n            chunk['hybrid_score'] = (\n                semantic_weight * chunk['similarity'] +\n                keyword_weight * keyword_score\n            )\n\n        # Sort by hybrid score\n        semantic_chunks.sort(key=lambda x: x['hybrid_score'], reverse=True)\n\n        return semantic_chunks[:n_results]\n\n    def get_statistics(self, collection_name: str) -> Dict[str, Any]:\n        \"\"\"Get retrieval statistics for a collection.\n\n        Args:\n            collection_name: ChromaDB collection name\n\n        Returns:\n            Dictionary with collection statistics\n        \"\"\"\n        return self.vector_store.get_collection_stats(collection_name)\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/retrieval/retriever.py", "relative_path": "services/rag-pipeline/src/retrieval/retriever.py", "filename": "retriever.py", "extension": ".py"}}, {"id": "file___init___py_d7be03bb", "content": "\"\"\"Retrieval module for RAG system.\"\"\"\n\nfrom .retriever import CodeRetriever\nfrom .reranker import Reranker\nfrom .context import ContextAssembler\n\n__all__ = ['CodeRetriever', 'Reranker', 'ContextAssembler']\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/retrieval/__init__.py", "relative_path": "services/rag-pipeline/src/retrieval/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_context_py_ad6902bf", "content": "\"\"\"Context assembly for LLM prompts.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextAssembler:\n    \"\"\"Assemble context from retrieved chunks for LLM prompts.\"\"\"\n\n    def __init__(self, max_tokens: int = 4000):\n        \"\"\"Initialize context assembler.\n\n        Args:\n            max_tokens: Maximum tokens for context (rough estimate)\n        \"\"\"\n        self.max_tokens = max_tokens\n        self.max_chars = max_tokens * 4  # Rough: 1 token \u2248 4 chars\n\n        logger.info(f\"Context assembler initialized (max_tokens={max_tokens})\")\n\n    def assemble_context(\n        self,\n        chunks: List[Dict[str, Any]],\n        query: str,\n        include_metadata: bool = True,\n        include_file_paths: bool = True,\n        max_chunks: Optional[int] = None\n    ) -> str:\n        \"\"\"Assemble context from retrieved chunks.\n\n        Args:\n            chunks: List of retrieved chunks\n            query: User's query\n            include_metadata: Include metadata annotations\n            include_file_paths: Include file paths in context\n            max_chunks: Maximum number of chunks to include\n\n        Returns:\n            Assembled context string\n        \"\"\"\n        if not chunks:\n            return \"\"\n\n        logger.info(f\"Assembling context from {len(chunks)} chunks\")\n\n        # Limit number of chunks\n        if max_chunks:\n            chunks = chunks[:max_chunks]\n\n        context_parts = []\n        total_chars = 0\n\n        for idx, chunk in enumerate(chunks, 1):\n            # Build chunk context\n            chunk_text = self._format_chunk(\n                chunk,\n                idx,\n                include_metadata,\n                include_file_paths\n            )\n\n            # Check if we exceed max chars\n            if total_chars + len(chunk_text) > self.max_chars:\n                logger.info(f\"Reached max chars, stopping at chunk {idx-1}\")\n                break\n\n            context_parts.append(chunk_text)\n            total_chars += len(chunk_text)\n\n        context = \"\\n\\n\".join(context_parts)\n\n        logger.info(f\"Assembled context: {len(context_parts)} chunks, {total_chars} chars\")\n        return context\n\n    def assemble_prompt(\n        self,\n        chunks: List[Dict[str, Any]],\n        query: str,\n        system_prompt: Optional[str] = None,\n        include_instructions: bool = True\n    ) -> str:\n        \"\"\"Assemble full prompt with context for LLM.\n\n        Args:\n            chunks: Retrieved chunks\n            query: User's query\n            system_prompt: Optional system prompt override\n            include_instructions: Include query instructions\n\n        Returns:\n            Complete prompt string\n        \"\"\"\n        # Default system prompt\n        if system_prompt is None:\n            system_prompt = self._get_default_system_prompt()\n\n        # Assemble context\n        context = self.assemble_context(chunks, query)\n\n        # Build prompt sections\n        sections = [system_prompt]\n\n        if context:\n            sections.append(f\"# Relevant Code Context\\n\\n{context}\")\n\n        if include_instructions:\n            sections.append(self._get_query_instructions())\n\n        sections.append(f\"# User Query\\n\\n{query}\")\n\n        prompt = \"\\n\\n\".join(sections)\n\n        logger.info(f\"Assembled prompt: {len(prompt)} chars\")\n        return prompt\n\n    def assemble_chat_context(\n        self,\n        chunks: List[Dict[str, Any]],\n        query: str,\n        conversation_history: Optional[List[Dict[str, str]]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Assemble context for chat-based LLM APIs.\n\n        Args:\n            chunks: Retrieved chunks\n            query: Current query\n            conversation_history: Previous messages [{\"role\": \"user/assistant\", \"content\": \"...\"}]\n\n        Returns:\n            Dict with system message and messages list\n        \"\"\"\n        context = self.assemble_context(chunks, query, max_chunks=10)\n\n        system_message = f\"\"\"{self._get_default_system_prompt()}\n\n# Relevant Code Context\n\n{context}\n\nUse the code context above to answer the user's question accurately.\"\"\"\n\n        messages = []\n\n        # Add conversation history\n        if conversation_history:\n            messages.extend(conversation_history)\n\n        # Add current query\n        messages.append({\n            \"role\": \"user\",\n            \"content\": query\n        })\n\n        return {\n            \"system\": system_message,\n            \"messages\": messages\n        }\n\n    def _format_chunk(\n        self,\n        chunk: Dict[str, Any],\n        index: int,\n        include_metadata: bool,\n        include_file_paths: bool\n    ) -> str:\n        \"\"\"Format a single chunk for context.\n\n        Args:\n            chunk: Chunk dictionary\n            index: Chunk index\n            include_metadata: Include metadata\n            include_file_paths: Include file paths\n\n        Returns:\n            Formatted chunk string\n        \"\"\"\n        parts = []\n\n        # Header\n        header_parts = [f\"[{index}]\"]\n\n        if include_file_paths:\n            file_path = chunk.get('file_path', 'unknown')\n            header_parts.append(file_path)\n\n        if include_metadata:\n            name = chunk.get('name', '')\n            chunk_type = chunk.get('chunk_type', '')\n            language = chunk.get('language', '')\n\n            if name and name != 'unknown':\n                header_parts.append(f\"{chunk_type}: {name}\")\n\n            if language and language != 'unknown':\n                header_parts.append(f\"({language})\")\n\n        header = \" \".join(header_parts)\n        parts.append(header)\n\n        # Code content\n        code = chunk.get('code', '')\n        language = chunk.get('language', '').lower()\n\n        # Format as code block\n        parts.append(f\"```{language}\\n{code}\\n```\")\n\n        # Additional metadata\n        if include_metadata:\n            metadata_items = []\n\n            start_line = chunk.get('start_line')\n            end_line = chunk.get('end_line')\n            if start_line and end_line:\n                metadata_items.append(f\"Lines {start_line}-{end_line}\")\n\n            similarity = chunk.get('similarity')\n            if similarity is not None:\n                metadata_items.append(f\"Relevance: {similarity:.2%}\")\n\n            if metadata_items:\n                parts.append(f\"({', '.join(metadata_items)})\")\n\n        return \"\\n\".join(parts)\n\n    def _get_default_system_prompt(self) -> str:\n        \"\"\"Get default system prompt for code Q&A.\n\n        Returns:\n            System prompt string\n        \"\"\"\n        return \"\"\"You are an expert programming assistant with deep knowledge of software development.\n\nYour role is to help developers understand their codebase by:\n- Answering questions about code functionality and structure\n- Explaining complex code patterns and algorithms\n- Identifying potential bugs or issues\n- Suggesting improvements and best practices\n- Providing clear, concise explanations\n\nUse the provided code context to give accurate, relevant answers. If the context doesn't contain enough information, say so clearly.\n\nAlways cite specific files and line numbers when referencing code.\"\"\"\n\n    def _get_query_instructions(self) -> str:\n        \"\"\"Get query-specific instructions.\n\n        Returns:\n            Instruction string\n        \"\"\"\n        return \"\"\"# Instructions\n\nAnswer the user's query based on the code context provided above. Be specific and reference exact file paths, function names, and line numbers when applicable.\n\nIf the context is insufficient to answer the question completely, acknowledge what you can answer and what information is missing.\"\"\"\n\n    def group_chunks_by_file(\n        self,\n        chunks: List[Dict[str, Any]]\n    ) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Group chunks by file path.\n\n        Args:\n            chunks: List of chunks\n\n        Returns:\n            Dictionary mapping file paths to chunk lists\n        \"\"\"\n        grouped = {}\n\n        for chunk in chunks:\n            file_path = chunk.get('file_path', 'unknown')\n\n            if file_path not in grouped:\n                grouped[file_path] = []\n\n            grouped[file_path].append(chunk)\n\n        # Sort chunks within each file by line number\n        for file_path in grouped:\n            grouped[file_path].sort(\n                key=lambda c: c.get('start_line', 0)\n            )\n\n        return grouped\n\n    def build_file_summary(\n        self,\n        chunks: List[Dict[str, Any]]\n    ) -> str:\n        \"\"\"Build a summary of files in the context.\n\n        Args:\n            chunks: List of chunks\n\n        Returns:\n            Summary string\n        \"\"\"\n        grouped = self.group_chunks_by_file(chunks)\n\n        summary_lines = [\"# Files in Context\\n\"]\n\n        for file_path, file_chunks in grouped.items():\n            languages = {c.get('language', 'unknown') for c in file_chunks}\n            chunk_types = {c.get('chunk_type', 'unknown') for c in file_chunks}\n\n            summary_lines.append(\n                f\"- {file_path} ({', '.join(languages)}): \"\n                f\"{len(file_chunks)} chunk(s) - {', '.join(chunk_types)}\"\n            )\n\n        return \"\\n\".join(summary_lines)\n\n    def estimate_token_count(self, text: str) -> int:\n        \"\"\"Estimate token count for text.\n\n        Args:\n            text: Text to estimate\n\n        Returns:\n            Estimated token count\n        \"\"\"\n        # Rough estimate: 1 token \u2248 4 characters\n        return len(text) // 4\n\n    def truncate_to_tokens(\n        self,\n        text: str,\n        max_tokens: int,\n        preserve_end: bool = False\n    ) -> str:\n        \"\"\"Truncate text to approximate token limit.\n\n        Args:\n            text: Text to truncate\n            max_tokens: Maximum tokens\n            preserve_end: If True, truncate from start instead of end\n\n        Returns:\n            Truncated text\n        \"\"\"\n        max_chars = max_tokens * 4\n\n        if len(text) <= max_chars:\n            return text\n\n        if preserve_end:\n            # Keep end\n            truncated = \"...\\n\" + text[-max_chars:]\n        else:\n            # Keep start\n            truncated = text[:max_chars] + \"\\n...\"\n\n        return truncated\n\n    def build_metadata_summary(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Build summary statistics about retrieved chunks.\n\n        Args:\n            chunks: List of chunks\n\n        Returns:\n            Dictionary with summary statistics\n        \"\"\"\n        if not chunks:\n            return {}\n\n        languages = {}\n        chunk_types = {}\n        files = set()\n\n        for chunk in chunks:\n            lang = chunk.get('language', 'unknown')\n            languages[lang] = languages.get(lang, 0) + 1\n\n            ctype = chunk.get('chunk_type', 'unknown')\n            chunk_types[ctype] = chunk_types.get(ctype, 0) + 1\n\n            files.add(chunk.get('file_path', 'unknown'))\n\n        avg_similarity = sum(\n            c.get('similarity', 0) for c in chunks\n        ) / len(chunks)\n\n        return {\n            'total_chunks': len(chunks),\n            'unique_files': len(files),\n            'languages': languages,\n            'chunk_types': chunk_types,\n            'avg_similarity': avg_similarity,\n            'files': sorted(files)\n        }\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/retrieval/context.py", "relative_path": "services/rag-pipeline/src/retrieval/context.py", "filename": "context.py", "extension": ".py"}}, {"id": "file_metadata_db_py_026b9616", "content": "\"\"\"SQLite metadata database for multi-repository tracking.\"\"\"\n\nimport sqlite3\nimport uuid\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Any\nfrom contextlib import contextmanager\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass MetadataDB:\n    \"\"\"Manage SQLite database for repository metadata.\"\"\"\n\n    def __init__(self, db_path: str = \"/app/data/metadata/repos.db\"):\n        \"\"\"Initialize metadata database.\n\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = Path(db_path)\n        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n        self._init_database()\n\n    @contextmanager\n    def _get_connection(self):\n        \"\"\"Context manager for database connections.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row  # Enable column access by name\n        try:\n            yield conn\n            conn.commit()\n        except Exception as e:\n            conn.rollback()\n            logger.error(f\"Database error: {e}\")\n            raise\n        finally:\n            conn.close()\n\n    def _init_database(self):\n        \"\"\"Initialize database schema.\"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n\n            # Repository tracking table\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS repositories (\n                    id TEXT PRIMARY KEY,\n                    name TEXT NOT NULL,\n                    path TEXT NOT NULL UNIQUE,\n                    chroma_collection_name TEXT NOT NULL,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    last_indexed_at TIMESTAMP,\n                    last_commit_hash TEXT,\n                    is_active BOOLEAN DEFAULT 0,\n                    indexing_status TEXT CHECK(indexing_status IN ('pending', 'in_progress', 'completed', 'failed')),\n                    total_chunks INTEGER DEFAULT 0,\n                    total_files INTEGER DEFAULT 0,\n                    embedding_provider TEXT DEFAULT 'local',\n                    embedding_model TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2',\n                    embedding_dimension INTEGER DEFAULT 384\n                )\n            \"\"\")\n\n            # File tracking table\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS indexed_files (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    repo_id TEXT NOT NULL,\n                    file_path TEXT NOT NULL,\n                    file_hash TEXT NOT NULL,\n                    last_indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    chunk_count INTEGER DEFAULT 0,\n                    language TEXT,\n                    FOREIGN KEY (repo_id) REFERENCES repositories(id) ON DELETE CASCADE,\n                    UNIQUE(repo_id, file_path)\n                )\n            \"\"\")\n\n            # Commit tracking table\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS indexed_commits (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    repo_id TEXT NOT NULL,\n                    commit_hash TEXT NOT NULL,\n                    commit_message TEXT,\n                    author TEXT,\n                    committed_at TIMESTAMP,\n                    indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    chunk_count INTEGER DEFAULT 0,\n                    FOREIGN KEY (repo_id) REFERENCES repositories(id) ON DELETE CASCADE,\n                    UNIQUE(repo_id, commit_hash)\n                )\n            \"\"\")\n\n            # Indexing jobs queue\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS indexing_queue (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    repo_id TEXT NOT NULL,\n                    job_type TEXT CHECK(job_type IN ('full', 'incremental', 'file', 'commit')),\n                    target_path TEXT,\n                    status TEXT CHECK(status IN ('pending', 'processing', 'completed', 'failed')),\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    started_at TIMESTAMP,\n                    completed_at TIMESTAMP,\n                    error_message TEXT,\n                    FOREIGN KEY (repo_id) REFERENCES repositories(id) ON DELETE CASCADE\n                )\n            \"\"\")\n\n            # Create indexes\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_indexed_files_repo ON indexed_files(repo_id)\")\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_indexed_files_hash ON indexed_files(file_hash)\")\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_indexed_commits_repo ON indexed_commits(repo_id)\")\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_indexing_queue_status ON indexing_queue(status, created_at)\")\n\n            # Run migrations for embedding tracking (Phase 1.7)\n            self._migrate_embedding_columns(cursor)\n\n            logger.info(f\"Database initialized at {self.db_path}\")\n\n    def _migrate_embedding_columns(self, cursor):\n        \"\"\"Migrate existing databases to add embedding tracking columns.\n\n        Args:\n            cursor: Database cursor\n        \"\"\"\n        try:\n            # Check if embedding columns exist\n            cursor.execute(\"PRAGMA table_info(repositories)\")\n            columns = [col[1] for col in cursor.fetchall()]\n\n            # Add missing columns if they don't exist\n            if 'embedding_provider' not in columns:\n                cursor.execute(\"ALTER TABLE repositories ADD COLUMN embedding_provider TEXT DEFAULT 'local'\")\n                logger.info(\"Added embedding_provider column to repositories table\")\n\n            if 'embedding_model' not in columns:\n                cursor.execute(\"ALTER TABLE repositories ADD COLUMN embedding_model TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2'\")\n                logger.info(\"Added embedding_model column to repositories table\")\n\n            if 'embedding_dimension' not in columns:\n                cursor.execute(\"ALTER TABLE repositories ADD COLUMN embedding_dimension INTEGER DEFAULT 384\")\n                logger.info(\"Added embedding_dimension column to repositories table\")\n\n        except Exception as e:\n            logger.warning(f\"Migration check/execution encountered issue: {e}\")\n\n    # Repository methods\n    def add_repository(self, path: str, name: Optional[str] = None) -> str:\n        \"\"\"Add a new repository to track.\n\n        Args:\n            path: Absolute path to repository\n            name: Optional name (defaults to directory name)\n\n        Returns:\n            Repository ID (UUID)\n        \"\"\"\n        repo_path = Path(path).resolve()\n        repo_id = str(uuid.uuid4())\n        repo_name = name or repo_path.name\n        collection_name = f\"repo_{repo_id.replace('-', '_')}\"\n\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO repositories (id, name, path, chroma_collection_name, indexing_status)\n                VALUES (?, ?, ?, ?, 'pending')\n            \"\"\", (repo_id, repo_name, str(repo_path), collection_name))\n\n            logger.info(f\"Added repository: {repo_name} ({repo_id})\")\n            return repo_id\n\n    def get_repository(self, repo_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get repository by ID.\n\n        Args:\n            repo_id: Repository UUID\n\n        Returns:\n            Repository data dict or None\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM repositories WHERE id = ?\", (repo_id,))\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    def get_repository_by_path(self, path: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get repository by path.\n\n        Args:\n            path: Repository path\n\n        Returns:\n            Repository data dict or None\n        \"\"\"\n        repo_path = str(Path(path).resolve())\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM repositories WHERE path = ?\", (repo_path,))\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    def list_repositories(self) -> List[Dict[str, Any]]:\n        \"\"\"List all repositories.\n\n        Returns:\n            List of repository dicts\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM repositories ORDER BY created_at DESC\")\n            return [dict(row) for row in cursor.fetchall()]\n\n    def set_active_repository(self, repo_id: str):\n        \"\"\"Set a repository as active (deactivate others).\n\n        Args:\n            repo_id: Repository UUID to activate\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            # Deactivate all\n            cursor.execute(\"UPDATE repositories SET is_active = 0\")\n            # Activate specified\n            cursor.execute(\"UPDATE repositories SET is_active = 1 WHERE id = ?\", (repo_id,))\n            logger.info(f\"Set active repository: {repo_id}\")\n\n    def get_active_repository(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Get the currently active repository.\n\n        Returns:\n            Active repository dict or None\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM repositories WHERE is_active = 1 LIMIT 1\")\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    def update_repository_status(self, repo_id: str, status: str, **kwargs):\n        \"\"\"Update repository status and metadata.\n\n        Args:\n            repo_id: Repository UUID\n            status: Indexing status\n            **kwargs: Additional fields to update\n        \"\"\"\n        fields = {\"indexing_status\": status}\n        fields.update(kwargs)\n\n        set_clause = \", \".join([f\"{k} = ?\" for k in fields.keys()])\n        values = list(fields.values()) + [repo_id]\n\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(f\"UPDATE repositories SET {set_clause} WHERE id = ?\", values)\n            logger.debug(f\"Updated repository {repo_id}: {fields}\")\n\n    def update_repository(self, repo_id: str, **kwargs):\n        \"\"\"Update repository fields.\n\n        Args:\n            repo_id: Repository UUID\n            **kwargs: Fields to update (e.g., last_indexed_at, total_chunks, indexing_status)\n        \"\"\"\n        if not kwargs:\n            logger.warning(f\"No fields provided to update for repository {repo_id}\")\n            return\n\n        # Handle CURRENT_TIMESTAMP for timestamp fields\n        set_clause_parts = []\n        values = []\n\n        for key, value in kwargs.items():\n            if value == 'CURRENT_TIMESTAMP':\n                set_clause_parts.append(f\"{key} = CURRENT_TIMESTAMP\")\n            else:\n                set_clause_parts.append(f\"{key} = ?\")\n                values.append(value)\n\n        set_clause = \", \".join(set_clause_parts)\n        values.append(repo_id)\n\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(f\"UPDATE repositories SET {set_clause} WHERE id = ?\", values)\n            logger.debug(f\"Updated repository {repo_id}: {kwargs}\")\n\n    def update_repository_embedding_info(\n        self,\n        repo_id: str,\n        embedding_provider: str,\n        embedding_model: str,\n        embedding_dimension: int\n    ):\n        \"\"\"Update embedding information for a repository.\n\n        Args:\n            repo_id: Repository UUID\n            embedding_provider: Embedding provider ('local' or 'openai')\n            embedding_model: Model name used for embeddings\n            embedding_dimension: Dimension of the embeddings\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                UPDATE repositories\n                SET embedding_provider = ?, embedding_model = ?, embedding_dimension = ?\n                WHERE id = ?\n            \"\"\", (embedding_provider, embedding_model, embedding_dimension, repo_id))\n            logger.info(f\"Updated embedding info for repository {repo_id}: {embedding_provider}/{embedding_model} ({embedding_dimension}D)\")\n\n    def delete_repository(self, repo_id: str):\n        \"\"\"Delete a repository and all associated data.\n\n        Args:\n            repo_id: Repository UUID\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"DELETE FROM repositories WHERE id = ?\", (repo_id,))\n            logger.info(f\"Deleted repository: {repo_id}\")\n\n    # File tracking methods\n    def upsert_file(self, repo_id: str, file_path: str, file_hash: str,\n                    chunk_count: int = 0, language: Optional[str] = None):\n        \"\"\"Insert or update file tracking record.\n\n        Args:\n            repo_id: Repository UUID\n            file_path: Relative file path\n            file_hash: SHA256 hash of file content\n            chunk_count: Number of chunks created\n            language: Programming language\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO indexed_files (repo_id, file_path, file_hash, chunk_count, language, last_indexed_at)\n                VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP)\n                ON CONFLICT(repo_id, file_path) DO UPDATE SET\n                    file_hash = excluded.file_hash,\n                    chunk_count = excluded.chunk_count,\n                    language = excluded.language,\n                    last_indexed_at = CURRENT_TIMESTAMP\n            \"\"\", (repo_id, file_path, file_hash, chunk_count, language))\n\n    def get_file(self, repo_id: str, file_path: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get file tracking record.\n\n        Args:\n            repo_id: Repository UUID\n            file_path: Relative file path\n\n        Returns:\n            File data dict or None\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT * FROM indexed_files\n                WHERE repo_id = ? AND file_path = ?\n            \"\"\", (repo_id, file_path))\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    def list_files(self, repo_id: str) -> List[Dict[str, Any]]:\n        \"\"\"List all indexed files for a repository.\n\n        Args:\n            repo_id: Repository UUID\n\n        Returns:\n            List of file dicts\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT * FROM indexed_files\n                WHERE repo_id = ?\n                ORDER BY last_indexed_at DESC\n            \"\"\", (repo_id,))\n            return [dict(row) for row in cursor.fetchall()]\n\n    # Commit tracking methods\n    def upsert_commit(self, repo_id: str, commit_hash: str, commit_message: str,\n                      author: str, committed_at: datetime, chunk_count: int = 0):\n        \"\"\"Insert or update commit tracking record.\n\n        Args:\n            repo_id: Repository UUID\n            commit_hash: Git commit hash\n            commit_message: Commit message\n            author: Commit author\n            committed_at: Commit timestamp\n            chunk_count: Number of chunks created\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO indexed_commits (repo_id, commit_hash, commit_message, author, committed_at, chunk_count)\n                VALUES (?, ?, ?, ?, ?, ?)\n                ON CONFLICT(repo_id, commit_hash) DO UPDATE SET\n                    commit_message = excluded.commit_message,\n                    chunk_count = excluded.chunk_count,\n                    indexed_at = CURRENT_TIMESTAMP\n            \"\"\", (repo_id, commit_hash, commit_message, author, committed_at, chunk_count))\n\n    def get_commit(self, repo_id: str, commit_hash: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get commit tracking record.\n\n        Args:\n            repo_id: Repository UUID\n            commit_hash: Git commit hash\n\n        Returns:\n            Commit data dict or None\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT * FROM indexed_commits\n                WHERE repo_id = ? AND commit_hash = ?\n            \"\"\", (repo_id, commit_hash))\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    # Indexing queue methods\n    def add_indexing_job(self, repo_id: str, job_type: str, target_path: Optional[str] = None) -> int:\n        \"\"\"Add a new indexing job to the queue.\n\n        Args:\n            repo_id: Repository UUID\n            job_type: Job type (full, incremental, file, commit)\n            target_path: Optional target file or commit hash\n\n        Returns:\n            Job ID\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO indexing_queue (repo_id, job_type, target_path, status)\n                VALUES (?, ?, ?, 'pending')\n            \"\"\", (repo_id, job_type, target_path))\n            return cursor.lastrowid\n\n    def get_pending_jobs(self, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get pending indexing jobs.\n\n        Args:\n            limit: Maximum number of jobs to return\n\n        Returns:\n            List of job dicts\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT * FROM indexing_queue\n                WHERE status = 'pending'\n                ORDER BY created_at ASC\n                LIMIT ?\n            \"\"\", (limit,))\n            return [dict(row) for row in cursor.fetchall()]\n\n    def update_job_status(self, job_id: int, status: str, error_message: Optional[str] = None):\n        \"\"\"Update indexing job status.\n\n        Args:\n            job_id: Job ID\n            status: New status\n            error_message: Optional error message\n        \"\"\"\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n\n            if status == 'processing':\n                cursor.execute(\"\"\"\n                    UPDATE indexing_queue\n                    SET status = ?, started_at = CURRENT_TIMESTAMP\n                    WHERE id = ?\n                \"\"\", (status, job_id))\n            elif status in ('completed', 'failed'):\n                cursor.execute(\"\"\"\n                    UPDATE indexing_queue\n                    SET status = ?, completed_at = CURRENT_TIMESTAMP, error_message = ?\n                    WHERE id = ?\n                \"\"\", (status, error_message, job_id))\n            else:\n                cursor.execute(\"\"\"\n                    UPDATE indexing_queue\n                    SET status = ?\n                    WHERE id = ?\n                \"\"\", (status, job_id))\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/db/metadata_db.py", "relative_path": "services/rag-pipeline/src/db/metadata_db.py", "filename": "metadata_db.py", "extension": ".py"}}, {"id": "file___init___py_efcd8c68", "content": "\"\"\"Database module for metadata storage.\"\"\"\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/db/__init__.py", "relative_path": "services/rag-pipeline/src/db/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_models_py_872d056b", "content": "\"\"\"Pydantic models for API requests and responses.\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom datetime import datetime\n\n\nclass RepositoryCreate(BaseModel):\n    \"\"\"Request model for creating a repository.\"\"\"\n    path: str = Field(..., description=\"Absolute path to Git repository\")\n    name: Optional[str] = Field(None, description=\"Optional repository name\")\n\n\nclass RepositoryResponse(BaseModel):\n    \"\"\"Response model for repository data.\"\"\"\n    id: str\n    name: str\n    path: str\n    chroma_collection_name: str\n    created_at: datetime\n    last_indexed_at: Optional[datetime] = None\n    last_commit_hash: Optional[str] = None\n    is_active: bool\n    indexing_status: str\n    total_chunks: int\n    total_files: int\n    # NEW: Iteration 2 - Embedding tracking\n    embedding_provider: Optional[str] = Field(None, description=\"Embedding provider used (local/openai)\")\n    embedding_model: Optional[str] = Field(None, description=\"Embedding model name\")\n    embedding_dimension: Optional[int] = Field(None, description=\"Embedding dimension\")\n\n\nclass RepositoryStats(BaseModel):\n    \"\"\"Repository statistics.\"\"\"\n    path: str\n    branch: str\n    total_commits: int\n    total_files: int\n    modified_files: int\n    untracked_files: int\n    latest_commit: Optional[dict] = None\n\n\nclass IndexingRequest(BaseModel):\n    \"\"\"Request model for triggering indexing.\"\"\"\n    force_reindex: bool = Field(False, description=\"Force full reindex even if already indexed\")\n    # NEW: Iteration 2 - Embedding selection\n    embedding_provider: Optional[str] = Field(None, description=\"Embedding provider: 'local' or 'openai'\")\n    embedding_model: Optional[str] = Field(None, description=\"Specific embedding model name (optional)\")\n\n\nclass QueryRequest(BaseModel):\n    \"\"\"Request model for RAG query.\"\"\"\n    query: str = Field(..., description=\"User query\")\n    repo_id: Optional[str] = Field(None, description=\"Optional repository ID (uses active if not specified)\")\n    n_results: Optional[int] = Field(10, description=\"Number of results to return\")\n    use_reranking: bool = Field(True, description=\"Whether to apply MMR reranking\")\n    language: Optional[str] = Field(None, description=\"Filter by programming language\")\n    file_path: Optional[str] = Field(None, description=\"Filter by file path\")\n\n\nclass QueryResponse(BaseModel):\n    \"\"\"Response model for RAG query.\"\"\"\n    answer: str\n    sources: List[dict]\n    repo_id: str\n    metadata: Optional[dict] = None\n\n\nclass HealthResponse(BaseModel):\n    \"\"\"Health check response.\"\"\"\n    status: str\n    version: str\n    chromadb_connected: bool\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/api/models.py", "relative_path": "services/rag-pipeline/src/api/models.py", "filename": "models.py", "extension": ".py"}}, {"id": "file___init___py_9a85df3f", "content": "\"\"\"API module.\"\"\"\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/api/__init__.py", "relative_path": "services/rag-pipeline/src/api/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_routes_py_5aee3462", "content": "\"\"\"API routes for RAG pipeline.\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, Depends\nfrom fastapi.responses import StreamingResponse\nfrom typing import List, Optional\nimport logging\n\nfrom ..config import get_settings, Settings\nfrom ..db.metadata_db import MetadataDB\nfrom ..core.git_ops import GitOperations\nfrom ..core.vector_store import VectorStore\nfrom ..core.embedder import BaseEmbedder, create_embedder\nfrom ..indexing.indexer import RepositoryIndexer\nfrom ..retrieval.retriever import CodeRetriever\nfrom ..retrieval.reranker import Reranker\nfrom ..retrieval.context import ContextAssembler\nfrom ..llm.factory import LLMFactory\nfrom ..llm.base import LLMError\nfrom .models import (\n    RepositoryCreate,\n    RepositoryResponse,\n    RepositoryStats,\n    IndexingRequest,\n    QueryRequest,\n    QueryResponse,\n    HealthResponse\n)\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n\ndef get_db() -> MetadataDB:\n    \"\"\"Get database instance.\"\"\"\n    settings = get_settings()\n    return MetadataDB(settings.metadata_db_path)\n\n\ndef get_vector_store() -> VectorStore:\n    \"\"\"Get vector store instance.\"\"\"\n    settings = get_settings()\n    # Don't pass embedding_model when using OpenAI or other pre-computed embeddings\n    # ChromaDB's SentenceTransformer function is only needed for local embeddings\n    embedding_model = None if settings.embedding_provider == \"openai\" else settings.embedding_model\n    return VectorStore(\n        host=settings.chroma_host,\n        port=settings.chroma_port,\n        embedding_model=embedding_model\n    )\n\n\ndef get_embedder() -> BaseEmbedder:\n    \"\"\"Get embedder instance.\"\"\"\n    settings = get_settings()\n    kwargs = {}\n    if settings.embedding_provider == \"openai\":\n        kwargs[\"api_key\"] = settings.openai_api_key\n        model_name = settings.openai_embedding_model\n    else:\n        model_name = settings.embedding_model\n\n    return create_embedder(\n        provider=settings.embedding_provider,\n        model_name=model_name,\n        **kwargs\n    )\n\n\ndef get_indexer(\n    db: MetadataDB = Depends(get_db),\n    vector_store: VectorStore = Depends(get_vector_store),\n    embedder: BaseEmbedder = Depends(get_embedder)\n) -> RepositoryIndexer:\n    \"\"\"Get indexer instance.\"\"\"\n    return RepositoryIndexer(\n        metadata_db=db,\n        vector_store=vector_store,\n        embedder=embedder\n    )\n\n\ndef get_retriever(\n    vector_store: VectorStore = Depends(get_vector_store),\n    embedder: BaseEmbedder = Depends(get_embedder)\n) -> CodeRetriever:\n    \"\"\"Get retriever instance.\"\"\"\n    return CodeRetriever(\n        vector_store=vector_store,\n        embedder=embedder\n    )\n\n\ndef get_reranker(embedder: BaseEmbedder = Depends(get_embedder)) -> Reranker:\n    \"\"\"Get reranker instance.\"\"\"\n    return Reranker(embedder=embedder)\n\n\ndef get_context_assembler() -> ContextAssembler:\n    \"\"\"Get context assembler instance.\"\"\"\n    return ContextAssembler(max_tokens=4000)\n\n\ndef get_llm_provider(settings: Settings = Depends(get_settings)):\n    \"\"\"Get LLM provider instance.\"\"\"\n    return LLMFactory.create_from_settings(settings)\n\n\n@router.get(\"/health\", response_model=HealthResponse)\nasync def health_check(\n    settings: Settings = Depends(get_settings),\n    vector_store: VectorStore = Depends(get_vector_store)\n):\n    \"\"\"Health check endpoint.\"\"\"\n    # Check ChromaDB connection\n    try:\n        vector_store.client.heartbeat()\n        chromadb_connected = True\n    except Exception as e:\n        logger.error(f\"ChromaDB health check failed: {e}\")\n        chromadb_connected = False\n\n    return HealthResponse(\n        status=\"healthy\" if chromadb_connected else \"degraded\",\n        version=\"0.1.0\",\n        chromadb_connected=chromadb_connected\n    )\n\n\n@router.get(\"/codex/status\")\nasync def codex_status():\n    \"\"\"Check Codex CLI availability and authentication status.\"\"\"\n    import subprocess\n    import json as json_lib\n\n    try:\n        # Check if codex is installed\n        version_result = subprocess.run(\n            ['codex', '--version'],\n            capture_output=True,\n            text=True,\n            timeout=5\n        )\n\n        if version_result.returncode != 0:\n            return {\n                \"installed\": False,\n                \"authenticated\": False,\n                \"version\": None,\n                \"error\": \"Codex CLI not found\"\n            }\n\n        version = version_result.stdout.strip()\n\n        # Try a simple test to check authentication\n        # Note: --dangerously-bypass-approvals-and-sandbox is required in Docker containers\n        test_result = subprocess.run(\n            ['codex', 'exec', '--skip-git-repo-check', '--dangerously-bypass-approvals-and-sandbox', '--json', 'echo test'],\n            capture_output=True,\n            text=True,\n            timeout=15\n        )\n\n        authenticated = test_result.returncode == 0\n        error_msg = None\n\n        if not authenticated:\n            stderr = test_result.stderr\n            if \"403\" in stderr or \"Unauthorized\" in stderr:\n                error_msg = \"Not authenticated. Please run 'codex' on host to login.\"\n            else:\n                error_msg = stderr[:200] if stderr else \"Unknown authentication error\"\n\n        return {\n            \"installed\": True,\n            \"authenticated\": authenticated,\n            \"version\": version,\n            \"error\": error_msg\n        }\n\n    except subprocess.TimeoutExpired:\n        return {\n            \"installed\": True,\n            \"authenticated\": None,\n            \"version\": None,\n            \"error\": \"Codex CLI timeout - may be waiting for input\"\n        }\n    except FileNotFoundError:\n        return {\n            \"installed\": False,\n            \"authenticated\": False,\n            \"version\": None,\n            \"error\": \"Codex CLI not installed in container\"\n        }\n    except Exception as e:\n        return {\n            \"installed\": None,\n            \"authenticated\": None,\n            \"version\": None,\n            \"error\": str(e)\n        }\n\n\n@router.post(\"/repos\", response_model=RepositoryResponse)\nasync def create_repository(\n    repo_data: RepositoryCreate,\n    db: MetadataDB = Depends(get_db)\n):\n    \"\"\"Add a new repository to track.\"\"\"\n    try:\n        # Validate Git repository\n        if not GitOperations.is_git_repository(repo_data.path):\n            raise HTTPException(status_code=400, detail=\"Path is not a valid Git repository\")\n\n        # Check if repository already exists\n        existing = db.get_repository_by_path(repo_data.path)\n        if existing:\n            raise HTTPException(status_code=409, detail=\"Repository already exists\")\n\n        # Add repository\n        repo_id = db.add_repository(repo_data.path, repo_data.name)\n        repo = db.get_repository(repo_id)\n\n        return RepositoryResponse(**repo)\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error creating repository: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/repos\", response_model=List[RepositoryResponse])\nasync def list_repositories(db: MetadataDB = Depends(get_db)):\n    \"\"\"List all repositories.\"\"\"\n    try:\n        repos = db.list_repositories()\n        return [RepositoryResponse(**repo) for repo in repos]\n    except Exception as e:\n        logger.error(f\"Error listing repositories: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/repos/{repo_id}\", response_model=RepositoryResponse)\nasync def get_repository(repo_id: str, db: MetadataDB = Depends(get_db)):\n    \"\"\"Get repository details.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n    return RepositoryResponse(**repo)\n\n\n@router.put(\"/repos/{repo_id}/activate\")\nasync def activate_repository(repo_id: str, db: MetadataDB = Depends(get_db)):\n    \"\"\"Set a repository as active.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    db.set_active_repository(repo_id)\n    return {\"message\": \"Repository activated\", \"repo_id\": repo_id}\n\n\n@router.delete(\"/repos/{repo_id}\")\nasync def delete_repository(\n    repo_id: str,\n    db: MetadataDB = Depends(get_db),\n    vector_store: VectorStore = Depends(get_vector_store)\n):\n    \"\"\"Delete a repository.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        # Delete from ChromaDB\n        collection_name = repo['chroma_collection_name']\n        vector_store.delete_collection(collection_name)\n        logger.info(f\"Deleted ChromaDB collection: {collection_name}\")\n    except Exception as e:\n        logger.error(f\"Failed to delete ChromaDB collection: {e}\")\n\n    # Delete from metadata database\n    db.delete_repository(repo_id)\n    return {\"message\": \"Repository deleted\", \"repo_id\": repo_id}\n\n\n@router.get(\"/repos/{repo_id}/stats\", response_model=RepositoryStats)\nasync def get_repository_stats(repo_id: str, db: MetadataDB = Depends(get_db)):\n    \"\"\"Get repository statistics.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        git_ops = GitOperations(repo[\"path\"])\n        stats = git_ops.get_repo_stats()\n        return RepositoryStats(**stats)\n    except Exception as e:\n        logger.error(f\"Error getting repository stats: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/repos/{repo_id}/index\")\nasync def trigger_indexing(\n    repo_id: str,\n    request: Optional[IndexingRequest] = None,\n    db: MetadataDB = Depends(get_db),\n    vector_store: VectorStore = Depends(get_vector_store)\n):\n    \"\"\"Trigger repository indexing with optional embedding provider selection.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        settings = get_settings()\n        force_reindex = request.force_reindex if request else False\n\n        # Get embedding provider from request or use default from settings\n        embedding_provider = request.embedding_provider if request and request.embedding_provider else settings.embedding_provider\n        embedding_model = request.embedding_model if request and request.embedding_model else None\n\n        # Create embedder with specified provider\n        if embedding_provider == \"openai\":\n            embedder = create_embedder(\n                provider=\"openai\",\n                model_name=embedding_model or settings.openai_embedding_model,\n                api_key=settings.openai_api_key\n            )\n        else:\n            embedder = create_embedder(\n                provider=\"local\",\n                model_name=embedding_model or settings.embedding_model\n            )\n\n        logger.info(f\"Using embedder: {embedding_provider}/{embedder.get_model_info()['model_name']}\")\n\n        # Create indexer with custom embedder\n        indexer = RepositoryIndexer(\n            metadata_db=db,\n            vector_store=vector_store,\n            embedder=embedder\n        )\n\n        # Run full indexing\n        result = indexer.index_repository(\n            repo_id=repo_id,\n            repo_path=repo['path'],\n            force_reindex=force_reindex\n        )\n\n        # Update repository with embedding info\n        db.update_repository_embedding_info(\n            repo_id=repo_id,\n            embedding_provider=embedding_provider,\n            embedding_model=embedder.get_model_info()['model_name'],\n            embedding_dimension=embedder.get_embedding_dimension()\n        )\n\n        return {\n            \"message\": \"Indexing completed\",\n            \"repo_id\": repo_id,\n            \"indexed_files\": result['indexed_files'],\n            \"total_chunks\": result['total_chunks'],\n            \"status\": result['status'],\n            \"embedding_provider\": embedding_provider,\n            \"embedding_model\": embedder.get_model_info()['model_name'],\n            \"embedding_dimension\": embedder.get_embedding_dimension()\n        }\n    except Exception as e:\n        logger.error(f\"Error during indexing: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/repos/{repo_id}/index/file\")\nasync def index_file(\n    repo_id: str,\n    file_path: str,\n    indexer: RepositoryIndexer = Depends(get_indexer),\n    db: MetadataDB = Depends(get_db)\n):\n    \"\"\"Index a specific file in the repository.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        chunks_added = indexer.index_file(\n            repo_id=repo_id,\n            file_path=file_path,\n            is_uncommitted=True\n        )\n\n        return {\n            \"message\": \"File indexed successfully\",\n            \"file_path\": file_path,\n            \"chunks_added\": chunks_added\n        }\n    except Exception as e:\n        logger.error(f\"Error indexing file: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/repos/{repo_id}/index/incremental\")\nasync def incremental_index(\n    repo_id: str,\n    indexer: RepositoryIndexer = Depends(get_indexer),\n    db: MetadataDB = Depends(get_db)\n):\n    \"\"\"Perform incremental indexing (only modified files).\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        result = indexer.incremental_index(repo_id)\n\n        return {\n            \"message\": \"Incremental indexing completed\",\n            \"repo_id\": repo_id,\n            \"indexed_files\": result['indexed_files'],\n            \"total_chunks\": result['total_chunks'],\n            \"status\": result['status']\n        }\n    except Exception as e:\n        logger.error(f\"Error during incremental indexing: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/repos/{repo_id}/index/status\")\nasync def get_indexing_status(\n    repo_id: str,\n    indexer: RepositoryIndexer = Depends(get_indexer),\n    db: MetadataDB = Depends(get_db)\n):\n    \"\"\"Get indexing status and statistics for a repository.\"\"\"\n    repo = db.get_repository(repo_id)\n    if not repo:\n        raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        stats = indexer.get_indexing_stats(repo_id)\n        return stats\n    except Exception as e:\n        logger.error(f\"Error getting indexing status: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/query\", response_model=QueryResponse)\nasync def query(\n    query_data: QueryRequest,\n    db: MetadataDB = Depends(get_db),\n    vector_store: VectorStore = Depends(get_vector_store),\n    reranker: Reranker = Depends(get_reranker),\n    context_assembler: ContextAssembler = Depends(get_context_assembler),\n    llm_provider = Depends(get_llm_provider)\n):\n    \"\"\"Query the RAG system with dynamic embedder matching repository's embedding.\"\"\"\n    # Get active repository if not specified\n    if not query_data.repo_id:\n        active_repo = db.get_active_repository()\n        if not active_repo:\n            raise HTTPException(status_code=400, detail=\"No active repository\")\n        repo_id = active_repo[\"id\"]\n    else:\n        repo_id = query_data.repo_id\n        repo = db.get_repository(repo_id)\n        if not repo:\n            raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        # Get repository info\n        repo = db.get_repository(repo_id)\n        collection_name = repo['chroma_collection_name']\n\n        # Create embedder matching the repository's embedding provider\n        settings = get_settings()\n        embedding_provider = repo.get('embedding_provider', 'local')\n        embedding_model = repo.get('embedding_model')\n\n        logger.info(f\"Creating query embedder: {embedding_provider}/{embedding_model}\")\n\n        # Always use OpenAI embeddings (local embeddings no longer supported)\n        if embedding_provider == \"local\":\n            logger.warning(f\"Repository was indexed with 'local' embeddings, but only 'openai' is supported now. Falling back to OpenAI.\")\n            embedding_provider = \"openai\"\n\n        if embedding_provider == \"openai\":\n            embedder = create_embedder(\n                provider=\"openai\",\n                model_name=embedding_model if embedding_provider == \"openai\" else settings.openai_embedding_model,\n                api_key=settings.openai_api_key\n            )\n        else:\n            # This shouldn't happen anymore, but just in case\n            raise ValueError(f\"Unsupported embedding provider: {embedding_provider}. Only 'openai' is supported.\")\n\n        # Create retriever with matching embedder\n        retriever = CodeRetriever(\n            vector_store=vector_store,\n            embedder=embedder\n        )\n\n        # Build filters from query parameters\n        filters = {}\n        if query_data.language:\n            filters['language'] = query_data.language\n        if query_data.file_path:\n            filters['file_path'] = query_data.file_path\n\n        # Retrieve relevant chunks\n        logger.info(f\"Querying: {query_data.query[:100]}\")\n        chunks = retriever.retrieve(\n            collection_name=collection_name,\n            query=query_data.query,\n            n_results=query_data.n_results or 20,\n            filters=filters if filters else None\n        )\n\n        if not chunks:\n            return QueryResponse(\n                answer=\"No relevant code found for your query. The repository may not be indexed yet.\",\n                sources=[],\n                repo_id=repo_id,\n                metadata={\n                    'retrieved_chunks': 0,\n                    'collection': collection_name\n                }\n            )\n\n        # Apply reranking if requested\n        if query_data.use_reranking:\n            logger.info(\"Applying MMR reranking\")\n            chunks = reranker.mmr_rerank(\n                chunks=chunks,\n                lambda_param=0.5,\n                top_k=query_data.n_results or 10\n            )\n\n        # Limit to requested number\n        final_chunks = chunks[:query_data.n_results] if query_data.n_results else chunks\n\n        # Check if query is about Git history and augment with actual Git data\n        git_context = \"\"\n        query_lower = query_data.query.lower()\n        if any(keyword in query_lower for keyword in ['commit', 'git log', 'git history', 'latest change', 'recent change', 'what changed', 'changes in']):\n            try:\n                # Get latest commits\n                import subprocess\n                result = subprocess.run(\n                    ['git', '-C', repo['path'], 'log', '-5', '--format=%H|%s|%an|%ar'],\n                    capture_output=True,\n                    text=True,\n                    timeout=5\n                )\n\n                if result.returncode == 0 and result.stdout:\n                    git_context = \"\\n\\n# Actual Git History\\n\\nLatest 5 commits:\\n\\n\"\n                    commits = []\n                    for line in result.stdout.strip().split('\\n'):\n                        hash_val, msg, author, date = line.split('|', 3)\n                        git_context += f\"- `{hash_val[:7]}` - {msg} (by {author}, {date})\\n\"\n                        commits.append(hash_val[:7])\n\n                    # If query asks about \"latest commit\" or \"what changed\", include the diff\n                    if any(phrase in query_lower for phrase in ['latest commit', 'what changed', 'changes in', 'what did', 'functional improvement']):\n                        logger.info(\"Adding diff for latest commit\")\n                        diff_result = subprocess.run(\n                            ['git', '-C', repo['path'], 'show', '--stat', '--format=%B', commits[0]],\n                            capture_output=True,\n                            text=True,\n                            timeout=10\n                        )\n\n                        if diff_result.returncode == 0 and diff_result.stdout:\n                            # Limit diff to first 1500 chars to avoid overwhelming the context\n                            diff_text = diff_result.stdout[:1500]\n                            if len(diff_result.stdout) > 1500:\n                                diff_text += \"\\n... (diff truncated for brevity)\"\n\n                            git_context += f\"\\n\\n## Latest Commit Details (`{commits[0]}`)\\n\\n```diff\\n{diff_text}\\n```\\n\"\n\n                    logger.info(f\"Added Git history context for query: {query_data.query[:50]}\")\n            except Exception as e:\n                logger.warning(f\"Failed to get Git history: {e}\")\n\n        # Assemble context for LLM\n        context = context_assembler.assemble_context(\n            chunks=final_chunks,\n            query=query_data.query,\n            max_chunks=10\n        )\n\n        # Add Git context if available\n        if git_context:\n            context = git_context + \"\\n\\n\" + context\n\n        # Build prompt (for now, return context as answer)\n        # In Phase 6, this will call the actual LLM\n        prompt = context_assembler.assemble_prompt(\n            chunks=final_chunks,\n            query=query_data.query\n        )\n\n        # Inject Git context into prompt if available\n        if git_context:\n            prompt = prompt.replace(\"# Relevant Code Context\\n\\n\", f\"# Relevant Code Context\\n\\n{git_context}\\n\\n\")\n\n        # Build sources list\n        sources = [\n            {\n                'file_path': chunk['file_path'],\n                'chunk_type': chunk['chunk_type'],\n                'name': chunk['name'],\n                'start_line': chunk['start_line'],\n                'end_line': chunk['end_line'],\n                'similarity': chunk['similarity'],\n                'code_preview': chunk['code'][:200] + '...' if len(chunk['code']) > 200 else chunk['code']\n            }\n            for chunk in final_chunks\n        ]\n\n        # Get metadata summary\n        metadata_summary = context_assembler.build_metadata_summary(final_chunks)\n\n        # Call LLM to generate answer\n        try:\n            logger.info(\"Calling LLM to generate answer\")\n            answer = await llm_provider.generate(\n                prompt=prompt,\n                temperature=0.1,\n                max_tokens=2000\n            )\n            logger.info(f\"LLM generated {len(answer)} chars\")\n\n        except LLMError as e:\n            logger.error(f\"LLM generation failed: {e}\")\n            # Fallback to context only\n            answer = f\"\"\"# Error generating LLM response\n\n{str(e)}\n\n# Retrieved Code Context\n\n{context}\n\n---\n\nQuery: {query_data.query}\nRetrieved: {len(final_chunks)} relevant code chunks from {metadata_summary['unique_files']} file(s).\"\"\"\n\n        return QueryResponse(\n            answer=answer,\n            sources=sources,\n            repo_id=repo_id,\n            metadata={\n                'retrieved_chunks': len(chunks),\n                'final_chunks': len(final_chunks),\n                'collection': collection_name,\n                'reranking_applied': query_data.use_reranking,\n                'summary': metadata_summary,\n                'prompt_length': len(prompt),\n                'llm_provider': llm_provider.get_model_info()['provider']\n            }\n        )\n\n    except Exception as e:\n        logger.error(f\"Query failed: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/query/stream\")\nasync def query_stream(\n    query_data: QueryRequest,\n    db: MetadataDB = Depends(get_db),\n    vector_store: VectorStore = Depends(get_vector_store),\n    reranker: Reranker = Depends(get_reranker),\n    context_assembler: ContextAssembler = Depends(get_context_assembler),\n    llm_provider = Depends(get_llm_provider)\n):\n    \"\"\"Query the RAG system with streaming response and dynamic embedder.\"\"\"\n    # Get active repository if not specified\n    if not query_data.repo_id:\n        active_repo = db.get_active_repository()\n        if not active_repo:\n            raise HTTPException(status_code=400, detail=\"No active repository\")\n        repo_id = active_repo[\"id\"]\n    else:\n        repo_id = query_data.repo_id\n        repo = db.get_repository(repo_id)\n        if not repo:\n            raise HTTPException(status_code=404, detail=\"Repository not found\")\n\n    try:\n        # Get repository info\n        repo = db.get_repository(repo_id)\n        collection_name = repo['chroma_collection_name']\n\n        # Create embedder matching the repository's embedding provider\n        settings = get_settings()\n        embedding_provider = repo.get('embedding_provider', 'local')\n        embedding_model = repo.get('embedding_model')\n\n        # Always use OpenAI embeddings (local embeddings no longer supported)\n        if embedding_provider == \"local\":\n            logger.warning(f\"Repository was indexed with 'local' embeddings, but only 'openai' is supported now. Falling back to OpenAI.\")\n            embedding_provider = \"openai\"\n\n        if embedding_provider == \"openai\":\n            embedder = create_embedder(\n                provider=\"openai\",\n                model_name=embedding_model if embedding_provider == \"openai\" else settings.openai_embedding_model,\n                api_key=settings.openai_api_key\n            )\n        else:\n            # This shouldn't happen anymore, but just in case\n            raise ValueError(f\"Unsupported embedding provider: {embedding_provider}. Only 'openai' is supported.\")\n\n        # Create retriever with matching embedder\n        retriever = CodeRetriever(\n            vector_store=vector_store,\n            embedder=embedder\n        )\n\n        # Build filters\n        filters = {}\n        if query_data.language:\n            filters['language'] = query_data.language\n        if query_data.file_path:\n            filters['file_path'] = query_data.file_path\n\n        # Retrieve chunks\n        chunks = retriever.retrieve(\n            collection_name=collection_name,\n            query=query_data.query,\n            n_results=query_data.n_results or 20,\n            filters=filters if filters else None\n        )\n\n        if not chunks:\n            async def error_stream():\n                yield \"No relevant code found for your query.\"\n            return StreamingResponse(error_stream(), media_type=\"text/plain\")\n\n        # Apply reranking\n        if query_data.use_reranking:\n            chunks = reranker.mmr_rerank(chunks, lambda_param=0.5, top_k=query_data.n_results or 10)\n\n        # Limit chunks\n        final_chunks = chunks[:query_data.n_results] if query_data.n_results else chunks\n\n        # Assemble prompt\n        prompt = context_assembler.assemble_prompt(chunks=final_chunks, query=query_data.query)\n\n        # Stream LLM response\n        async def generate_stream():\n            try:\n                async for chunk in llm_provider.generate_stream(prompt=prompt, temperature=0.1, max_tokens=2000):\n                    yield chunk\n            except LLMError as e:\n                yield f\"\\n\\n[Error: {str(e)}]\"\n\n        return StreamingResponse(generate_stream(), media_type=\"text/plain\")\n\n    except Exception as e:\n        logger.error(f\"Streaming query failed: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=str(e))\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/api/routes.py", "relative_path": "services/rag-pipeline/src/api/routes.py", "filename": "routes.py", "extension": ".py"}}, {"id": "file_optimized_indexer_py_fc3e76b4", "content": "\"\"\"Optimized indexing orchestration with parallel processing.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nimport hashlib\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom threading import Lock\nimport time\n\nfrom ..core.git_ops import GitOperations\nfrom ..core.parser import CodeParser\nfrom ..core.chunker import CodeChunker\nfrom ..core.embedder import BaseEmbedder\nfrom ..core.vector_store import VectorStore\nfrom ..db.metadata_db import MetadataDB\n\nlogger = logging.getLogger(__name__)\n\n\nclass OptimizedRepositoryIndexer:\n    \"\"\"Optimized indexer with parallel processing and batching.\"\"\"\n\n    def __init__(\n        self,\n        metadata_db: MetadataDB,\n        vector_store: VectorStore,\n        embedder: BaseEmbedder,\n        parser: Optional[CodeParser] = None,\n        chunker: Optional[CodeChunker] = None,\n        max_workers: int = 4,\n        batch_size: int = 50\n    ):\n        \"\"\"Initialize the optimized indexer.\n\n        Args:\n            metadata_db: Metadata database instance\n            vector_store: Vector store instance\n            embedder: Embedder instance (BaseEmbedder)\n            parser: Code parser (optional)\n            chunker: Code chunker (optional)\n            max_workers: Maximum number of parallel workers\n            batch_size: Number of chunks to batch for embedding\n        \"\"\"\n        self.metadata_db = metadata_db\n        self.vector_store = vector_store\n        self.embedder = embedder\n        self.parser = parser or CodeParser()\n        self.chunker = chunker or CodeChunker()\n        self.max_workers = max_workers\n        self.batch_size = batch_size\n\n        # Thread-safe counters\n        self._lock = Lock()\n        self._total_chunks = 0\n        self._indexed_files = 0\n        self._skipped_files = 0\n        self._failed_files = 0\n\n        logger.info(f\"Optimized indexer initialized with {max_workers} workers, batch size {batch_size}\")\n\n    def index_repository(\n        self,\n        repo_id: str,\n        repo_path: str,\n        force_reindex: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Index entire repository with parallel processing.\n\n        Args:\n            repo_id: Repository UUID\n            repo_path: Path to the repository\n            force_reindex: If True, reindex all files even if unchanged\n\n        Returns:\n            Dictionary with indexing results\n        \"\"\"\n        start_time = time.time()\n        logger.info(f\"Starting optimized repository indexing: {repo_path}\")\n\n        # Update indexing status\n        self.metadata_db.update_repository(repo_id, indexing_status='in_progress')\n\n        # Reset counters\n        self._total_chunks = 0\n        self._indexed_files = 0\n        self._skipped_files = 0\n        self._failed_files = 0\n\n        try:\n            # Initialize Git operations\n            git_ops = GitOperations(repo_path)\n\n            if not git_ops.is_git_repository(repo_path):\n                raise ValueError(f\"Not a valid Git repository: {repo_path}\")\n\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            collection_name = repo_info['chroma_collection_name']\n\n            # Get all tracked files\n            tracked_files = git_ops.get_tracked_files()\n            logger.info(f\"Found {len(tracked_files)} tracked files\")\n\n            # Get latest commit hash\n            commits = git_ops.get_commit_history(max_count=1)\n            latest_commit = commits[0]['hash'] if commits else None\n\n            # Filter files that need indexing\n            files_to_index = []\n            for file_path in tracked_files:\n                absolute_file_path = Path(repo_path) / file_path\n\n                # Check if we should index this file\n                if not self.chunker.should_index_file(file_path):\n                    with self._lock:\n                        self._skipped_files += 1\n                    continue\n\n                # Check if file has changed (unless force_reindex)\n                if not force_reindex:\n                    file_hash = self._compute_file_hash(absolute_file_path)\n                    existing_file = self.metadata_db.get_file(repo_id, str(file_path))\n\n                    if existing_file and existing_file['file_hash'] == file_hash:\n                        logger.debug(f\"Skipping unchanged file: {file_path}\")\n                        with self._lock:\n                            self._skipped_files += 1\n                        continue\n\n                files_to_index.append((absolute_file_path, file_path, latest_commit))\n\n            logger.info(f\"Indexing {len(files_to_index)} files in parallel with {self.max_workers} workers\")\n\n            # Process files in parallel\n            all_chunks = []\n            file_metadata = []\n\n            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                # Submit all files for processing\n                future_to_file = {\n                    executor.submit(\n                        self._process_file,\n                        repo_id,\n                        absolute_path,\n                        relative_path,\n                        commit_hash\n                    ): (absolute_path, relative_path)\n                    for absolute_path, relative_path, commit_hash in files_to_index\n                }\n\n                # Collect results as they complete\n                for future in as_completed(future_to_file):\n                    absolute_path, relative_path = future_to_file[future]\n                    try:\n                        chunks, metadata = future.result()\n                        if chunks:\n                            all_chunks.extend(chunks)\n                            file_metadata.append(metadata)\n\n                            with self._lock:\n                                self._indexed_files += 1\n\n                            if len(all_chunks) % 100 == 0:\n                                logger.info(f\"Processed {self._indexed_files}/{len(files_to_index)} files, {len(all_chunks)} chunks so far\")\n                    except Exception as e:\n                        logger.error(f\"Failed to process file {relative_path}: {e}\")\n                        with self._lock:\n                            self._failed_files += 1\n\n            # Batch embed and store all chunks\n            if all_chunks:\n                logger.info(f\"Embedding and storing {len(all_chunks)} chunks in batches...\")\n                self._batch_embed_and_store(collection_name, all_chunks)\n\n                with self._lock:\n                    self._total_chunks = len(all_chunks)\n\n            # Update file metadata in database\n            logger.info(\"Updating file metadata in database...\")\n            for metadata in file_metadata:\n                self.metadata_db.upsert_file(**metadata)\n\n            # Update repository metadata\n            self.metadata_db.update_repository(\n                repo_id,\n                last_indexed_at='CURRENT_TIMESTAMP',\n                last_commit_hash=latest_commit,\n                total_chunks=self._total_chunks,\n                total_files=self._indexed_files,\n                indexing_status='completed'\n            )\n\n            elapsed_time = time.time() - start_time\n            result = {\n                'repo_id': repo_id,\n                'repo_path': repo_path,\n                'collection_name': collection_name,\n                'total_files': len(tracked_files),\n                'indexed_files': self._indexed_files,\n                'skipped_files': self._skipped_files,\n                'failed_files': self._failed_files,\n                'total_chunks': self._total_chunks,\n                'latest_commit': latest_commit,\n                'elapsed_time_seconds': round(elapsed_time, 2),\n                'chunks_per_second': round(self._total_chunks / elapsed_time, 2) if elapsed_time > 0 else 0,\n                'status': 'completed'\n            }\n\n            logger.info(\n                f\"Repository indexing completed: {self._indexed_files} files, \"\n                f\"{self._total_chunks} chunks in {elapsed_time:.2f}s \"\n                f\"({result['chunks_per_second']:.2f} chunks/sec)\"\n            )\n            return result\n\n        except Exception as e:\n            logger.error(f\"Repository indexing failed: {e}\", exc_info=True)\n            self.metadata_db.update_repository(repo_id, indexing_status='failed')\n            raise\n\n    def _process_file(\n        self,\n        repo_id: str,\n        file_path: Path,\n        relative_path: str,\n        commit_hash: Optional[str]\n    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        \"\"\"Process a single file (parse and chunk).\n\n        Args:\n            repo_id: Repository UUID\n            file_path: Absolute path to file\n            relative_path: Relative path from repo root\n            commit_hash: Git commit hash\n\n        Returns:\n            Tuple of (chunks, file_metadata)\n        \"\"\"\n        # Read file content\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            logger.warning(f\"Skipping binary file: {file_path}\")\n            return [], {}\n        except Exception as e:\n            logger.error(f\"Failed to read file {file_path}: {e}\")\n            return [], {}\n\n        # Parse file into chunks\n        parsed_chunks = self.parser.parse_file(file_path, content)\n\n        # If parser doesn't support this file type, try text chunking\n        if not parsed_chunks:\n            parsed_chunks = self.chunker.chunk_text(content, file_path)\n\n        # Apply chunking strategy\n        final_chunks = self.chunker.chunk_code(parsed_chunks)\n\n        # Add metadata to chunks\n        for chunk in final_chunks:\n            chunk['commit_hash'] = commit_hash or ''\n            chunk['is_uncommitted'] = False\n\n        # Prepare file metadata for DB\n        file_hash = self._compute_file_hash(file_path)\n        file_metadata = {\n            'repo_id': repo_id,\n            'file_path': str(file_path),\n            'file_hash': file_hash,\n            'chunk_count': len(final_chunks),\n            'language': parsed_chunks[0].get('language', 'unknown') if parsed_chunks else 'unknown'\n        }\n\n        return final_chunks, file_metadata\n\n    def _batch_embed_and_store(\n        self,\n        collection_name: str,\n        chunks: List[Dict[str, Any]]\n    ):\n        \"\"\"Embed and store chunks in batches.\n\n        Args:\n            collection_name: ChromaDB collection name\n            chunks: List of chunks to embed and store\n        \"\"\"\n        total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size\n\n        for i in range(0, len(chunks), self.batch_size):\n            batch = chunks[i:i + self.batch_size]\n            batch_num = i // self.batch_size + 1\n\n            logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)\")\n\n            # Extract texts for embedding\n            texts = [chunk['code'] for chunk in batch]\n\n            # Generate embeddings\n            try:\n                embeddings = self.embedder.embed_batch(texts, show_progress=False)\n\n                # Store in vector database\n                self.vector_store.add_chunks(collection_name, batch, embeddings=embeddings)\n\n                logger.debug(f\"Batch {batch_num} stored successfully\")\n            except Exception as e:\n                logger.error(f\"Failed to process batch {batch_num}: {e}\")\n                raise\n\n    def _compute_file_hash(self, file_path: Path) -> str:\n        \"\"\"Compute SHA256 hash of a file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            SHA256 hash as hex string\n        \"\"\"\n        try:\n            sha256 = hashlib.sha256()\n            with open(file_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(4096), b''):\n                    sha256.update(chunk)\n            return sha256.hexdigest()\n        except Exception as e:\n            logger.error(f\"Failed to compute hash for {file_path}: {e}\")\n            return ''\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/indexing/optimized_indexer.py", "relative_path": "services/rag-pipeline/src/indexing/optimized_indexer.py", "filename": "optimized_indexer.py", "extension": ".py"}}, {"id": "file___init___py_404e356a", "content": "\"\"\"Indexing module for Git repository processing.\"\"\"\n\nfrom .indexer import RepositoryIndexer\n\n__all__ = ['RepositoryIndexer']\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/indexing/__init__.py", "relative_path": "services/rag-pipeline/src/indexing/__init__.py", "filename": "__init__.py", "extension": ".py"}}, {"id": "file_indexer_py_9a1edc8d", "content": "\"\"\"Main indexing orchestration for Git repositories.\"\"\"\n\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nimport hashlib\n\nfrom ..core.git_ops import GitOperations\nfrom ..core.parser import CodeParser\nfrom ..core.chunker import CodeChunker\nfrom ..core.embedder import BaseEmbedder\nfrom ..core.vector_store import VectorStore\nfrom ..db.metadata_db import MetadataDB\n\nlogger = logging.getLogger(__name__)\n\n\nclass RepositoryIndexer:\n    \"\"\"Orchestrates the indexing of Git repositories into ChromaDB.\"\"\"\n\n    def __init__(\n        self,\n        metadata_db: MetadataDB,\n        vector_store: VectorStore,\n        embedder: BaseEmbedder,\n        parser: Optional[CodeParser] = None,\n        chunker: Optional[CodeChunker] = None\n    ):\n        \"\"\"Initialize the indexer.\n\n        Args:\n            metadata_db: Metadata database instance\n            vector_store: Vector store instance\n            embedder: Embedder instance (BaseEmbedder)\n            parser: Code parser (optional, creates new if not provided)\n            chunker: Code chunker (optional, creates new if not provided)\n        \"\"\"\n        self.metadata_db = metadata_db\n        self.vector_store = vector_store\n        self.embedder = embedder\n        self.parser = parser or CodeParser()\n        self.chunker = chunker or CodeChunker()\n\n        logger.info(\"Repository indexer initialized\")\n\n    def index_repository(\n        self,\n        repo_id: str,\n        repo_path: str,\n        force_reindex: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Index an entire repository.\n\n        Args:\n            repo_id: Repository UUID\n            repo_path: Path to the repository\n            force_reindex: If True, reindex all files even if unchanged\n\n        Returns:\n            Dictionary with indexing results\n        \"\"\"\n        logger.info(f\"Starting full repository indexing: {repo_path}\")\n\n        # Update indexing status\n        self.metadata_db.update_repository(\n            repo_id,\n            indexing_status='in_progress'\n        )\n\n        try:\n            # Initialize Git operations\n            git_ops = GitOperations(repo_path)\n\n            # Verify it's a valid Git repository\n            if not git_ops.is_git_repository(repo_path):\n                raise ValueError(f\"Not a valid Git repository: {repo_path}\")\n\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            collection_name = repo_info['chroma_collection_name']\n\n            # Get all tracked files\n            tracked_files = git_ops.get_tracked_files()\n            logger.info(f\"Found {len(tracked_files)} tracked files\")\n\n            # Get latest commit hash\n            commits = git_ops.get_commit_history(max_count=1)\n            latest_commit = commits[0]['hash'] if commits else None\n\n            # Process files\n            total_chunks = 0\n            indexed_files = 0\n            skipped_files = 0\n\n            for file_path in tracked_files:\n                # Convert to absolute path\n                absolute_file_path = Path(repo_path) / file_path\n\n                # Check if we should index this file\n                if not self.chunker.should_index_file(file_path):\n                    skipped_files += 1\n                    continue\n\n                # Check if file has changed (unless force_reindex)\n                if not force_reindex:\n                    file_hash = self._compute_file_hash(absolute_file_path)\n                    existing_file = self.metadata_db.get_file(repo_id, str(file_path))\n\n                    if existing_file and existing_file['file_hash'] == file_hash:\n                        logger.debug(f\"Skipping unchanged file: {file_path}\")\n                        skipped_files += 1\n                        continue\n\n                # Index the file\n                try:\n                    chunks_added = self._index_file(\n                        repo_id=repo_id,\n                        collection_name=collection_name,\n                        file_path=absolute_file_path,\n                        commit_hash=latest_commit,\n                        is_uncommitted=False\n                    )\n\n                    total_chunks += chunks_added\n                    indexed_files += 1\n\n                except Exception as e:\n                    logger.error(f\"Failed to index file {file_path}: {e}\")\n                    continue\n\n            # Update repository metadata\n            self.metadata_db.update_repository(\n                repo_id,\n                last_indexed_at='CURRENT_TIMESTAMP',\n                last_commit_hash=latest_commit,\n                total_chunks=total_chunks,\n                total_files=indexed_files,\n                indexing_status='completed'\n            )\n\n            result = {\n                'repo_id': repo_id,\n                'repo_path': repo_path,\n                'collection_name': collection_name,\n                'total_files': len(tracked_files),\n                'indexed_files': indexed_files,\n                'skipped_files': skipped_files,\n                'total_chunks': total_chunks,\n                'latest_commit': latest_commit,\n                'status': 'completed'\n            }\n\n            logger.info(f\"Repository indexing completed: {indexed_files} files, {total_chunks} chunks\")\n            return result\n\n        except Exception as e:\n            logger.error(f\"Repository indexing failed: {e}\")\n\n            # Update status to failed\n            self.metadata_db.update_repository(\n                repo_id,\n                indexing_status='failed'\n            )\n\n            raise\n\n    def index_file(\n        self,\n        repo_id: str,\n        file_path: str,\n        is_uncommitted: bool = False\n    ) -> int:\n        \"\"\"Index a single file.\n\n        Args:\n            repo_id: Repository UUID\n            file_path: Path to the file (relative or absolute)\n            is_uncommitted: Whether this is an uncommitted change\n\n        Returns:\n            Number of chunks indexed\n        \"\"\"\n        logger.info(f\"Indexing file: {file_path}\")\n\n        try:\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            collection_name = repo_info['chroma_collection_name']\n            repo_path = Path(repo_info['path'])\n\n            # Resolve file path\n            file_path = Path(file_path)\n            if not file_path.is_absolute():\n                file_path = repo_path / file_path\n\n            # Get latest commit hash\n            git_ops = GitOperations(str(repo_path))\n            commits = git_ops.get_commit_history(max_count=1)\n            latest_commit = commits[0]['hash'] if commits else None\n\n            # Index the file\n            chunks_added = self._index_file(\n                repo_id=repo_id,\n                collection_name=collection_name,\n                file_path=file_path,\n                commit_hash=latest_commit,\n                is_uncommitted=is_uncommitted\n            )\n\n            logger.info(f\"File indexed: {chunks_added} chunks\")\n            return chunks_added\n\n        except Exception as e:\n            logger.error(f\"Failed to index file {file_path}: {e}\")\n            raise\n\n    def _index_file(\n        self,\n        repo_id: str,\n        collection_name: str,\n        file_path: Path,\n        commit_hash: Optional[str] = None,\n        is_uncommitted: bool = False\n    ) -> int:\n        \"\"\"Internal method to index a single file.\n\n        Args:\n            repo_id: Repository UUID\n            collection_name: ChromaDB collection name\n            file_path: Path to the file\n            commit_hash: Commit hash (if committed)\n            is_uncommitted: Whether this is an uncommitted change\n\n        Returns:\n            Number of chunks indexed\n        \"\"\"\n        # Read file content\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            logger.warning(f\"Skipping binary file: {file_path}\")\n            return 0\n        except Exception as e:\n            logger.error(f\"Failed to read file {file_path}: {e}\")\n            return 0\n\n        # Parse file into chunks\n        parsed_chunks = self.parser.parse_file(file_path, content)\n\n        # If parser doesn't support this file type, try text chunking\n        if not parsed_chunks:\n            parsed_chunks = self.chunker.chunk_text(content, file_path)\n\n        # Apply chunking strategy (may split large chunks)\n        final_chunks = self.chunker.chunk_code(parsed_chunks)\n\n        # Add metadata to chunks\n        for chunk in final_chunks:\n            chunk['commit_hash'] = commit_hash or ''\n            chunk['is_uncommitted'] = is_uncommitted\n\n        # Generate embeddings and add chunks to vector store\n        if final_chunks:\n            # Extract code texts for embedding\n            chunk_texts = [chunk['code'] for chunk in final_chunks]\n\n            # Generate embeddings using the embedder\n            logger.debug(f\"Generating embeddings for {len(chunk_texts)} chunks from {file_path}\")\n            embeddings = self.embedder.embed_batch(chunk_texts, show_progress=False)\n            logger.debug(f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1] if len(embeddings.shape) > 1 else 'N/A'}\")\n\n            # Add chunks with pre-computed embeddings to vector store\n            self.vector_store.add_chunks(collection_name, final_chunks, embeddings=embeddings)\n\n            # Update file tracking in metadata DB\n            file_hash = self._compute_file_hash(file_path)\n            self.metadata_db.upsert_file(\n                repo_id=repo_id,\n                file_path=str(file_path),\n                file_hash=file_hash,\n                chunk_count=len(final_chunks),\n                language=parsed_chunks[0].get('language', 'unknown') if parsed_chunks else 'unknown'\n            )\n\n        return len(final_chunks)\n\n    def delete_file_chunks(\n        self,\n        repo_id: str,\n        file_path: str\n    ) -> bool:\n        \"\"\"Delete all chunks for a specific file.\n\n        Args:\n            repo_id: Repository UUID\n            file_path: Path to the file\n\n        Returns:\n            True if successful\n        \"\"\"\n        logger.info(f\"Deleting chunks for file: {file_path}\")\n\n        try:\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            collection_name = repo_info['chroma_collection_name']\n\n            # Delete chunks from vector store using metadata filter\n            self.vector_store.delete_chunks(\n                collection_name,\n                where={'file_path': str(file_path)}\n            )\n\n            # Delete file from metadata DB\n            # Note: metadata_db doesn't have a delete_file method yet\n            # This would need to be added to metadata_db.py\n\n            logger.info(f\"Deleted chunks for file: {file_path}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to delete file chunks: {e}\")\n            return False\n\n    def incremental_index(\n        self,\n        repo_id: str\n    ) -> Dict[str, Any]:\n        \"\"\"Perform incremental indexing (only modified files).\n\n        Args:\n            repo_id: Repository UUID\n\n        Returns:\n            Dictionary with indexing results\n        \"\"\"\n        logger.info(f\"Starting incremental indexing for repo: {repo_id}\")\n\n        try:\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            repo_path = repo_info['path']\n            collection_name = repo_info['chroma_collection_name']\n\n            # Initialize Git operations\n            git_ops = GitOperations(repo_path)\n\n            # Get modified files (uncommitted changes)\n            modified_files = git_ops.get_modified_files()\n            logger.info(f\"Found {len(modified_files)} modified files\")\n\n            total_chunks = 0\n            indexed_files = 0\n\n            for file_path_str in modified_files:\n                file_path = Path(repo_path) / file_path_str\n\n                if not self.chunker.should_index_file(file_path):\n                    continue\n\n                try:\n                    # Delete old chunks for this file\n                    self.delete_file_chunks(repo_id, str(file_path))\n\n                    # Re-index the file\n                    chunks_added = self._index_file(\n                        repo_id=repo_id,\n                        collection_name=collection_name,\n                        file_path=file_path,\n                        commit_hash=None,\n                        is_uncommitted=True\n                    )\n\n                    total_chunks += chunks_added\n                    indexed_files += 1\n\n                except Exception as e:\n                    logger.error(f\"Failed to index modified file {file_path}: {e}\")\n                    continue\n\n            result = {\n                'repo_id': repo_id,\n                'indexed_files': indexed_files,\n                'total_chunks': total_chunks,\n                'status': 'completed'\n            }\n\n            logger.info(f\"Incremental indexing completed: {indexed_files} files, {total_chunks} chunks\")\n            return result\n\n        except Exception as e:\n            logger.error(f\"Incremental indexing failed: {e}\")\n            raise\n\n    def _compute_file_hash(self, file_path: Path) -> str:\n        \"\"\"Compute SHA256 hash of a file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            SHA256 hash as hex string\n        \"\"\"\n        try:\n            sha256 = hashlib.sha256()\n            with open(file_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(4096), b''):\n                    sha256.update(chunk)\n            return sha256.hexdigest()\n        except Exception as e:\n            logger.error(f\"Failed to compute hash for {file_path}: {e}\")\n            return ''\n\n    def get_indexing_stats(self, repo_id: str) -> Dict[str, Any]:\n        \"\"\"Get indexing statistics for a repository.\n\n        Args:\n            repo_id: Repository UUID\n\n        Returns:\n            Dictionary with statistics\n        \"\"\"\n        try:\n            # Get repository info\n            repo_info = self.metadata_db.get_repository(repo_id)\n            if not repo_info:\n                raise ValueError(f\"Repository not found: {repo_id}\")\n\n            # Get collection stats from vector store\n            collection_stats = self.vector_store.get_collection_stats(\n                repo_info['chroma_collection_name']\n            )\n\n            # Combine with metadata DB stats\n            stats = {\n                'repo_id': repo_id,\n                'repo_name': repo_info['name'],\n                'repo_path': repo_info['path'],\n                'total_files': repo_info.get('total_files', 0),\n                'total_chunks': repo_info.get('total_chunks', 0),\n                'last_indexed_at': repo_info.get('last_indexed_at'),\n                'last_commit_hash': repo_info.get('last_commit_hash'),\n                'indexing_status': repo_info.get('indexing_status', 'unknown'),\n                'chroma_chunk_count': collection_stats.get('count', 0),\n                'collection_name': repo_info['chroma_collection_name']\n            }\n\n            return stats\n\n        except Exception as e:\n            logger.error(f\"Failed to get indexing stats: {e}\")\n            return {}\n", "metadata": {"full_path": "/Users/sana/Documents/git-rag-chat-local/services/rag-pipeline/src/indexing/indexer.py", "relative_path": "services/rag-pipeline/src/indexing/indexer.py", "filename": "indexer.py", "extension": ".py"}}]}